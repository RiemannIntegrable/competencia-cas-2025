{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Clasificación - Gastos Adicionales de Vivienda\n",
    "\n",
    "## Objetivo\n",
    "Predecir si un asegurado presentará siniestros en la cobertura de Gastos Adicionales de Vivienda (clasificación binaria).\n",
    "\n",
    "## Pipeline\n",
    "1. **Fase 1**: Preprocesamiento + GLM + LazyPredict\n",
    "2. **Fase 2**: Optimización de top 5 modelos con GridSearchCV\n",
    "3. **Fase 3**: Evaluación y selección del mejor modelo\n",
    "\n",
    "**Dataset**: 7,999 registros | Desbalanceo: 94.8% no siniestrados vs 5.2% siniestrados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Librerías importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocesamiento\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, make_scorer\n",
    ")\n",
    "\n",
    "# LazyPredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# Selección de variables\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original: (7999, 9)\n",
      "\n",
      "Columnas: ['año_cursado', 'estudios_area', 'calif_promedio', '2_o_mas_inquilinos', 'distancia_al_campus', 'genero', 'extintor_incendios', 'Gastos_Adicionales_siniestros_num', 'Gastos_Adicionales_siniestros_monto']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>año_cursado</th>\n",
       "      <th>estudios_area</th>\n",
       "      <th>calif_promedio</th>\n",
       "      <th>2_o_mas_inquilinos</th>\n",
       "      <th>distancia_al_campus</th>\n",
       "      <th>genero</th>\n",
       "      <th>extintor_incendios</th>\n",
       "      <th>Gastos_Adicionales_siniestros_num</th>\n",
       "      <th>Gastos_Adicionales_siniestros_monto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3er año</td>\n",
       "      <td>Humanidades</td>\n",
       "      <td>3.01</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Si</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3er año</td>\n",
       "      <td>Ciencias</td>\n",
       "      <td>1.52</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>Si</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3er año</td>\n",
       "      <td>Administracion</td>\n",
       "      <td>7.68</td>\n",
       "      <td>No</td>\n",
       "      <td>0.22</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4to año</td>\n",
       "      <td>Administracion</td>\n",
       "      <td>8.06</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2do año</td>\n",
       "      <td>Administracion</td>\n",
       "      <td>6.72</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  año_cursado   estudios_area  calif_promedio 2_o_mas_inquilinos  \\\n",
       "0     3er año     Humanidades            3.01                 No   \n",
       "1     3er año        Ciencias            1.52                 No   \n",
       "2     3er año  Administracion            7.68                 No   \n",
       "3     4to año  Administracion            8.06                 No   \n",
       "4     2do año  Administracion            6.72                 No   \n",
       "\n",
       "   distancia_al_campus     genero extintor_incendios  \\\n",
       "0                 0.00  Masculino                 Si   \n",
       "1                 0.00   Femenino                 Si   \n",
       "2                 0.22   Femenino                 No   \n",
       "3                 0.00  Masculino                 No   \n",
       "4                 0.00   Femenino                 No   \n",
       "\n",
       "   Gastos_Adicionales_siniestros_num  Gastos_Adicionales_siniestros_monto  \n",
       "0                               0.00                                 0.00  \n",
       "1                               0.00                                 0.00  \n",
       "2                               0.00                                 0.00  \n",
       "3                               0.00                                 0.00  \n",
       "4                               0.00                                 0.00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar datos\n",
    "df = pd.read_csv(\"../data/processed/adicionales_full.csv\")\n",
    "\n",
    "print(f\"Dataset original: {df.shape}\")\n",
    "print(f\"\\nColumnas: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset preparado: (7999, 8)\n",
      "\n",
      "Distribución variable objetivo:\n",
      "siniestrado\n",
      "0    7583\n",
      "1     416\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Porcentaje siniestrados: 5.20%\n",
      "Desbalanceo: 18.2:1\n"
     ]
    }
   ],
   "source": [
    "# Eliminar columna de monto (no se usa en clasificación)\n",
    "df = df.drop('Gastos_Adicionales_siniestros_monto', axis=1)\n",
    "\n",
    "# Crear variable binaria objetivo: 0 si no siniestró, 1 si siniestró\n",
    "df['siniestrado'] = (df['Gastos_Adicionales_siniestros_num'] > 0).astype(int)\n",
    "\n",
    "# Eliminar columna original de número de siniestros\n",
    "df = df.drop('Gastos_Adicionales_siniestros_num', axis=1)\n",
    "\n",
    "print(f\"\\nDataset preparado: {df.shape}\")\n",
    "print(f\"\\nDistribución variable objetivo:\")\n",
    "print(df['siniestrado'].value_counts())\n",
    "print(f\"\\nPorcentaje siniestrados: {df['siniestrado'].mean()*100:.2f}%\")\n",
    "print(f\"Desbalanceo: {df['siniestrado'].value_counts()[0] / df['siniestrado'].value_counts()[1]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis Exploratorio Rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables numéricas: ['calif_promedio', 'distancia_al_campus']\n",
      "Variables categóricas: ['año_cursado', 'estudios_area', '2_o_mas_inquilinos', 'genero', 'extintor_incendios']\n",
      "\n",
      "Total features: 7\n",
      "\n",
      "Valores faltantes:\n",
      "año_cursado            0\n",
      "estudios_area          0\n",
      "calif_promedio         0\n",
      "2_o_mas_inquilinos     0\n",
      "distancia_al_campus    0\n",
      "genero                 0\n",
      "extintor_incendios     0\n",
      "siniestrado            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identificar tipos de variables\n",
    "numeric_features = ['calif_promedio', 'distancia_al_campus']\n",
    "categorical_features = ['año_cursado', 'estudios_area', '2_o_mas_inquilinos', 'genero', 'extintor_incendios']\n",
    "\n",
    "print(\"Variables numéricas:\", numeric_features)\n",
    "print(\"Variables categóricas:\", categorical_features)\n",
    "print(f\"\\nTotal features: {len(numeric_features) + len(categorical_features)}\")\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(f\"\\nValores faltantes:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ESTADÍSTICAS DESCRIPTIVAS POR CLASE\n",
      "================================================================================\n",
      "\n",
      "año_cursado:\n",
      "siniestrado     0    1\n",
      "año_cursado           \n",
      "1er año     94.14 5.86\n",
      "2do año     95.21 4.79\n",
      "3er año     94.88 5.12\n",
      "4to año     94.59 5.41\n",
      "posgrado    95.62 4.38\n",
      "\n",
      "estudios_area:\n",
      "siniestrado        0    1\n",
      "estudios_area            \n",
      "Administracion 94.58 5.42\n",
      "Ciencias       94.99 5.01\n",
      "Humanidades    95.41 4.59\n",
      "Otro           94.22 5.78\n",
      "\n",
      "2_o_mas_inquilinos:\n",
      "siniestrado            0    1\n",
      "2_o_mas_inquilinos           \n",
      "No                 95.68 4.32\n",
      "Si                 91.15 8.85\n",
      "\n",
      "genero:\n",
      "siniestrado      0    1\n",
      "genero                 \n",
      "Femenino     94.86 5.14\n",
      "Masculino    94.50 5.50\n",
      "No respuesta 95.98 4.02\n",
      "Otro         95.86 4.14\n",
      "\n",
      "extintor_incendios:\n",
      "siniestrado            0    1\n",
      "extintor_incendios           \n",
      "No                 94.38 5.62\n",
      "Si                 94.98 5.02\n"
     ]
    }
   ],
   "source": [
    "# Estadísticas descriptivas por clase\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS POR CLASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for var in categorical_features:\n",
    "    print(f\"\\n{var}:\")\n",
    "    tabla = pd.crosstab(df[var], df['siniestrado'], normalize='index') * 100\n",
    "    print(tabla.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento y Split de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (6399, 7) | Siniestrados: 333 (5.20%)\n",
      "Test set:  (1600, 7) | Siniestrados: 83 (5.19%)\n"
     ]
    }
   ],
   "source": [
    "# Separar features y target\n",
    "X = df.drop('siniestrado', axis=1)\n",
    "y = df['siniestrado']\n",
    "\n",
    "# Split train/test estratificado 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape} | Siniestrados: {y_train.sum()} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Test set:  {X_test.shape} | Siniestrados: {y_test.sum()} ({y_test.mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features después de preprocesamiento: 14\n",
      "Shape X_train: (6399, 14)\n",
      "Shape X_test: (1600, 14)\n"
     ]
    }
   ],
   "source": [
    "# Pipeline de preprocesamiento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Ajustar y transformar datos\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Obtener nombres de features después de one-hot encoding\n",
    "feature_names_num = numeric_features\n",
    "feature_names_cat = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "feature_names = list(feature_names_num) + list(feature_names_cat)\n",
    "\n",
    "# Convertir a DataFrame (mantener índices originales para alinear con y_train/y_test)\n",
    "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index)\n",
    "X_test_df = pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index)\n",
    "\n",
    "print(f\"\\nFeatures después de preprocesamiento: {len(feature_names)}\")\n",
    "print(f\"Shape X_train: {X_train_df.shape}\")\n",
    "print(f\"Shape X_test: {X_test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fase 1 - GLM con Selección Backward de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODELO LOGÍSTICO COMPLETO (todas las variables)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:            siniestrado   No. Observations:                 6399\n",
      "Model:                          Logit   Df Residuals:                     6384\n",
      "Method:                           MLE   Df Model:                           14\n",
      "Date:                Fri, 10 Oct 2025   Pseudo R-squ.:                 0.02224\n",
      "Time:                        02:23:43   Log-Likelihood:                -1279.3\n",
      "converged:                       True   LL-Null:                       -1308.4\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.408e-07\n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "const                        -2.7643      0.183    -15.132      0.000      -3.122      -2.406\n",
      "calif_promedio               -0.0519      0.056     -0.921      0.357      -0.162       0.058\n",
      "distancia_al_campus           0.0841      0.052      1.630      0.103      -0.017       0.185\n",
      "año_cursado_2do año          -0.2412      0.169     -1.425      0.154      -0.573       0.091\n",
      "año_cursado_3er año          -0.0642      0.163     -0.394      0.694      -0.384       0.255\n",
      "año_cursado_4to año          -0.1187      0.165     -0.719      0.472      -0.443       0.205\n",
      "año_cursado_posgrado         -0.2643      0.219     -1.205      0.228      -0.694       0.165\n",
      "estudios_area_Ciencias       -0.2160      0.162     -1.332      0.183      -0.534       0.102\n",
      "estudios_area_Humanidades    -0.2243      0.162     -1.384      0.166      -0.542       0.093\n",
      "estudios_area_Otro            0.0908      0.152      0.598      0.550      -0.206       0.388\n",
      "2_o_mas_inquilinos_Si         0.8005      0.121      6.610      0.000       0.563       1.038\n",
      "genero_Masculino              0.0666      0.118      0.564      0.573      -0.165       0.298\n",
      "genero_No respuesta          -0.5482      0.424     -1.292      0.196      -1.380       0.283\n",
      "genero_Otro                  -0.0731      0.246     -0.298      0.766      -0.555       0.408\n",
      "extintor_incendios_Si        -0.2382      0.120     -1.989      0.047      -0.473      -0.004\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Modelo logístico completo para referencia\n",
    "X_train_const = sm.add_constant(X_train_df)\n",
    "logit_full = sm.Logit(y_train, X_train_const)\n",
    "result_full = logit_full.fit(disp=0)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODELO LOGÍSTICO COMPLETO (todas las variables)\")\n",
    "print(\"=\" * 80)\n",
    "print(result_full.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SELECCIÓN BACKWARD DE VARIABLES (α = 0.1)\n",
      "================================================================================\n",
      "\n",
      "Eliminando: genero_Otro (p-valor: 0.7660)\n",
      "Eliminando: año_cursado_3er año (p-valor: 0.6920)\n",
      "Eliminando: año_cursado_4to año (p-valor: 0.5401)\n",
      "Eliminando: estudios_area_Otro (p-valor: 0.5434)\n",
      "Eliminando: genero_Masculino (p-valor: 0.5134)\n",
      "Eliminando: calif_promedio (p-valor: 0.3606)\n",
      "Eliminando: año_cursado_posgrado (p-valor: 0.3071)\n",
      "Eliminando: año_cursado_2do año (p-valor: 0.2712)\n",
      "Eliminando: genero_No respuesta (p-valor: 0.1630)\n",
      "Eliminando: distancia_al_campus (p-valor: 0.1063)\n",
      "\n",
      "================================================================================\n",
      "VARIABLES SELECCIONADAS: 4\n",
      "================================================================================\n",
      "  • estudios_area_Ciencias\n",
      "  • estudios_area_Humanidades\n",
      "  • 2_o_mas_inquilinos_Si\n",
      "  • extintor_incendios_Si\n",
      "\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:            siniestrado   No. Observations:                 6399\n",
      "Model:                          Logit   Df Residuals:                     6394\n",
      "Method:                           MLE   Df Model:                            4\n",
      "Date:                Fri, 10 Oct 2025   Pseudo R-squ.:                 0.01867\n",
      "Time:                        02:23:43   Log-Likelihood:                -1284.0\n",
      "converged:                       True   LL-Null:                       -1308.4\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.238e-10\n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "const                        -2.8216      0.115    -24.460      0.000      -3.048      -2.596\n",
      "estudios_area_Ciencias       -0.2639      0.142     -1.856      0.063      -0.543       0.015\n",
      "estudios_area_Humanidades    -0.2737      0.142     -1.925      0.054      -0.552       0.005\n",
      "2_o_mas_inquilinos_Si         0.8004      0.121      6.626      0.000       0.564       1.037\n",
      "extintor_incendios_Si        -0.2342      0.119     -1.960      0.050      -0.468    1.56e-05\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Selección Backward basada en p-valores\n",
    "def backward_elimination(X, y, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Realiza selección backward de variables basada en p-valores del GLM.\n",
    "    \"\"\"\n",
    "    features = list(X.columns)\n",
    "    \n",
    "    while True:\n",
    "        X_const = sm.add_constant(X[features])\n",
    "        model = sm.Logit(y, X_const).fit(disp=0)\n",
    "        p_values = model.pvalues.iloc[1:]\n",
    "        max_p_value = p_values.max()\n",
    "        \n",
    "        if max_p_value <= significance_level:\n",
    "            break\n",
    "            \n",
    "        feature_to_remove = p_values.idxmax()\n",
    "        features.remove(feature_to_remove)\n",
    "        print(f\"Eliminando: {feature_to_remove} (p-valor: {max_p_value:.4f})\")\n",
    "    \n",
    "    return features, model\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SELECCIÓN BACKWARD DE VARIABLES (α = 0.1)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "selected_features, logit_backward = backward_elimination(X_train_df, y_train, significance_level=0.1)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(f\"VARIABLES SELECCIONADAS: {len(selected_features)}\")\n",
    "print(\"=\" * 80)\n",
    "for feat in selected_features:\n",
    "    print(f\"  • {feat}\")\n",
    "print()\n",
    "print(logit_backward.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURES FINALES (con todos los niveles de categóricas)\n",
      "================================================================================\n",
      "\n",
      "Total features: 5\n",
      "\n",
      "Features seleccionadas:\n",
      "  ✓ 2_o_mas_inquilinos_Si\n",
      "  ✓ estudios_area_Ciencias\n",
      "  ✓ estudios_area_Humanidades\n",
      "  + estudios_area_Otro\n",
      "  ✓ extintor_incendios_Si\n",
      "\n",
      "Shape final - Train: (6399, 5) | Test: (1600, 5)\n"
     ]
    }
   ],
   "source": [
    "# Expandir variables categóricas completas\n",
    "def expand_categorical_features(selected_features, all_feature_names, categorical_features):\n",
    "    selected_cat_vars = set()\n",
    "    for feat in selected_features:\n",
    "        for cat_var in categorical_features:\n",
    "            if feat.startswith(cat_var + '_'):\n",
    "                selected_cat_vars.add(cat_var)\n",
    "                break\n",
    "    \n",
    "    final_features = []\n",
    "    for feat in selected_features:\n",
    "        is_categorical = any(feat.startswith(cat_var + '_') for cat_var in categorical_features)\n",
    "        if not is_categorical:\n",
    "            final_features.append(feat)\n",
    "    \n",
    "    for feat in all_feature_names:\n",
    "        for cat_var in selected_cat_vars:\n",
    "            if feat.startswith(cat_var + '_') and feat not in final_features:\n",
    "                final_features.append(feat)\n",
    "                break\n",
    "    \n",
    "    return sorted(final_features)\n",
    "\n",
    "final_selected_features = expand_categorical_features(selected_features, feature_names, categorical_features)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURES FINALES (con todos los niveles de categóricas)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal features: {len(final_selected_features)}\")\n",
    "print(\"\\nFeatures seleccionadas:\")\n",
    "for feat in final_selected_features:\n",
    "    indicator = \"✓\" if feat in selected_features else \"+\"\n",
    "    print(f\"  {indicator} {feat}\")\n",
    "\n",
    "X_train_selected = X_train_df[final_selected_features]\n",
    "X_test_selected = X_test_df[final_selected_features]\n",
    "\n",
    "print(f\"\\nShape final - Train: {X_train_selected.shape} | Test: {X_test_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fase 1 - LazyPredict para Identificar Mejores Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EJECUTANDO LAZYPREDICT\n",
      "================================================================================\n",
      "\n",
      "Dataset: 6399 muestras, 5 features\n",
      "Esto puede tomar varios minutos...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a58903bd21459ba42f693e5bb0b332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 333, number of negative: 6066\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 15\n",
      "[LightGBM] [Info] Number of data points in the train set: 6399, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.052039 -> initscore=-2.902312\n",
      "[LightGBM] [Info] Start training from score -2.902312\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "================================================================================\n",
      "RESULTADOS DE LAZYPREDICT\n",
      "================================================================================\n",
      "\n",
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "NearestCentroid                    0.73               0.54     0.54      0.80   \n",
      "AdaBoostClassifier                 0.95               0.50     0.50      0.92   \n",
      "BernoulliNB                        0.95               0.50     0.50      0.92   \n",
      "CalibratedClassifierCV             0.95               0.50     0.50      0.92   \n",
      "DecisionTreeClassifier             0.95               0.50     0.50      0.92   \n",
      "DummyClassifier                    0.95               0.50     0.50      0.92   \n",
      "ExtraTreeClassifier                0.95               0.50     0.50      0.92   \n",
      "ExtraTreesClassifier               0.95               0.50     0.50      0.92   \n",
      "GaussianNB                         0.95               0.50     0.50      0.92   \n",
      "BaggingClassifier                  0.95               0.50     0.50      0.92   \n",
      "KNeighborsClassifier               0.95               0.50     0.50      0.92   \n",
      "LabelPropagation                   0.95               0.50     0.50      0.92   \n",
      "LinearDiscriminantAnalysis         0.95               0.50     0.50      0.92   \n",
      "LabelSpreading                     0.95               0.50     0.50      0.92   \n",
      "LinearSVC                          0.95               0.50     0.50      0.92   \n",
      "LogisticRegression                 0.95               0.50     0.50      0.92   \n",
      "PassiveAggressiveClassifier        0.95               0.50     0.50      0.92   \n",
      "Perceptron                         0.95               0.50     0.50      0.92   \n",
      "QuadraticDiscriminantAnalysis      0.95               0.50     0.50      0.92   \n",
      "RandomForestClassifier             0.95               0.50     0.50      0.92   \n",
      "RidgeClassifier                    0.95               0.50     0.50      0.92   \n",
      "RidgeClassifierCV                  0.95               0.50     0.50      0.92   \n",
      "SGDClassifier                      0.95               0.50     0.50      0.92   \n",
      "SVC                                0.95               0.50     0.50      0.92   \n",
      "XGBClassifier                      0.95               0.50     0.50      0.92   \n",
      "LGBMClassifier                     0.95               0.50     0.50      0.92   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "NearestCentroid                      0.02  \n",
      "AdaBoostClassifier                   0.13  \n",
      "BernoulliNB                          0.01  \n",
      "CalibratedClassifierCV               0.06  \n",
      "DecisionTreeClassifier               0.01  \n",
      "DummyClassifier                      0.01  \n",
      "ExtraTreeClassifier                  0.02  \n",
      "ExtraTreesClassifier                 0.18  \n",
      "GaussianNB                           0.02  \n",
      "BaggingClassifier                    0.04  \n",
      "KNeighborsClassifier                 0.04  \n",
      "LabelPropagation                     0.54  \n",
      "LinearDiscriminantAnalysis           0.02  \n",
      "LabelSpreading                       1.03  \n",
      "LinearSVC                            0.02  \n",
      "LogisticRegression                   0.02  \n",
      "PassiveAggressiveClassifier          0.02  \n",
      "Perceptron                           0.03  \n",
      "QuadraticDiscriminantAnalysis        0.02  \n",
      "RandomForestClassifier               0.22  \n",
      "RidgeClassifier                      0.02  \n",
      "RidgeClassifierCV                    0.03  \n",
      "SGDClassifier                        0.02  \n",
      "SVC                                  0.18  \n",
      "XGBClassifier                        0.13  \n",
      "LGBMClassifier                       0.09  \n"
     ]
    }
   ],
   "source": [
    "# Ejecutar LazyPredict\n",
    "print(\"=\" * 80)\n",
    "print(\"EJECUTANDO LAZYPREDICT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset: {X_train_selected.shape[0]} muestras, {X_train_selected.shape[1]} features\")\n",
    "print(\"Esto puede tomar varios minutos...\\n\")\n",
    "\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, predictions=True, random_state=42)\n",
    "models, predictions = clf.fit(X_train_selected, X_test_selected, y_train, y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTADOS DE LAZYPREDICT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exportación del Modelo Seleccionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOP 5 MODELOS POR F1-SCORE\n",
      "================================================================================\n",
      "\n",
      "                        Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                    \n",
      "AdaBoostClassifier          0.95               0.50     0.50      0.92   \n",
      "BernoulliNB                 0.95               0.50     0.50      0.92   \n",
      "CalibratedClassifierCV      0.95               0.50     0.50      0.92   \n",
      "DecisionTreeClassifier      0.95               0.50     0.50      0.92   \n",
      "DummyClassifier             0.95               0.50     0.50      0.92   \n",
      "\n",
      "                        Time Taken  \n",
      "Model                               \n",
      "AdaBoostClassifier            0.13  \n",
      "BernoulliNB                   0.01  \n",
      "CalibratedClassifierCV        0.06  \n",
      "DecisionTreeClassifier        0.01  \n",
      "DummyClassifier               0.01  \n",
      "\n",
      "================================================================================\n",
      "MODELOS SELECCIONADOS PARA FASE 2 (GridSearchCV)\n",
      "================================================================================\n",
      "\n",
      "Total modelos a optimizar: 9\n",
      "\n",
      "De LazyPredict (Top 5):\n",
      "1. AdaBoostClassifier - F1: 0.9229 | Accuracy: 0.9481\n",
      "2. BernoulliNB - F1: 0.9229 | Accuracy: 0.9481\n",
      "3. CalibratedClassifierCV - F1: 0.9229 | Accuracy: 0.9481\n",
      "4. DecisionTreeClassifier - F1: 0.9229 | Accuracy: 0.9481\n",
      "5. DummyClassifier - F1: 0.9229 | Accuracy: 0.9481\n",
      "\n",
      "Modelos adicionales solicitados:\n",
      "  • NearestCentroid - F1: 0.8022 | Accuracy: 0.7288\n",
      "  • LogisticRegression - F1: 0.9229 | Accuracy: 0.9481\n",
      "  • XGBClassifier - F1: 0.9229 | Accuracy: 0.9481\n",
      "  • RandomForestClassifier - F1: 0.9229 | Accuracy: 0.9481\n"
     ]
    }
   ],
   "source": [
    "# Top 5 modelos + modelos adicionales solicitados\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 5 MODELOS POR F1-SCORE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "top_5_models = models.nlargest(5, 'F1 Score')\n",
    "print(top_5_models)\n",
    "\n",
    "# Agregar modelos adicionales solicitados explícitamente\n",
    "modelos_adicionales = ['NearestCentroid', 'LogisticRegression', 'XGBClassifier', 'RandomForestClassifier']\n",
    "modelos_para_optimizar = list(set(list(top_5_models.index) + modelos_adicionales))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODELOS SELECCIONADOS PARA FASE 2 (GridSearchCV)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal modelos a optimizar: {len(modelos_para_optimizar)}\")\n",
    "print(\"\\nDe LazyPredict (Top 5):\")\n",
    "for i, model_name in enumerate(top_5_models.index, 1):\n",
    "    f1 = top_5_models.loc[model_name, 'F1 Score']\n",
    "    acc = top_5_models.loc[model_name, 'Accuracy']\n",
    "    print(f\"{i}. {model_name} - F1: {f1:.4f} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nModelos adicionales solicitados:\")\n",
    "for model_name in modelos_adicionales:\n",
    "    if model_name in models.index:\n",
    "        f1 = models.loc[model_name, 'F1 Score']\n",
    "        acc = models.loc[model_name, 'Accuracy']\n",
    "        print(f\"  • {model_name} - F1: {f1:.4f} | Accuracy: {acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"  • {model_name} - (no evaluado en LazyPredict)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## FASE 2: Optimización de Hiperparámetros con GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Funciones de evaluación definidas\n"
     ]
    }
   ],
   "source": [
    "# Funciones de evaluación\n",
    "def gini_coefficient(y_true, y_pred_proba):\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    return 2 * auc - 1\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba, metric='f1'):\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        if metric == 'f1':\n",
    "            score = f1_score(y_true, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y_true, y_pred, zero_division=0)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_true, y_pred)\n",
    "        scores.append(score)\n",
    "    \n",
    "    optimal_idx = np.argmax(scores)\n",
    "    return thresholds[optimal_idx], scores[optimal_idx]\n",
    "\n",
    "print(\"✓ Funciones de evaluación definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Grillas con parámetros de regularización/suavizado:\n",
      "  - AdaBoostClassifier: learning_rate + estimator (algoritmo SAMME)\n",
      "  - BernoulliNB: alpha (suavizado Laplace)\n",
      "  - CalibratedClassifierCV: estimator regularizado\n",
      "  - DecisionTreeClassifier: grilla reducida con parámetros de poda\n",
      "  - DummyClassifier: baseline\n",
      "  - NearestCentroid: shrink_threshold (regularización)\n",
      "  - LogisticRegression: C (regularización L1/L2) + class_weight\n",
      "  - XGBClassifier: reg_alpha/lambda + scale_pos_weight para desbalanceo\n",
      "  - RandomForestClassifier: poda + class_weight balanceado\n"
     ]
    }
   ],
   "source": [
    "# Grillas de hiperparámetros con parámetros de suavizado/regularización\n",
    "param_grids = {\n",
    "    'AdaBoostClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "        'algorithm': ['SAMME'],  # SAMME.R no está disponible en versiones recientes\n",
    "        'estimator': [\n",
    "            DecisionTreeClassifier(max_depth=1, random_state=42),  # Stumps\n",
    "            DecisionTreeClassifier(max_depth=2, random_state=42),\n",
    "            DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    'BernoulliNB': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],  # Suavizado de Laplace\n",
    "        'binarize': [0.0, 0.3, 0.5, 0.7, None],\n",
    "        'fit_prior': [True, False]\n",
    "    },\n",
    "    \n",
    "    'CalibratedClassifierCV': {\n",
    "        'method': ['sigmoid', 'isotonic'],\n",
    "        'cv': [3, 5],\n",
    "        'estimator': [\n",
    "            LogisticRegression(C=0.1, penalty='l2', solver='liblinear', random_state=42, max_iter=1000),\n",
    "            LogisticRegression(C=1.0, penalty='l2', solver='liblinear', random_state=42, max_iter=1000),\n",
    "            DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, random_state=42)\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    'DecisionTreeClassifier': {\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'min_samples_split': [10, 20, 50],\n",
    "        'min_samples_leaf': [5, 10, 20],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'ccp_alpha': [0.0, 0.01, 0.05]  # Poda de complejidad de costos\n",
    "    },\n",
    "    \n",
    "    'DummyClassifier': {\n",
    "        'strategy': ['stratified', 'most_frequent', 'prior']\n",
    "    },\n",
    "    \n",
    "    # NUEVOS MODELOS\n",
    "    'NearestCentroid': {\n",
    "        'metric': ['euclidean', 'manhattan'],\n",
    "        'shrink_threshold': [None, 0.1, 0.3, 0.5, 1.0, 2.0, 5.0]  # Regularización por encogimiento\n",
    "    },\n",
    "    \n",
    "    'LogisticRegression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],  # Inverso de fuerza de regularización\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear'],  # Compatible con l1 y l2\n",
    "        'class_weight': ['balanced', None],\n",
    "        'max_iter': [1000]\n",
    "    },\n",
    "    \n",
    "    'XGBClassifier': {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'subsample': [0.7, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 1.0],  # L1 regularization\n",
    "        'reg_lambda': [1, 2, 5],  # L2 regularization\n",
    "        'scale_pos_weight': [18.2],  # Ratio de desbalanceo (7583/416)\n",
    "        'random_state': [42],\n",
    "        'eval_metric': ['logloss']\n",
    "    },\n",
    "    \n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7, 10, None],\n",
    "        'min_samples_split': [10, 20, 50],\n",
    "        'min_samples_leaf': [5, 10, 20],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'class_weight': ['balanced', 'balanced_subsample', None],\n",
    "        'ccp_alpha': [0.0, 0.01, 0.05],  # Poda de complejidad\n",
    "        'random_state': [42]\n",
    "    }\n",
    "}\n",
    "\n",
    "model_mapping = {\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(random_state=42),\n",
    "    'BernoulliNB': BernoulliNB(),\n",
    "    'CalibratedClassifierCV': CalibratedClassifierCV(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n",
    "    'DummyClassifier': DummyClassifier(random_state=42),\n",
    "    'NearestCentroid': NearestCentroid(),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42),\n",
    "    'XGBClassifier': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "print(\"✓ Grillas con parámetros de regularización/suavizado:\")\n",
    "print(\"  - AdaBoostClassifier: learning_rate + estimator (algoritmo SAMME)\")\n",
    "print(\"  - BernoulliNB: alpha (suavizado Laplace)\")\n",
    "print(\"  - CalibratedClassifierCV: estimator regularizado\")\n",
    "print(\"  - DecisionTreeClassifier: grilla reducida con parámetros de poda\")\n",
    "print(\"  - DummyClassifier: baseline\")\n",
    "print(\"  - NearestCentroid: shrink_threshold (regularización)\")\n",
    "print(\"  - LogisticRegression: C (regularización L1/L2) + class_weight\")\n",
    "print(\"  - XGBClassifier: reg_alpha/lambda + scale_pos_weight para desbalanceo\")\n",
    "print(\"  - RandomForestClassifier: poda + class_weight balanceado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GRID SEARCH CON VALIDACIÓN CRUZADA\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[1/9] Optimizando: DecisionTreeClassifier\n",
      "================================================================================\n",
      "Combinaciones: 864 | Total fits: 4320\n",
      "✓ F1-Score CV: 0.1199\n",
      "Mejores parámetros: {'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "\n",
      "================================================================================\n",
      "[2/9] Optimizando: DummyClassifier\n",
      "================================================================================\n",
      "Combinaciones: 3 | Total fits: 15\n",
      "✓ F1-Score CV: 0.0649\n",
      "Mejores parámetros: {'strategy': 'stratified'}\n",
      "\n",
      "================================================================================\n",
      "[3/9] Optimizando: LogisticRegression\n",
      "================================================================================\n",
      "Combinaciones: 24 | Total fits: 120\n",
      "✓ F1-Score CV: 0.1421\n",
      "Mejores parámetros: {'C': 0.01, 'class_weight': 'balanced', 'max_iter': 1000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "\n",
      "================================================================================\n",
      "[4/9] Optimizando: NearestCentroid\n",
      "================================================================================\n",
      "Combinaciones: 14 | Total fits: 70\n",
      "✓ F1-Score CV: 0.1421\n",
      "Mejores parámetros: {'metric': 'euclidean', 'shrink_threshold': 2.0}\n",
      "\n",
      "================================================================================\n",
      "[5/9] Optimizando: CalibratedClassifierCV\n",
      "================================================================================\n",
      "Combinaciones: 12 | Total fits: 60\n",
      "✓ F1-Score CV: 0.0000\n",
      "Mejores parámetros: {'cv': 3, 'estimator': LogisticRegression(C=0.1, max_iter=1000, random_state=42, solver='liblinear'), 'method': 'sigmoid'}\n",
      "\n",
      "================================================================================\n",
      "[6/9] Optimizando: RandomForestClassifier\n",
      "================================================================================\n",
      "Combinaciones: 2430 | Total fits: 12150\n",
      "✓ F1-Score CV: 0.1421\n",
      "Mejores parámetros: {'ccp_alpha': 0.01, 'class_weight': 'balanced', 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 50, 'random_state': 42}\n",
      "\n",
      "================================================================================\n",
      "[7/9] Optimizando: BernoulliNB\n",
      "================================================================================\n",
      "Combinaciones: 80 | Total fits: 400\n",
      "✓ F1-Score CV: 0.1349\n",
      "Mejores parámetros: {'alpha': 10.0, 'binarize': 0.0, 'fit_prior': False}\n",
      "\n",
      "================================================================================\n",
      "[8/9] Optimizando: AdaBoostClassifier\n",
      "================================================================================\n",
      "Combinaciones: 36 | Total fits: 180\n",
      "✓ F1-Score CV: 0.0000\n",
      "Mejores parámetros: {'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=1, random_state=42), 'learning_rate': 0.01, 'n_estimators': 50}\n",
      "\n",
      "================================================================================\n",
      "[9/9] Optimizando: XGBClassifier\n",
      "================================================================================\n",
      "Combinaciones: 2187 | Total fits: 10935\n",
      "✓ F1-Score CV: 0.1297\n",
      "Mejores parámetros: {'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 18.2, 'subsample': 1.0}\n",
      "\n",
      "================================================================================\n",
      "Completado: 9/9 modelos\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV\n",
    "print(\"=\" * 80)\n",
    "print(\"GRID SEARCH CON VALIDACIÓN CRUZADA\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "optimized_models = {}\n",
    "grid_results = {}\n",
    "\n",
    "for i, model_name in enumerate(modelos_para_optimizar, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{i}/{len(modelos_para_optimizar)}] Optimizando: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if model_name not in param_grids or model_name not in model_mapping:\n",
    "        print(f\"⚠ No hay grilla para {model_name}, saltando...\")\n",
    "        continue\n",
    "    \n",
    "    base_model = model_mapping[model_name]\n",
    "    param_grid = param_grids[model_name]\n",
    "    \n",
    "    n_combinations = np.prod([len(v) if isinstance(v, list) else 1 for v in param_grid.values()])\n",
    "    print(f\"Combinaciones: {int(n_combinations)} | Total fits: {int(5 * n_combinations)}\")\n",
    "    \n",
    "    try:\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=f1_scorer,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "        \n",
    "        optimized_models[model_name] = grid_search.best_estimator_\n",
    "        grid_results[model_name] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score_cv': grid_search.best_score_\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ F1-Score CV: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"Mejores parámetros: {grid_search.best_params_}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {str(e)}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Completado: {len(optimized_models)}/{len(modelos_para_optimizar)} modelos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUACIÓN EN TEST SET\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Evaluando: DecisionTreeClassifier\n",
      "================================================================================\n",
      "Threshold: 0.560\n",
      "F1 (CV): 0.1199 | F1 (Test): 0.1277\n",
      "Accuracy: 0.7950 | Precision: 0.0819 | Recall: 0.2892\n",
      "AUC: 0.5398 | Gini: 0.0796\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1248 | FP:  269\n",
      "  FN:   59 | TP:   24\n",
      "\n",
      "================================================================================\n",
      "Evaluando: DummyClassifier\n",
      "================================================================================\n",
      "Threshold: 0.100\n",
      "F1 (CV): 0.0649 | F1 (Test): 0.0706\n",
      "Accuracy: 0.9012 | Precision: 0.0690 | Recall: 0.0723\n",
      "AUC: 0.5094 | Gini: 0.0189\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1436 | FP:   81\n",
      "  FN:   77 | TP:    6\n",
      "\n",
      "================================================================================\n",
      "Evaluando: LogisticRegression\n",
      "================================================================================\n",
      "Threshold: 0.510\n",
      "F1 (CV): 0.1421 | F1 (Test): 0.1307\n",
      "Accuracy: 0.7837 | Precision: 0.0825 | Recall: 0.3133\n",
      "AUC: 0.5358 | Gini: 0.0715\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1228 | FP:  289\n",
      "  FN:   57 | TP:   26\n",
      "\n",
      "================================================================================\n",
      "Evaluando: NearestCentroid\n",
      "================================================================================\n",
      "Threshold: 0.470\n",
      "F1 (CV): 0.1421 | F1 (Test): 0.1307\n",
      "Accuracy: 0.7837 | Precision: 0.0825 | Recall: 0.3133\n",
      "AUC: 0.5496 | Gini: 0.0992\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1228 | FP:  289\n",
      "  FN:   57 | TP:   26\n",
      "\n",
      "================================================================================\n",
      "Evaluando: CalibratedClassifierCV\n",
      "================================================================================\n",
      "Threshold: 0.100\n",
      "F1 (CV): 0.0000 | F1 (Test): 0.0381\n",
      "Accuracy: 0.9369 | Precision: 0.0909 | Recall: 0.0241\n",
      "AUC: 0.5143 | Gini: 0.0285\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1497 | FP:   20\n",
      "  FN:   81 | TP:    2\n",
      "\n",
      "================================================================================\n",
      "Evaluando: RandomForestClassifier\n",
      "================================================================================\n",
      "Threshold: 0.490\n",
      "F1 (CV): 0.1421 | F1 (Test): 0.1307\n",
      "Accuracy: 0.7837 | Precision: 0.0825 | Recall: 0.3133\n",
      "AUC: 0.5614 | Gini: 0.1227\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1228 | FP:  289\n",
      "  FN:   57 | TP:   26\n",
      "\n",
      "================================================================================\n",
      "Evaluando: BernoulliNB\n",
      "================================================================================\n",
      "Threshold: 0.560\n",
      "F1 (CV): 0.1349 | F1 (Test): 0.1307\n",
      "Accuracy: 0.7837 | Precision: 0.0825 | Recall: 0.3133\n",
      "AUC: 0.5346 | Gini: 0.0692\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1228 | FP:  289\n",
      "  FN:   57 | TP:   26\n",
      "\n",
      "================================================================================\n",
      "Evaluando: AdaBoostClassifier\n",
      "================================================================================\n",
      "Threshold: 0.100\n",
      "F1 (CV): 0.0000 | F1 (Test): 0.0986\n",
      "Accuracy: 0.0519 | Precision: 0.0519 | Recall: 1.0000\n",
      "AUC: 0.5000 | Gini: 0.0000\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN:    0 | FP: 1517\n",
      "  FN:    0 | TP:   83\n",
      "\n",
      "================================================================================\n",
      "Evaluando: XGBClassifier\n",
      "================================================================================\n",
      "Threshold: 0.530\n",
      "F1 (CV): 0.1297 | F1 (Test): 0.1307\n",
      "Accuracy: 0.7837 | Precision: 0.0825 | Recall: 0.3133\n",
      "AUC: 0.5396 | Gini: 0.0793\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1228 | FP:  289\n",
      "  FN:   57 | TP:   26\n",
      "\n",
      "================================================================================\n",
      "RESUMEN DE RESULTADOS\n",
      "================================================================================\n",
      "\n",
      "                Modelo  F1_CV  F1_Test  Accuracy  Precision  Recall  AUC  Gini  Threshold\n",
      "    LogisticRegression   0.14     0.13      0.78       0.08    0.31 0.54  0.07       0.51\n",
      "         XGBClassifier   0.13     0.13      0.78       0.08    0.31 0.54  0.08       0.53\n",
      "       NearestCentroid   0.14     0.13      0.78       0.08    0.31 0.55  0.10       0.47\n",
      "           BernoulliNB   0.13     0.13      0.78       0.08    0.31 0.53  0.07       0.56\n",
      "RandomForestClassifier   0.14     0.13      0.78       0.08    0.31 0.56  0.12       0.49\n",
      "DecisionTreeClassifier   0.12     0.13      0.80       0.08    0.29 0.54  0.08       0.56\n",
      "    AdaBoostClassifier   0.00     0.10      0.05       0.05    1.00 0.50  0.00       0.10\n",
      "       DummyClassifier   0.06     0.07      0.90       0.07    0.07 0.51  0.02       0.10\n",
      "CalibratedClassifierCV   0.00     0.04      0.94       0.09    0.02 0.51  0.03       0.10\n"
     ]
    }
   ],
   "source": [
    "# Evaluación en test set\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUACIÓN EN TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for model_name, model in optimized_models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluando: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test_selected)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        y_pred_proba = model.decision_function(X_test_selected)\n",
    "        y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())\n",
    "    else:\n",
    "        y_pred_proba = model.predict(X_test_selected).astype(float)\n",
    "    \n",
    "    optimal_threshold, _ = find_optimal_threshold(y_test, y_pred_proba)\n",
    "    y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_optimal)\n",
    "    precision = precision_score(y_test, y_pred_optimal, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_optimal, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_optimal, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        gini = gini_coefficient(y_test, y_pred_proba)\n",
    "    except:\n",
    "        auc = gini = np.nan\n",
    "    \n",
    "    test_results.append({\n",
    "        'Modelo': model_name,\n",
    "        'F1_CV': grid_results[model_name]['best_score_cv'],\n",
    "        'F1_Test': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'AUC': auc,\n",
    "        'Gini': gini,\n",
    "        'Threshold': optimal_threshold\n",
    "    })\n",
    "    \n",
    "    print(f\"Threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"F1 (CV): {grid_results[model_name]['best_score_cv']:.4f} | F1 (Test): {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f} | Gini: {gini:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "    print(f\"\\nMatriz de Confusión:\")\n",
    "    print(f\"  TN: {cm[0,0]:4d} | FP: {cm[0,1]:4d}\")\n",
    "    print(f\"  FN: {cm[1,0]:4d} | TP: {cm[1,1]:4d}\")\n",
    "\n",
    "df_results = pd.DataFrame(test_results).sort_values('F1_Test', ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESUMEN DE RESULTADOS\")\n",
    "print(f\"{'='*80}\")\n",
    "print()\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exportación del Modelo Seleccionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REENTRENAMIENTO CON DATASET COMPLETO\n",
      "================================================================================\n",
      "\n",
      "Dataset completo: (7999, 5)\n",
      "Siniestrados totales: 416 (5.20%)\n",
      "\n",
      "Mejores parámetros del DummyClassifier:\n",
      "  strategy: stratified\n",
      "\n",
      "✓ Modelo reentrenado con 7999 registros\n",
      "✓ Modelo actualizado en optimized_models\n"
     ]
    }
   ],
   "source": [
    "# Reentrenar DummyClassifier con TODO el dataset usando mejores hiperparámetros\n",
    "print(\"=\" * 80)\n",
    "print(\"REENTRENAMIENTO CON DATASET COMPLETO\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Combinar train y test\n",
    "X_full = pd.concat([X_train_selected, X_test_selected])\n",
    "y_full = pd.concat([y_train, y_test])\n",
    "\n",
    "print(f\"Dataset completo: {X_full.shape}\")\n",
    "print(f\"Siniestrados totales: {y_full.sum()} ({y_full.mean()*100:.2f}%)\")\n",
    "\n",
    "# Obtener los mejores parámetros del DummyClassifier\n",
    "best_params_dummy = grid_results['DummyClassifier']['best_params']\n",
    "print(f\"\\nMejores parámetros del DummyClassifier:\")\n",
    "for param, value in best_params_dummy.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Crear y entrenar modelo con todos los datos\n",
    "dummy_final = DummyClassifier(**best_params_dummy)\n",
    "dummy_final.fit(X_full, y_full)\n",
    "\n",
    "# Actualizar en el diccionario de modelos optimizados\n",
    "optimized_models['DummyClassifier'] = dummy_final\n",
    "\n",
    "print(f\"\\n✓ Modelo reentrenado con {len(X_full)} registros\")\n",
    "print(f\"✓ Modelo actualizado en optimized_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODELO EXPORTADO EXITOSAMENTE\n",
      "================================================================================\n",
      "\n",
      "Ruta: ../models/clasificacion_adicionales.pkl\n",
      "\n",
      "Modelo: DummyClassifier\n",
      "Mejores parámetros: {'strategy': 'stratified'}\n",
      "\n",
      "Métricas en Test Set:\n",
      "  F1-Score:  0.0706\n",
      "  Accuracy:  0.9012\n",
      "  Precision: 0.0690\n",
      "  Recall:    0.0723\n",
      "  AUC-ROC:   0.5094\n",
      "  Gini:      0.0189\n",
      "\n",
      "Threshold óptimo: 0.100\n",
      "\n",
      "Contenido del archivo:\n",
      "  • modelo: DummyClassifier optimizado\n",
      "  • preprocessor: ColumnTransformer (StandardScaler + OneHotEncoder)\n",
      "  • features_seleccionadas: ['2_o_mas_inquilinos_Si', 'estudios_area_Ciencias', 'estudios_area_Humanidades', 'estudios_area_Otro', 'extintor_incendios_Si']\n",
      "  • threshold_optimo: 0.100\n",
      "  • metricas: diccionario con todas las métricas\n",
      "  • mejores_parametros: {'strategy': 'stratified'}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Crear directorio models si no existe\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Preparar objeto para exportar\n",
    "modelo_export = {\n",
    "    'modelo': optimized_models['DummyClassifier'],\n",
    "    'preprocessor': preprocessor,\n",
    "    'features_seleccionadas': final_selected_features,\n",
    "    'threshold_optimo': df_results[df_results['Modelo'] == 'DummyClassifier']['Threshold'].values[0],\n",
    "    'metricas': {\n",
    "        'F1_CV': grid_results['DummyClassifier']['best_score_cv'],\n",
    "        'F1_Test': df_results[df_results['Modelo'] == 'DummyClassifier']['F1_Test'].values[0],\n",
    "        'Accuracy': df_results[df_results['Modelo'] == 'DummyClassifier']['Accuracy'].values[0],\n",
    "        'Precision': df_results[df_results['Modelo'] == 'DummyClassifier']['Precision'].values[0],\n",
    "        'Recall': df_results[df_results['Modelo'] == 'DummyClassifier']['Recall'].values[0],\n",
    "        'AUC': df_results[df_results['Modelo'] == 'DummyClassifier']['AUC'].values[0],\n",
    "        'Gini': df_results[df_results['Modelo'] == 'DummyClassifier']['Gini'].values[0]\n",
    "    },\n",
    "    'mejores_parametros': grid_results['DummyClassifier']['best_params']\n",
    "}\n",
    "\n",
    "# Exportar modelo\n",
    "model_path = '../models/clasificacion_adicionales.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(modelo_export, f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODELO EXPORTADO EXITOSAMENTE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nRuta: {model_path}\")\n",
    "print(f\"\\nModelo: DummyClassifier\")\n",
    "print(f\"Mejores parámetros: {modelo_export['mejores_parametros']}\")\n",
    "print(f\"\\nMétricas en Test Set:\")\n",
    "print(f\"  F1-Score:  {modelo_export['metricas']['F1_Test']:.4f}\")\n",
    "print(f\"  Accuracy:  {modelo_export['metricas']['Accuracy']:.4f}\")\n",
    "print(f\"  Precision: {modelo_export['metricas']['Precision']:.4f}\")\n",
    "print(f\"  Recall:    {modelo_export['metricas']['Recall']:.4f}\")\n",
    "print(f\"  AUC-ROC:   {modelo_export['metricas']['AUC']:.4f}\")\n",
    "print(f\"  Gini:      {modelo_export['metricas']['Gini']:.4f}\")\n",
    "print(f\"\\nThreshold óptimo: {modelo_export['threshold_optimo']:.3f}\")\n",
    "print(f\"\\nContenido del archivo:\")\n",
    "print(f\"  • modelo: DummyClassifier optimizado\")\n",
    "print(f\"  • preprocessor: ColumnTransformer (StandardScaler + OneHotEncoder)\")\n",
    "print(f\"  • features_seleccionadas: {final_selected_features}\")\n",
    "print(f\"  • threshold_optimo: {modelo_export['threshold_optimo']:.3f}\")\n",
    "print(f\"  • metricas: diccionario con todas las métricas\")\n",
    "print(f\"  • mejores_parametros: {modelo_export['mejores_parametros']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
