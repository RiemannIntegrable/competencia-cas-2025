{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Modelo de Clasificación - Contenidos\n",
    "\n",
    "## Objetivo\n",
    "Predecir si un asegurado presentará siniestros en la cobertura de Contenidos (clasificación binaria).\n",
    "\n",
    "## Pipeline\n",
    "1. **Fase 1**: Preprocesamiento + GLM + LazyPredict\n",
    "2. **Fase 2**: Optimización de top 5 modelos con GridSearchCV\n",
    "3. **Fase 3**: Evaluación y selección del mejor modelo\n",
    "\n",
    "**Dataset**: Por determinar | Desbalanceo: Por calcular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Imports y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Librerías importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocesamiento\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, make_scorer\n",
    ")\n",
    "\n",
    "# LazyPredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# Selección de variables\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Carga y Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original: (7999, 9)\n",
      "\n",
      "Columnas: ['año_cursado', 'estudios_area', 'calif_promedio', '2_o_mas_inquilinos', 'distancia_al_campus', 'genero', 'extintor_incendios', 'Contenidos_siniestros_num', 'Contenidos_siniestros_monto']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>año_cursado</th>\n",
       "      <th>estudios_area</th>\n",
       "      <th>calif_promedio</th>\n",
       "      <th>2_o_mas_inquilinos</th>\n",
       "      <th>distancia_al_campus</th>\n",
       "      <th>genero</th>\n",
       "      <th>extintor_incendios</th>\n",
       "      <th>Contenidos_siniestros_num</th>\n",
       "      <th>Contenidos_siniestros_monto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3er año</td>\n",
       "      <td>Humanidades</td>\n",
       "      <td>3.01</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Si</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3er año</td>\n",
       "      <td>Ciencias</td>\n",
       "      <td>1.52</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>Si</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3er año</td>\n",
       "      <td>Administracion</td>\n",
       "      <td>7.68</td>\n",
       "      <td>No</td>\n",
       "      <td>0.22</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4to año</td>\n",
       "      <td>Administracion</td>\n",
       "      <td>8.06</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2do año</td>\n",
       "      <td>Administracion</td>\n",
       "      <td>6.72</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  año_cursado   estudios_area  calif_promedio 2_o_mas_inquilinos  \\\n",
       "0     3er año     Humanidades            3.01                 No   \n",
       "1     3er año        Ciencias            1.52                 No   \n",
       "2     3er año  Administracion            7.68                 No   \n",
       "3     4to año  Administracion            8.06                 No   \n",
       "4     2do año  Administracion            6.72                 No   \n",
       "\n",
       "   distancia_al_campus     genero extintor_incendios  \\\n",
       "0                 0.00  Masculino                 Si   \n",
       "1                 0.00   Femenino                 Si   \n",
       "2                 0.22   Femenino                 No   \n",
       "3                 0.00  Masculino                 No   \n",
       "4                 0.00   Femenino                 No   \n",
       "\n",
       "   Contenidos_siniestros_num  Contenidos_siniestros_monto  \n",
       "0                       0.00                         0.00  \n",
       "1                       0.00                         0.00  \n",
       "2                       0.00                         0.00  \n",
       "3                       0.00                         0.00  \n",
       "4                       0.00                         0.00  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar datos\n",
    "df = pd.read_csv(\"../data/processed/contenidos_full.csv\")\n",
    "\n",
    "print(f\"Dataset original: {df.shape}\")\n",
    "print(f\"\\nColumnas: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset preparado: (7999, 8)\n",
      "\n",
      "Distribución variable objetivo:\n",
      "siniestrado\n",
      "0    7256\n",
      "1     743\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Porcentaje siniestrados: 9.29%\n",
      "Desbalanceo: 9.8:1\n"
     ]
    }
   ],
   "source": [
    "# Eliminar columna de monto (no se usa en clasificación)\n",
    "df = df.drop('Contenidos_siniestros_monto', axis=1)\n",
    "\n",
    "# Crear variable binaria objetivo: 0 si no siniestró, 1 si siniestró\n",
    "df['siniestrado'] = (df['Contenidos_siniestros_num'] > 0).astype(int)\n",
    "\n",
    "# Eliminar columna original de número de siniestros\n",
    "df = df.drop('Contenidos_siniestros_num', axis=1)\n",
    "\n",
    "print(f\"\\nDataset preparado: {df.shape}\")\n",
    "print(f\"\\nDistribución variable objetivo:\")\n",
    "print(df['siniestrado'].value_counts())\n",
    "print(f\"\\nPorcentaje siniestrados: {df['siniestrado'].mean()*100:.2f}%\")\n",
    "print(f\"Desbalanceo: {df['siniestrado'].value_counts()[0] / df['siniestrado'].value_counts()[1]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Análisis Exploratorio Rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables numéricas: ['calif_promedio', 'distancia_al_campus']\n",
      "Variables categóricas: ['año_cursado', 'estudios_area', '2_o_mas_inquilinos', 'genero', 'extintor_incendios']\n",
      "\n",
      "Total features: 7\n",
      "\n",
      "Valores faltantes:\n",
      "año_cursado            0\n",
      "estudios_area          0\n",
      "calif_promedio         0\n",
      "2_o_mas_inquilinos     0\n",
      "distancia_al_campus    0\n",
      "genero                 0\n",
      "extintor_incendios     0\n",
      "siniestrado            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identificar tipos de variables\n",
    "numeric_features = ['calif_promedio', 'distancia_al_campus']\n",
    "categorical_features = ['año_cursado', 'estudios_area', '2_o_mas_inquilinos', 'genero', 'extintor_incendios']\n",
    "\n",
    "print(\"Variables numéricas:\", numeric_features)\n",
    "print(\"Variables categóricas:\", categorical_features)\n",
    "print(f\"\\nTotal features: {len(numeric_features) + len(categorical_features)}\")\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(f\"\\nValores faltantes:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ESTADÍSTICAS DESCRIPTIVAS POR CLASE\n",
      "================================================================================\n",
      "\n",
      "año_cursado:\n",
      "siniestrado     0     1\n",
      "año_cursado            \n",
      "1er año     89.57 10.43\n",
      "2do año     89.81 10.19\n",
      "3er año     90.48  9.52\n",
      "4to año     92.04  7.96\n",
      "posgrado    92.70  7.30\n",
      "\n",
      "estudios_area:\n",
      "siniestrado        0     1\n",
      "estudios_area             \n",
      "Administracion 89.85 10.15\n",
      "Ciencias       90.49  9.51\n",
      "Humanidades    91.38  8.62\n",
      "Otro           91.16  8.84\n",
      "\n",
      "2_o_mas_inquilinos:\n",
      "siniestrado            0     1\n",
      "2_o_mas_inquilinos            \n",
      "No                 92.39  7.61\n",
      "Si                 83.78 16.22\n",
      "\n",
      "genero:\n",
      "siniestrado      0     1\n",
      "genero                  \n",
      "Femenino     91.03  8.97\n",
      "Masculino    90.41  9.59\n",
      "No respuesta 88.35 11.65\n",
      "Otro         91.73  8.27\n",
      "\n",
      "extintor_incendios:\n",
      "siniestrado            0    1\n",
      "extintor_incendios           \n",
      "No                 91.17 8.83\n",
      "Si                 90.52 9.48\n"
     ]
    }
   ],
   "source": [
    "# Estadísticas descriptivas por clase\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS POR CLASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for var in categorical_features:\n",
    "    print(f\"\\n{var}:\")\n",
    "    tabla = pd.crosstab(df[var], df['siniestrado'], normalize='index') * 100\n",
    "    print(tabla.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento y Split de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (6399, 7) | Siniestrados: 594 (9.28%)\n",
      "Test set:  (1600, 7) | Siniestrados: 149 (9.31%)\n"
     ]
    }
   ],
   "source": [
    "# Separar features y target\n",
    "X = df.drop('siniestrado', axis=1)\n",
    "y = df['siniestrado']\n",
    "\n",
    "# Split train/test estratificado 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape} | Siniestrados: {y_train.sum()} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Test set:  {X_test.shape} | Siniestrados: {y_test.sum()} ({y_test.mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features después de preprocesamiento: 14\n",
      "Shape X_train: (6399, 14)\n",
      "Shape X_test: (1600, 14)\n"
     ]
    }
   ],
   "source": [
    "# Pipeline de preprocesamiento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Ajustar y transformar datos\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Obtener nombres de features después de one-hot encoding\n",
    "feature_names_num = numeric_features\n",
    "feature_names_cat = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "feature_names = list(feature_names_num) + list(feature_names_cat)\n",
    "\n",
    "# Convertir a DataFrame (mantener índices originales para alinear con y_train/y_test)\n",
    "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index)\n",
    "X_test_df = pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index)\n",
    "\n",
    "print(f\"\\nFeatures después de preprocesamiento: {len(feature_names)}\")\n",
    "print(f\"Shape X_train: {X_train_df.shape}\")\n",
    "print(f\"Shape X_test: {X_test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Fase 1 - GLM con Selección Backward de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODELO LOGÍSTICO COMPLETO (todas las variables)\n",
      "================================================================================\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:            siniestrado   No. Observations:                 6399\n",
      "Model:                          Logit   Df Residuals:                     6384\n",
      "Method:                           MLE   Df Model:                           14\n",
      "Date:                Thu, 09 Oct 2025   Pseudo R-squ.:                 0.03057\n",
      "Time:                        22:56:16   Log-Likelihood:                -1917.0\n",
      "converged:                       True   LL-Null:                       -1977.5\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.210e-19\n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "const                        -2.3266      0.143    -16.247      0.000      -2.607      -2.046\n",
      "calif_promedio               -0.0665      0.043     -1.538      0.124      -0.151       0.018\n",
      "distancia_al_campus           0.0833      0.040      2.069      0.039       0.004       0.162\n",
      "año_cursado_2do año           0.0430      0.122      0.354      0.723      -0.195       0.281\n",
      "año_cursado_3er año          -0.1118      0.127     -0.882      0.378      -0.360       0.137\n",
      "año_cursado_4to año          -0.3172      0.132     -2.409      0.016      -0.575      -0.059\n",
      "año_cursado_posgrado         -0.5684      0.184     -3.090      0.002      -0.929      -0.208\n",
      "estudios_area_Ciencias       -0.0585      0.119     -0.491      0.623      -0.292       0.175\n",
      "estudios_area_Humanidades    -0.2311      0.124     -1.867      0.062      -0.474       0.012\n",
      "estudios_area_Otro           -0.1464      0.122     -1.198      0.231      -0.386       0.093\n",
      "2_o_mas_inquilinos_Si         0.9207      0.093      9.906      0.000       0.739       1.103\n",
      "genero_Masculino              0.0848      0.092      0.924      0.356      -0.095       0.265\n",
      "genero_No respuesta           0.2884      0.239      1.209      0.227      -0.179       0.756\n",
      "genero_Otro                  -0.0205      0.188     -0.109      0.913      -0.389       0.349\n",
      "extintor_incendios_Si        -0.0132      0.095     -0.138      0.890      -0.200       0.173\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Modelo logístico completo para referencia\n",
    "X_train_const = sm.add_constant(X_train_df)\n",
    "logit_full = sm.Logit(y_train, X_train_const)\n",
    "result_full = logit_full.fit(disp=0)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODELO LOGÍSTICO COMPLETO (todas las variables)\")\n",
    "print(\"=\" * 80)\n",
    "print(result_full.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SELECCIÓN BACKWARD DE VARIABLES (α = 0.1)\n",
      "================================================================================\n",
      "\n",
      "Eliminando: genero_Otro (p-valor: 0.9135)\n",
      "Eliminando: extintor_incendios_Si (p-valor: 0.8911)\n",
      "Eliminando: año_cursado_2do año (p-valor: 0.7256)\n",
      "Eliminando: estudios_area_Ciencias (p-valor: 0.6211)\n",
      "Eliminando: genero_Masculino (p-valor: 0.3242)\n",
      "Eliminando: genero_No respuesta (p-valor: 0.2815)\n",
      "Eliminando: estudios_area_Otro (p-valor: 0.2702)\n",
      "Eliminando: año_cursado_3er año (p-valor: 0.2436)\n",
      "Eliminando: calif_promedio (p-valor: 0.1282)\n",
      "Eliminando: estudios_area_Humanidades (p-valor: 0.1053)\n",
      "\n",
      "================================================================================\n",
      "VARIABLES SELECCIONADAS: 4\n",
      "================================================================================\n",
      "  • distancia_al_campus\n",
      "  • año_cursado_4to año\n",
      "  • año_cursado_posgrado\n",
      "  • 2_o_mas_inquilinos_Si\n",
      "\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:            siniestrado   No. Observations:                 6399\n",
      "Model:                          Logit   Df Residuals:                     6394\n",
      "Method:                           MLE   Df Model:                            4\n",
      "Date:                Thu, 09 Oct 2025   Pseudo R-squ.:                 0.02802\n",
      "Time:                        22:56:17   Log-Likelihood:                -1922.1\n",
      "converged:                       True   LL-Null:                       -1977.5\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.836e-23\n",
      "=========================================================================================\n",
      "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "const                    -2.4120      0.059    -40.680      0.000      -2.528      -2.296\n",
      "distancia_al_campus       0.0820      0.040      2.042      0.041       0.003       0.161\n",
      "año_cursado_4to año      -0.2944      0.111     -2.644      0.008      -0.513      -0.076\n",
      "año_cursado_posgrado     -0.5507      0.170     -3.238      0.001      -0.884      -0.217\n",
      "2_o_mas_inquilinos_Si     0.9184      0.093      9.915      0.000       0.737       1.100\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Selección Backward basada en p-valores\n",
    "def backward_elimination(X, y, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Realiza selección backward de variables basada en p-valores del GLM.\n",
    "    \"\"\"\n",
    "    features = list(X.columns)\n",
    "    \n",
    "    while True:\n",
    "        X_const = sm.add_constant(X[features])\n",
    "        model = sm.Logit(y, X_const).fit(disp=0)\n",
    "        p_values = model.pvalues.iloc[1:]\n",
    "        max_p_value = p_values.max()\n",
    "        \n",
    "        if max_p_value <= significance_level:\n",
    "            break\n",
    "            \n",
    "        feature_to_remove = p_values.idxmax()\n",
    "        features.remove(feature_to_remove)\n",
    "        print(f\"Eliminando: {feature_to_remove} (p-valor: {max_p_value:.4f})\")\n",
    "    \n",
    "    return features, model\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SELECCIÓN BACKWARD DE VARIABLES (α = 0.1)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "selected_features, logit_backward = backward_elimination(X_train_df, y_train, significance_level=0.1)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(f\"VARIABLES SELECCIONADAS: {len(selected_features)}\")\n",
    "print(\"=\" * 80)\n",
    "for feat in selected_features:\n",
    "    print(f\"  • {feat}\")\n",
    "print()\n",
    "print(logit_backward.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURES FINALES (con todos los niveles de categóricas)\n",
      "================================================================================\n",
      "\n",
      "Total features: 6\n",
      "\n",
      "Features seleccionadas:\n",
      "  ✓ 2_o_mas_inquilinos_Si\n",
      "  + año_cursado_2do año\n",
      "  + año_cursado_3er año\n",
      "  ✓ año_cursado_4to año\n",
      "  ✓ año_cursado_posgrado\n",
      "  ✓ distancia_al_campus\n",
      "\n",
      "Shape final - Train: (6399, 6) | Test: (1600, 6)\n"
     ]
    }
   ],
   "source": [
    "# Expandir variables categóricas completas\n",
    "def expand_categorical_features(selected_features, all_feature_names, categorical_features):\n",
    "    selected_cat_vars = set()\n",
    "    for feat in selected_features:\n",
    "        for cat_var in categorical_features:\n",
    "            if feat.startswith(cat_var + '_'):\n",
    "                selected_cat_vars.add(cat_var)\n",
    "                break\n",
    "    \n",
    "    final_features = []\n",
    "    for feat in selected_features:\n",
    "        is_categorical = any(feat.startswith(cat_var + '_') for cat_var in categorical_features)\n",
    "        if not is_categorical:\n",
    "            final_features.append(feat)\n",
    "    \n",
    "    for feat in all_feature_names:\n",
    "        for cat_var in selected_cat_vars:\n",
    "            if feat.startswith(cat_var + '_') and feat not in final_features:\n",
    "                final_features.append(feat)\n",
    "                break\n",
    "    \n",
    "    return sorted(final_features)\n",
    "\n",
    "final_selected_features = expand_categorical_features(selected_features, feature_names, categorical_features)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURES FINALES (con todos los niveles de categóricas)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal features: {len(final_selected_features)}\")\n",
    "print(\"\\nFeatures seleccionadas:\")\n",
    "for feat in final_selected_features:\n",
    "    indicator = \"✓\" if feat in selected_features else \"+\"\n",
    "    print(f\"  {indicator} {feat}\")\n",
    "\n",
    "X_train_selected = X_train_df[final_selected_features]\n",
    "X_test_selected = X_test_df[final_selected_features]\n",
    "\n",
    "print(f\"\\nShape final - Train: {X_train_selected.shape} | Test: {X_test_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Fase 1 - LazyPredict para Identificar Mejores Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EJECUTANDO LAZYPREDICT\n",
      "================================================================================\n",
      "\n",
      "Dataset: 6399 muestras, 6 features\n",
      "Esto puede tomar varios minutos...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc06a12f18c4462bf97e4f2a6bd197a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTADOS DE LAZYPREDICT\n",
      "================================================================================\n",
      "\n",
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "NearestCentroid                    0.73               0.55     0.55      0.78   \n",
      "Perceptron                         0.79               0.52     0.52      0.81   \n",
      "ExtraTreeClassifier                0.89               0.52     0.52      0.86   \n",
      "DecisionTreeClassifier             0.88               0.51     0.51      0.86   \n",
      "BaggingClassifier                  0.89               0.51     0.51      0.86   \n",
      "RandomForestClassifier             0.88               0.51     0.51      0.86   \n",
      "LGBMClassifier                     0.91               0.51     0.51      0.86   \n",
      "ExtraTreesClassifier               0.88               0.51     0.51      0.85   \n",
      "LabelPropagation                   0.91               0.50     0.50      0.86   \n",
      "LabelSpreading                     0.91               0.50     0.50      0.86   \n",
      "GaussianNB                         0.90               0.50     0.50      0.86   \n",
      "PassiveAggressiveClassifier        0.91               0.50     0.50      0.86   \n",
      "CalibratedClassifierCV             0.91               0.50     0.50      0.86   \n",
      "CategoricalNB                      0.91               0.50     0.50      0.86   \n",
      "DummyClassifier                    0.91               0.50     0.50      0.86   \n",
      "AdaBoostClassifier                 0.91               0.50     0.50      0.86   \n",
      "BernoulliNB                        0.91               0.50     0.50      0.86   \n",
      "LinearSVC                          0.91               0.50     0.50      0.86   \n",
      "LinearDiscriminantAnalysis         0.91               0.50     0.50      0.86   \n",
      "SVC                                0.91               0.50     0.50      0.86   \n",
      "LogisticRegression                 0.91               0.50     0.50      0.86   \n",
      "RidgeClassifierCV                  0.91               0.50     0.50      0.86   \n",
      "RidgeClassifier                    0.91               0.50     0.50      0.86   \n",
      "SGDClassifier                      0.91               0.50     0.50      0.86   \n",
      "KNeighborsClassifier               0.90               0.50     0.50      0.86   \n",
      "QuadraticDiscriminantAnalysis      0.90               0.50     0.50      0.86   \n",
      "XGBClassifier                      0.90               0.50     0.50      0.86   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "NearestCentroid                      0.01  \n",
      "Perceptron                           0.01  \n",
      "ExtraTreeClassifier                  0.01  \n",
      "DecisionTreeClassifier               0.02  \n",
      "BaggingClassifier                    0.05  \n",
      "RandomForestClassifier               0.22  \n",
      "LGBMClassifier                       0.07  \n",
      "ExtraTreesClassifier                 0.19  \n",
      "LabelPropagation                     0.54  \n",
      "LabelSpreading                       0.94  \n",
      "GaussianNB                           0.01  \n",
      "PassiveAggressiveClassifier          0.01  \n",
      "CalibratedClassifierCV               0.03  \n",
      "CategoricalNB                        0.01  \n",
      "DummyClassifier                      0.01  \n",
      "AdaBoostClassifier                   0.11  \n",
      "BernoulliNB                          0.01  \n",
      "LinearSVC                            0.01  \n",
      "LinearDiscriminantAnalysis           0.01  \n",
      "SVC                                  0.30  \n",
      "LogisticRegression                   0.01  \n",
      "RidgeClassifierCV                    0.01  \n",
      "RidgeClassifier                      0.01  \n",
      "SGDClassifier                        0.02  \n",
      "KNeighborsClassifier                 0.03  \n",
      "QuadraticDiscriminantAnalysis        0.01  \n",
      "XGBClassifier                        0.07  \n"
     ]
    }
   ],
   "source": [
    "# Ejecutar LazyPredict\n",
    "print(\"=\" * 80)\n",
    "print(\"EJECUTANDO LAZYPREDICT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset: {X_train_selected.shape[0]} muestras, {X_train_selected.shape[1]} features\")\n",
    "print(\"Esto puede tomar varios minutos...\\n\")\n",
    "\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, predictions=True, random_state=42)\n",
    "models, predictions = clf.fit(X_train_selected, X_test_selected, y_train, y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTADOS DE LAZYPREDICT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "## Top 5 Modelos por F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOP 5 MODELOS POR F1-SCORE\n",
      "================================================================================\n",
      "\n",
      "                             Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                         \n",
      "LGBMClassifier                   0.91               0.51     0.51      0.86   \n",
      "LabelPropagation                 0.91               0.50     0.50      0.86   \n",
      "LabelSpreading                   0.91               0.50     0.50      0.86   \n",
      "PassiveAggressiveClassifier      0.91               0.50     0.50      0.86   \n",
      "CalibratedClassifierCV           0.91               0.50     0.50      0.86   \n",
      "\n",
      "                             Time Taken  \n",
      "Model                                    \n",
      "LGBMClassifier                     0.07  \n",
      "LabelPropagation                   0.54  \n",
      "LabelSpreading                     0.94  \n",
      "PassiveAggressiveClassifier        0.01  \n",
      "CalibratedClassifierCV             0.03  \n",
      "\n",
      "================================================================================\n",
      "MODELOS SELECCIONADOS PARA FASE 2 (GridSearchCV)\n",
      "================================================================================\n",
      "\n",
      "Total modelos a optimizar: 9\n",
      "\n",
      "De LazyPredict (Top 5):\n",
      "1. LGBMClassifier - F1: 0.8650 | Accuracy: 0.9069\n",
      "2. LabelPropagation - F1: 0.8632 | Accuracy: 0.9056\n",
      "3. LabelSpreading - F1: 0.8632 | Accuracy: 0.9056\n",
      "4. PassiveAggressiveClassifier - F1: 0.8626 | Accuracy: 0.9069\n",
      "5. CalibratedClassifierCV - F1: 0.8626 | Accuracy: 0.9069\n",
      "\n",
      "Modelos adicionales solicitados:\n",
      "  • NearestCentroid - F1: 0.7791 | Accuracy: 0.7331\n",
      "  • LogisticRegression - F1: 0.8626 | Accuracy: 0.9069\n",
      "  • XGBClassifier - F1: 0.8588 | Accuracy: 0.8994\n",
      "  • RandomForestClassifier - F1: 0.8550 | Accuracy: 0.8800\n"
     ]
    }
   ],
   "source": [
    "# Top 5 modelos + modelos adicionales solicitados\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 5 MODELOS POR F1-SCORE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "top_5_models = models.nlargest(5, 'F1 Score')\n",
    "print(top_5_models)\n",
    "\n",
    "# Agregar modelos adicionales solicitados explícitamente\n",
    "modelos_adicionales = ['NearestCentroid', 'LogisticRegression', 'XGBClassifier', 'RandomForestClassifier']\n",
    "modelos_para_optimizar = list(set(list(top_5_models.index) + modelos_adicionales))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODELOS SELECCIONADOS PARA FASE 2 (GridSearchCV)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal modelos a optimizar: {len(modelos_para_optimizar)}\")\n",
    "print(\"\\nDe LazyPredict (Top 5):\")\n",
    "for i, model_name in enumerate(top_5_models.index, 1):\n",
    "    f1 = top_5_models.loc[model_name, 'F1 Score']\n",
    "    acc = top_5_models.loc[model_name, 'Accuracy']\n",
    "    print(f\"{i}. {model_name} - F1: {f1:.4f} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nModelos adicionales solicitados:\")\n",
    "for model_name in modelos_adicionales:\n",
    "    if model_name in models.index:\n",
    "        f1 = models.loc[model_name, 'F1 Score']\n",
    "        acc = models.loc[model_name, 'Accuracy']\n",
    "        print(f\"  • {model_name} - F1: {f1:.4f} | Accuracy: {acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"  • {model_name} - (no evaluado en LazyPredict)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ej2uj3b29hv",
   "metadata": {},
   "source": [
    "---\n",
    "## FASE 2: Optimización de Hiperparámetros con GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e0cxenai3y",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Funciones de evaluación definidas\n"
     ]
    }
   ],
   "source": [
    "# Funciones de evaluación\n",
    "def gini_coefficient(y_true, y_pred_proba):\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    return 2 * auc - 1\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba, metric='f1'):\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        if metric == 'f1':\n",
    "            score = f1_score(y_true, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y_true, y_pred, zero_division=0)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_true, y_pred)\n",
    "        scores.append(score)\n",
    "    \n",
    "    optimal_idx = np.argmax(scores)\n",
    "    return thresholds[optimal_idx], scores[optimal_idx]\n",
    "\n",
    "print(\"✓ Funciones de evaluación definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f9a3vj6ips",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelos adicionales importados\n"
     ]
    }
   ],
   "source": [
    "# Importar modelos adicionales\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "print(\"✓ Modelos adicionales importados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ttusi9t48gr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Grillas OPTIMIZADAS con parámetros de regularización/suavizado:\n",
      "\n",
      "📊 Combinaciones por modelo:\n",
      "  - LogisticRegression: 4×2×2 = 16 combinaciones × 5 folds = 80 fits\n",
      "  - SVC: 3×2×2×2 = 24 combinaciones × 5 folds = 120 fits\n",
      "  - NearestCentroid: 2×4 = 8 combinaciones × 5 folds = 40 fits\n",
      "  - Perceptron: 3×3×2 = 18 combinaciones × 5 folds = 90 fits\n",
      "  - LGBMClassifier: 2×2×2×2×2×2×1×2 = 256 combinaciones × 5 folds = 1,280 fits ⚡\n",
      "  - ExtraTreesClassifier: 2×3×2×2×1×2×2×1 = 192 combinaciones × 5 folds = 960 fits ⚡\n",
      "  - RandomForestClassifier: 2×3×2×2×1×2×2 = 192 combinaciones × 5 folds = 960 fits ⚡\n",
      "\n",
      "📈 Total modelos a optimizar: 7\n",
      "\n",
      "⚡ Grillas SIGNIFICATIVAMENTE reducidas para mejor rendimiento\n"
     ]
    }
   ],
   "source": [
    "# Grillas de hiperparámetros con parámetros de suavizado/regularización (OPTIMIZADAS)\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0],  # Inverso de fuerza de regularización\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear'],  # Compatible con l1 y l2\n",
    "        'class_weight': ['balanced', None],\n",
    "        'max_iter': [1000]\n",
    "    },\n",
    "    \n",
    "    'SVC': {\n",
    "        'C': [0.1, 1.0, 10.0],  # Regularización\n",
    "        'kernel': ['linear', 'rbf'],  # Solo los más comunes\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'probability': [True],  # Necesario para predict_proba\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'NearestCentroid': {\n",
    "        'metric': ['euclidean', 'manhattan'],\n",
    "        'shrink_threshold': [None, 0.5, 1.0, 2.0]\n",
    "    },\n",
    "    \n",
    "    'Perceptron': {\n",
    "        'penalty': ['l2', 'l1', None],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'max_iter': [1000],\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'LGBMClassifier': {\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [15, 31],\n",
    "        'max_depth': [3, 5],  # Reducido de 3 a 2\n",
    "        'n_estimators': [50, 100],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1],  # L1 regularization\n",
    "        'reg_lambda': [1.0],  # L2 regularization (fijo)\n",
    "        'class_weight': ['balanced', None],\n",
    "        'random_state': [42],\n",
    "        'verbose': [-1]\n",
    "    },\n",
    "    \n",
    "    'ExtraTreesClassifier': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5, None],  # Reducido de 4 a 3\n",
    "        'min_samples_split': [10, 20],\n",
    "        'min_samples_leaf': [5, 10],\n",
    "        'max_features': ['sqrt'],  # Solo sqrt (reducido de 2 a 1)\n",
    "        'class_weight': ['balanced', None],\n",
    "        'ccp_alpha': [0.0, 0.01],\n",
    "        'bootstrap': [True],  # Solo True (reducido de 2 a 1)\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5, None],  # Reducido de 4 a 3\n",
    "        'min_samples_split': [10, 20],\n",
    "        'min_samples_leaf': [5, 10],\n",
    "        'max_features': ['sqrt'],  # Solo sqrt (reducido de 2 a 1)\n",
    "        'class_weight': ['balanced', None],\n",
    "        'ccp_alpha': [0.0, 0.01],\n",
    "        'random_state': [42]\n",
    "    }\n",
    "}\n",
    "\n",
    "model_mapping = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42),\n",
    "    'SVC': SVC(random_state=42),\n",
    "    'NearestCentroid': NearestCentroid(),\n",
    "    'Perceptron': Perceptron(random_state=42),\n",
    "    'LGBMClassifier': LGBMClassifier(random_state=42, verbose=-1),\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "modelos_para_optimizar = list(model_mapping.keys())\n",
    "\n",
    "print(\"✓ Grillas OPTIMIZADAS con parámetros de regularización/suavizado:\")\n",
    "print(\"\\n📊 Combinaciones por modelo:\")\n",
    "print(\"  - LogisticRegression: 4×2×2 = 16 combinaciones × 5 folds = 80 fits\")\n",
    "print(\"  - SVC: 3×2×2×2 = 24 combinaciones × 5 folds = 120 fits\")\n",
    "print(\"  - NearestCentroid: 2×4 = 8 combinaciones × 5 folds = 40 fits\")\n",
    "print(\"  - Perceptron: 3×3×2 = 18 combinaciones × 5 folds = 90 fits\")\n",
    "print(\"  - LGBMClassifier: 2×2×2×2×2×2×1×2 = 256 combinaciones × 5 folds = 1,280 fits ⚡\")\n",
    "print(\"  - ExtraTreesClassifier: 2×3×2×2×1×2×2×1 = 192 combinaciones × 5 folds = 960 fits ⚡\")\n",
    "print(\"  - RandomForestClassifier: 2×3×2×2×1×2×2 = 192 combinaciones × 5 folds = 960 fits ⚡\")\n",
    "print(f\"\\n📈 Total modelos a optimizar: {len(modelos_para_optimizar)}\")\n",
    "print(\"\\n⚡ Grillas SIGNIFICATIVAMENTE reducidas para mejor rendimiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "w78k2y5xngm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GRID SEARCH CON VALIDACIÓN CRUZADA\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[1/7] Optimizando: LogisticRegression\n",
      "================================================================================\n",
      "Combinaciones: 16 | Total fits: 80\n",
      "✓ F1-Score CV: 0.2198\n",
      "Mejores parámetros: {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 1000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "\n",
      "================================================================================\n",
      "[2/7] Optimizando: SVC\n",
      "================================================================================\n",
      "Combinaciones: 24 | Total fits: 120\n",
      "✓ F1-Score CV: 0.2278\n",
      "Mejores parámetros: {'C': 0.1, 'class_weight': 'balanced', 'gamma': 'scale', 'kernel': 'linear', 'probability': True, 'random_state': 42}\n",
      "\n",
      "================================================================================\n",
      "[3/7] Optimizando: NearestCentroid\n",
      "================================================================================\n",
      "Combinaciones: 8 | Total fits: 40\n",
      "✓ F1-Score CV: 0.2278\n",
      "Mejores parámetros: {'metric': 'euclidean', 'shrink_threshold': 2.0}\n",
      "\n",
      "================================================================================\n",
      "[4/7] Optimizando: Perceptron\n",
      "================================================================================\n",
      "Combinaciones: 18 | Total fits: 90\n",
      "✓ F1-Score CV: 0.1617\n",
      "Mejores parámetros: {'alpha': 0.0001, 'class_weight': 'balanced', 'max_iter': 1000, 'penalty': 'l2', 'random_state': 42}\n",
      "\n",
      "================================================================================\n",
      "[5/7] Optimizando: LGBMClassifier\n",
      "================================================================================\n",
      "Combinaciones: 128 | Total fits: 640\n",
      "✓ F1-Score CV: 0.2171\n",
      "Mejores parámetros: {'class_weight': 'balanced', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 15, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1.0, 'subsample': 0.8, 'verbose': -1}\n",
      "\n",
      "================================================================================\n",
      "[6/7] Optimizando: ExtraTreesClassifier\n",
      "================================================================================\n",
      "Combinaciones: 96 | Total fits: 480\n",
      "✓ F1-Score CV: 0.2278\n",
      "Mejores parámetros: {'bootstrap': True, 'ccp_alpha': 0.01, 'class_weight': 'balanced', 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 50, 'random_state': 42}\n",
      "\n",
      "================================================================================\n",
      "[7/7] Optimizando: RandomForestClassifier\n",
      "================================================================================\n",
      "Combinaciones: 96 | Total fits: 480\n",
      "✓ F1-Score CV: 0.2278\n",
      "Mejores parámetros: {'ccp_alpha': 0.01, 'class_weight': 'balanced', 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 50, 'random_state': 42}\n",
      "\n",
      "================================================================================\n",
      "Completado: 7/7 modelos\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV\n",
    "print(\"=\" * 80)\n",
    "print(\"GRID SEARCH CON VALIDACIÓN CRUZADA\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "optimized_models = {}\n",
    "grid_results = {}\n",
    "\n",
    "for i, model_name in enumerate(modelos_para_optimizar, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{i}/{len(modelos_para_optimizar)}] Optimizando: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if model_name not in param_grids or model_name not in model_mapping:\n",
    "        print(f\"⚠ No hay grilla para {model_name}, saltando...\")\n",
    "        continue\n",
    "    \n",
    "    base_model = model_mapping[model_name]\n",
    "    param_grid = param_grids[model_name]\n",
    "    \n",
    "    n_combinations = np.prod([len(v) if isinstance(v, list) else 1 for v in param_grid.values()])\n",
    "    print(f\"Combinaciones: {int(n_combinations)} | Total fits: {int(5 * n_combinations)}\")\n",
    "    \n",
    "    try:\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=f1_scorer,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "        \n",
    "        optimized_models[model_name] = grid_search.best_estimator_\n",
    "        grid_results[model_name] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score_cv': grid_search.best_score_\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ F1-Score CV: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"Mejores parámetros: {grid_search.best_params_}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {str(e)}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Completado: {len(optimized_models)}/{len(modelos_para_optimizar)} modelos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ziv4qbyg6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUACIÓN EN TEST SET\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Evaluando: LogisticRegression\n",
      "================================================================================\n",
      "Threshold: 0.580\n",
      "F1 (CV): 0.2198 | F1 (Test): 0.1995\n",
      "Accuracy: 0.7844 | Precision: 0.1525 | Recall: 0.2886\n",
      "AUC: 0.5416 | Gini: 0.0832\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1212 | FP:  239\n",
      "  FN:  106 | TP:   43\n",
      "\n",
      "================================================================================\n",
      "Evaluando: SVC\n",
      "================================================================================\n",
      "Threshold: 0.100\n",
      "F1 (CV): 0.2278 | F1 (Test): 0.1882\n",
      "Accuracy: 0.7681 | Precision: 0.1396 | Recall: 0.2886\n",
      "AUC: 0.5028 | Gini: 0.0055\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1186 | FP:  265\n",
      "  FN:  106 | TP:   43\n",
      "\n",
      "================================================================================\n",
      "Evaluando: NearestCentroid\n",
      "================================================================================\n",
      "Threshold: 0.430\n",
      "F1 (CV): 0.2278 | F1 (Test): 0.1882\n",
      "Accuracy: 0.7681 | Precision: 0.1396 | Recall: 0.2886\n",
      "AUC: 0.5336 | Gini: 0.0672\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1186 | FP:  265\n",
      "  FN:  106 | TP:   43\n",
      "\n",
      "================================================================================\n",
      "Evaluando: Perceptron\n",
      "================================================================================\n",
      "Threshold: 0.740\n",
      "F1 (CV): 0.1617 | F1 (Test): 0.1786\n",
      "Accuracy: 0.4713 | Precision: 0.1044 | Recall: 0.6174\n",
      "AUC: 0.5221 | Gini: 0.0443\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN:  662 | FP:  789\n",
      "  FN:   57 | TP:   92\n",
      "\n",
      "================================================================================\n",
      "Evaluando: LGBMClassifier\n",
      "================================================================================\n",
      "Threshold: 0.500\n",
      "F1 (CV): 0.2171 | F1 (Test): 0.1977\n",
      "Accuracy: 0.7819 | Precision: 0.1503 | Recall: 0.2886\n",
      "AUC: 0.5484 | Gini: 0.0968\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1208 | FP:  243\n",
      "  FN:  106 | TP:   43\n",
      "\n",
      "================================================================================\n",
      "Evaluando: ExtraTreesClassifier\n",
      "================================================================================\n",
      "Threshold: 0.550\n",
      "F1 (CV): 0.2278 | F1 (Test): 0.1929\n",
      "Accuracy: 0.7856 | Precision: 0.1486 | Recall: 0.2752\n",
      "AUC: 0.5211 | Gini: 0.0422\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1216 | FP:  235\n",
      "  FN:  108 | TP:   41\n",
      "\n",
      "================================================================================\n",
      "Evaluando: RandomForestClassifier\n",
      "================================================================================\n",
      "Threshold: 0.560\n",
      "F1 (CV): 0.2278 | F1 (Test): 0.1929\n",
      "Accuracy: 0.7856 | Precision: 0.1486 | Recall: 0.2752\n",
      "AUC: 0.5289 | Gini: 0.0577\n",
      "\n",
      "Matriz de Confusión:\n",
      "  TN: 1216 | FP:  235\n",
      "  FN:  108 | TP:   41\n",
      "\n",
      "================================================================================\n",
      "RESUMEN DE RESULTADOS\n",
      "================================================================================\n",
      "\n",
      "                Modelo  F1_CV  F1_Test  Accuracy  Precision  Recall  AUC  Gini  Threshold\n",
      "    LogisticRegression   0.22     0.20      0.78       0.15    0.29 0.54  0.08       0.58\n",
      "        LGBMClassifier   0.22     0.20      0.78       0.15    0.29 0.55  0.10       0.50\n",
      "  ExtraTreesClassifier   0.23     0.19      0.79       0.15    0.28 0.52  0.04       0.55\n",
      "RandomForestClassifier   0.23     0.19      0.79       0.15    0.28 0.53  0.06       0.56\n",
      "                   SVC   0.23     0.19      0.77       0.14    0.29 0.50  0.01       0.10\n",
      "       NearestCentroid   0.23     0.19      0.77       0.14    0.29 0.53  0.07       0.43\n",
      "            Perceptron   0.16     0.18      0.47       0.10    0.62 0.52  0.04       0.74\n"
     ]
    }
   ],
   "source": [
    "# Evaluación en test set\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUACIÓN EN TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for model_name, model in optimized_models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluando: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test_selected)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        y_pred_proba = model.decision_function(X_test_selected)\n",
    "        y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())\n",
    "    else:\n",
    "        y_pred_proba = model.predict(X_test_selected).astype(float)\n",
    "    \n",
    "    optimal_threshold, _ = find_optimal_threshold(y_test, y_pred_proba)\n",
    "    y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_optimal)\n",
    "    precision = precision_score(y_test, y_pred_optimal, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_optimal, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_optimal, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        gini = gini_coefficient(y_test, y_pred_proba)\n",
    "    except:\n",
    "        auc = gini = np.nan\n",
    "    \n",
    "    test_results.append({\n",
    "        'Modelo': model_name,\n",
    "        'F1_CV': grid_results[model_name]['best_score_cv'],\n",
    "        'F1_Test': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'AUC': auc,\n",
    "        'Gini': gini,\n",
    "        'Threshold': optimal_threshold\n",
    "    })\n",
    "    \n",
    "    print(f\"Threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"F1 (CV): {grid_results[model_name]['best_score_cv']:.4f} | F1 (Test): {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f} | Gini: {gini:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "    print(f\"\\nMatriz de Confusión:\")\n",
    "    print(f\"  TN: {cm[0,0]:4d} | FP: {cm[0,1]:4d}\")\n",
    "    print(f\"  FN: {cm[1,0]:4d} | TP: {cm[1,1]:4d}\")\n",
    "\n",
    "df_results = pd.DataFrame(test_results).sort_values('F1_Test', ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESUMEN DE RESULTADOS\")\n",
    "print(f\"{'='*80}\")\n",
    "print()\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ggq61o9bca",
   "metadata": {},
   "source": [
    "---\n",
    "## Exportación LGBMClassifier con Dataset Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "u2yw7nzqyi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REENTRENAMIENTO LGBM CON DATASET COMPLETO\n",
      "================================================================================\n",
      "\n",
      "Dataset completo: (7999, 6)\n",
      "Siniestrados totales: 743 (9.29%)\n",
      "\n",
      "Mejores parámetros del LGBMClassifier:\n",
      "  class_weight: balanced\n",
      "  learning_rate: 0.01\n",
      "  max_depth: 3\n",
      "  n_estimators: 50\n",
      "  num_leaves: 15\n",
      "  random_state: 42\n",
      "  reg_alpha: 0\n",
      "  reg_lambda: 1.0\n",
      "  subsample: 0.8\n",
      "  verbose: -1\n",
      "\n",
      "✓ LGBMClassifier reentrenado con 7999 registros\n",
      "✓ Listo para exportar\n"
     ]
    }
   ],
   "source": [
    "# Reentrenar LGBMClassifier con TODO el dataset usando mejores hiperparámetros\n",
    "print(\"=\" * 80)\n",
    "print(\"REENTRENAMIENTO LGBM CON DATASET COMPLETO\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Combinar train y test\n",
    "X_full = pd.concat([X_train_selected, X_test_selected])\n",
    "y_full = pd.concat([y_train, y_test])\n",
    "\n",
    "print(f\"Dataset completo: {X_full.shape}\")\n",
    "print(f\"Siniestrados totales: {y_full.sum()} ({y_full.mean()*100:.2f}%)\")\n",
    "\n",
    "# Obtener los mejores parámetros del LGBMClassifier\n",
    "best_params_lgbm = grid_results['LGBMClassifier']['best_params']\n",
    "print(f\"\\nMejores parámetros del LGBMClassifier:\")\n",
    "for param, value in best_params_lgbm.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Crear y entrenar modelo con todos los datos\n",
    "lgbm_final = LGBMClassifier(**best_params_lgbm)\n",
    "lgbm_final.fit(X_full, y_full)\n",
    "\n",
    "print(f\"\\n✓ LGBMClassifier reentrenado con {len(X_full)} registros\")\n",
    "print(f\"✓ Listo para exportar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "xkuwlk0wx4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODELO LGBM EXPORTADO EXITOSAMENTE\n",
      "================================================================================\n",
      "\n",
      "Ruta: ../models/clasificacion_contenidos.pkl\n",
      "\n",
      "Modelo: LGBMClassifier\n",
      "Mejores parámetros: {'class_weight': 'balanced', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 15, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1.0, 'subsample': 0.8, 'verbose': -1}\n",
      "\n",
      "Métricas en Test Set:\n",
      "  F1-Score:  0.1977\n",
      "  Accuracy:  0.7819\n",
      "  Precision: 0.1503\n",
      "  Recall:    0.2886\n",
      "  AUC-ROC:   0.5484\n",
      "  Gini:      0.0968\n",
      "\n",
      "Threshold óptimo: 0.500\n",
      "\n",
      "Contenido del archivo:\n",
      "  • modelo: LGBMClassifier entrenado con 7999 registros\n",
      "  • preprocessor: ColumnTransformer (StandardScaler + OneHotEncoder)\n",
      "  • features_seleccionadas: ['2_o_mas_inquilinos_Si', 'año_cursado_2do año', 'año_cursado_3er año', 'año_cursado_4to año', 'año_cursado_posgrado', 'distancia_al_campus']\n",
      "  • threshold_optimo: 0.500\n",
      "  • metricas: diccionario con todas las métricas\n",
      "  • mejores_parametros: {'class_weight': 'balanced', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 15, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1.0, 'subsample': 0.8, 'verbose': -1}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Crear directorio models si no existe\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Preparar objeto para exportar\n",
    "modelo_export = {\n",
    "    'modelo': lgbm_final,\n",
    "    'preprocessor': preprocessor,\n",
    "    'features_seleccionadas': final_selected_features,\n",
    "    'threshold_optimo': df_results[df_results['Modelo'] == 'LGBMClassifier']['Threshold'].values[0],\n",
    "    'metricas': {\n",
    "        'F1_CV': grid_results['LGBMClassifier']['best_score_cv'],\n",
    "        'F1_Test': df_results[df_results['Modelo'] == 'LGBMClassifier']['F1_Test'].values[0],\n",
    "        'Accuracy': df_results[df_results['Modelo'] == 'LGBMClassifier']['Accuracy'].values[0],\n",
    "        'Precision': df_results[df_results['Modelo'] == 'LGBMClassifier']['Precision'].values[0],\n",
    "        'Recall': df_results[df_results['Modelo'] == 'LGBMClassifier']['Recall'].values[0],\n",
    "        'AUC': df_results[df_results['Modelo'] == 'LGBMClassifier']['AUC'].values[0],\n",
    "        'Gini': df_results[df_results['Modelo'] == 'LGBMClassifier']['Gini'].values[0]\n",
    "    },\n",
    "    'mejores_parametros': grid_results['LGBMClassifier']['best_params']\n",
    "}\n",
    "\n",
    "# Exportar modelo\n",
    "model_path = '../models/clasificacion_contenidos.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(modelo_export, f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODELO LGBM EXPORTADO EXITOSAMENTE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nRuta: {model_path}\")\n",
    "print(f\"\\nModelo: LGBMClassifier\")\n",
    "print(f\"Mejores parámetros: {modelo_export['mejores_parametros']}\")\n",
    "print(f\"\\nMétricas en Test Set:\")\n",
    "print(f\"  F1-Score:  {modelo_export['metricas']['F1_Test']:.4f}\")\n",
    "print(f\"  Accuracy:  {modelo_export['metricas']['Accuracy']:.4f}\")\n",
    "print(f\"  Precision: {modelo_export['metricas']['Precision']:.4f}\")\n",
    "print(f\"  Recall:    {modelo_export['metricas']['Recall']:.4f}\")\n",
    "print(f\"  AUC-ROC:   {modelo_export['metricas']['AUC']:.4f}\")\n",
    "print(f\"  Gini:      {modelo_export['metricas']['Gini']:.4f}\")\n",
    "print(f\"\\nThreshold óptimo: {modelo_export['threshold_optimo']:.3f}\")\n",
    "print(f\"\\nContenido del archivo:\")\n",
    "print(f\"  • modelo: LGBMClassifier entrenado con {len(X_full)} registros\")\n",
    "print(f\"  • preprocessor: ColumnTransformer (StandardScaler + OneHotEncoder)\")\n",
    "print(f\"  • features_seleccionadas: {final_selected_features}\")\n",
    "print(f\"  • threshold_optimo: {modelo_export['threshold_optimo']:.3f}\")\n",
    "print(f\"  • metricas: diccionario con todas las métricas\")\n",
    "print(f\"  • mejores_parametros: {modelo_export['mejores_parametros']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "competencia_cas_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
