{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score, average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.ensemble import BalancedRandomForestClassifier, BalancedBaggingClassifier, EasyEnsembleClassifier\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LGBM_AVAILABLE = True\n",
    "except:\n",
    "    LGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7999, 9)\n",
      "Gastos_Medicos_RC_siniestros_num\n",
      "0.00    7816\n",
      "1.00     179\n",
      "2.00       4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/medicos_full.csv')\n",
    "print(df.shape)\n",
    "print(df['Gastos_Medicos_RC_siniestros_num'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    7816\n",
      "1     183\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['target'] = (df['Gastos_Medicos_RC_siniestros_num'] > 0).astype(int)\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles en el dataset:\n",
      "  año_cursado\n",
      "  estudios_area\n",
      "  calif_promedio\n",
      "  2_o_mas_inquilinos\n",
      "  distancia_al_campus\n",
      "  genero\n",
      "  extintor_incendios\n",
      "  Gastos_Medicos_RC_siniestros_num\n",
      "  Gastos_Medicos_RC_siniestros_monto\n",
      "  target\n",
      "\n",
      "Variable 'tiene_monto' creada: correlación con target = 1.0000\n",
      "✗ Variable 'tiene_monto' muy correlacionada con target - no agregada\n",
      "\n",
      "Variables finales seleccionadas (7):\n",
      "   1. año_cursado\n",
      "   2. estudios_area\n",
      "   3. calif_promedio\n",
      "   4. 2_o_mas_inquilinos\n",
      "   5. distancia_al_campus\n",
      "   6. genero\n",
      "   7. extintor_incendios\n",
      "\n",
      "Categóricas: ['año_cursado', 'estudios_area', 'genero', '2_o_mas_inquilinos', 'extintor_incendios']\n",
      "Numéricas: ['calif_promedio', 'distancia_al_campus']\n"
     ]
    }
   ],
   "source": [
    "feature_vars = ['año_cursado', 'estudios_area', 'calif_promedio', '2_o_mas_inquilinos', \n",
    "                'distancia_al_campus', 'genero', 'extintor_incendios']\n",
    "\n",
    "# Verificar si hay otras variables potenciales en el dataset\n",
    "print(\"Columnas disponibles en el dataset:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "# Agregar variable de monto si existe (como variable explicativa adicional)\n",
    "if 'Gastos_Medicos_RC_siniestros_monto' in df.columns:\n",
    "    # Solo para registros con siniestros, el monto puede ser informativo\n",
    "    df['tiene_monto'] = (df['Gastos_Medicos_RC_siniestros_monto'] > 0).astype(int)\n",
    "    print(f\"\\nVariable 'tiene_monto' creada: correlación con target = {df['tiene_monto'].corr(df['target']):.4f}\")\n",
    "    \n",
    "    # Solo agregar si la correlación es perfecta (redundante) o útil\n",
    "    if df['tiene_monto'].corr(df['target']) < 0.99:  # No perfectamente correlacionada\n",
    "        feature_vars.append('tiene_monto')\n",
    "        print(\"✓ Variable 'tiene_monto' agregada como predictora\")\n",
    "    else:\n",
    "        print(\"✗ Variable 'tiene_monto' muy correlacionada con target - no agregada\")\n",
    "\n",
    "X = df[feature_vars].copy()\n",
    "y = df['target'].copy()\n",
    "\n",
    "categorical_features = ['año_cursado', 'estudios_area', 'genero', '2_o_mas_inquilinos', 'extintor_incendios']\n",
    "numerical_features = ['calif_promedio', 'distancia_al_campus']\n",
    "\n",
    "# Actualizar listas si se agregó nueva variable\n",
    "if 'tiene_monto' in feature_vars:\n",
    "    categorical_features.append('tiene_monto')\n",
    "\n",
    "print(f\"\\nVariables finales seleccionadas ({len(feature_vars)}):\")\n",
    "for i, var in enumerate(feature_vars, 1):\n",
    "    print(f\"  {i:2d}. {var}\")\n",
    "    \n",
    "print(f\"\\nCategóricas: {categorical_features}\")\n",
    "print(f\"Numéricas: {numerical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed shape: (7999, 14)\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "feature_names = (numerical_features + \n",
    "                 list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "\n",
    "print(f\"Transformed shape: {X_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (6399, 14), Test: (1600, 14)\n",
      "Train target dist: [6253, 146]\n",
      "Test target dist: [1563, 37]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_transformed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Train target dist: {y_train.value_counts().tolist()}\")\n",
    "print(f\"Test target dist: {y_test.value_counts().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selection(X, y, feature_names, significance_level=0.05):\n",
    "    X_with_const = sm.add_constant(X)\n",
    "    current_features = feature_names.copy()\n",
    "    \n",
    "    while True:\n",
    "        feature_indices = [feature_names.index(f) for f in current_features]\n",
    "        current_X = X_with_const[:, [0] + [i+1 for i in feature_indices]]\n",
    "        \n",
    "        model = Logit(y, current_X).fit(disp=0)\n",
    "        p_values = model.pvalues[1:]\n",
    "        max_p_value = p_values.max()\n",
    "        \n",
    "        if max_p_value > significance_level:\n",
    "            worst_idx_in_pvalues = p_values.idxmax()\n",
    "            worst_idx_in_current = list(p_values.index).index(worst_idx_in_pvalues)\n",
    "            feature_to_remove = current_features[worst_idx_in_current]\n",
    "            \n",
    "            current_features.remove(feature_to_remove)\n",
    "            print(f\"Removed {feature_to_remove}, p-value: {max_p_value:.4f}\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    feature_indices = [feature_names.index(f) for f in current_features]\n",
    "    final_X = X_with_const[:, [0] + [i+1 for i in feature_indices]]\n",
    "    final_model = Logit(y, final_X).fit(disp=0)\n",
    "    \n",
    "    selected_indices = [feature_names.index(f) for f in current_features]\n",
    "    \n",
    "    return current_features, selected_indices, final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUACION BASELINE - DUMMYCLASSIFIER\n",
      "==================================================\n",
      "Most_Frequent: Acc=0.9769, F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "Stratified: Acc=0.9569, F1=0.0282, Prec=0.0294, Rec=0.0270\n",
      "Uniform: Acc=0.4981, F1=0.0383, Prec=0.0201, Rec=0.4324\n",
      "\n",
      "SELECCION BACKWARD DE VARIABLES\n",
      "==================================================\n",
      "Removed genero_Otro, p-value: 0.9054\n",
      "Removed calif_promedio, p-value: 0.9027\n",
      "Removed año_cursado_3er año, p-value: 0.8973\n",
      "Removed extintor_incendios_Si, p-value: 0.8709\n",
      "Removed estudios_area_Humanidades, p-value: 0.8666\n",
      "Removed genero_No respuesta, p-value: 0.7954\n",
      "Removed estudios_area_Ciencias, p-value: 0.5881\n",
      "Removed año_cursado_posgrado, p-value: 0.5648\n",
      "Removed estudios_area_Otro, p-value: 0.4220\n",
      "Removed genero_Masculino, p-value: 0.2060\n",
      "Removed año_cursado_4to año, p-value: 0.1965\n",
      "Removed año_cursado_2do año, p-value: 0.2244\n",
      "Variables seleccionadas: 2\n",
      "['distancia_al_campus', '2_o_mas_inquilinos_Si']\n"
     ]
    }
   ],
   "source": [
    "print(\"EVALUACION BASELINE - DUMMYCLASSIFIER\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "dummy_models = {\n",
    "    'Most_Frequent': DummyClassifier(strategy=\"most_frequent\", random_state=42),\n",
    "    'Stratified': DummyClassifier(strategy=\"stratified\", random_state=42),\n",
    "    'Uniform': DummyClassifier(strategy=\"uniform\", random_state=42)\n",
    "}\n",
    "\n",
    "dummy_results = {}\n",
    "for name, dummy in dummy_models.items():\n",
    "    dummy.fit(X_train, y_train)\n",
    "    y_pred_dummy = dummy.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_dummy)\n",
    "    f1 = f1_score(y_test, y_pred_dummy, zero_division=0)\n",
    "    precision = precision_score(y_test, y_pred_dummy, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_dummy, zero_division=0)\n",
    "    \n",
    "    dummy_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}: Acc={accuracy:.4f}, F1={f1:.4f}, Prec={precision:.4f}, Rec={recall:.4f}\")\n",
    "\n",
    "print(\"\\nSELECCION BACKWARD DE VARIABLES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "selected_vars, selected_indices, glm_model = backward_selection(X_train, y_train, feature_names)\n",
    "print(f\"Variables seleccionadas: {len(selected_vars)}\")\n",
    "print(selected_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTRATEGIAS DE SELECCION DE VARIABLES\n",
      "==================================================\n",
      "Estrategias de seleccion creadas:\n",
      "  Backward: 2 variables\n",
      "  All_Features: 14 variables\n",
      "  KBest_f_classif_5: 5 variables\n",
      "  KBest_f_classif_8: 8 variables\n",
      "  KBest_f_classif_10: 10 variables\n",
      "  KBest_mutual_info_5: 5 variables\n",
      "  KBest_mutual_info_8: 8 variables\n",
      "  KBest_mutual_info_10: 10 variables\n",
      "\n",
      "Usando estrategia 'Backward' para LazyPredict inicial...\n"
     ]
    }
   ],
   "source": [
    "print(\"ESTRATEGIAS DE SELECCION DE VARIABLES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "feature_selection_strategies = {}\n",
    "\n",
    "# Estrategia 1: Variables seleccionadas por backward\n",
    "categorical_base_names = ['año_cursado', 'estudios_area', 'genero', '2_o_mas_inquilinos', 'extintor_incendios']\n",
    "backward_selected_indices = []\n",
    "backward_selected_vars = []\n",
    "\n",
    "for var in selected_vars:\n",
    "    if var in numerical_features:\n",
    "        backward_selected_indices.append(feature_names.index(var))\n",
    "        backward_selected_vars.append(var)\n",
    "\n",
    "for base_name in categorical_base_names:\n",
    "    categories_in_selected = [var for var in selected_vars if var.startswith(base_name)]\n",
    "    if categories_in_selected:\n",
    "        all_categories = [var for var in feature_names if var.startswith(base_name)]\n",
    "        for cat_var in all_categories:\n",
    "            if cat_var not in backward_selected_vars:\n",
    "                backward_selected_indices.append(feature_names.index(cat_var))\n",
    "                backward_selected_vars.append(cat_var)\n",
    "\n",
    "feature_selection_strategies['Backward'] = {\n",
    "    'indices': backward_selected_indices,\n",
    "    'names': backward_selected_vars\n",
    "}\n",
    "\n",
    "# Estrategia 2: Todas las variables\n",
    "feature_selection_strategies['All_Features'] = {\n",
    "    'indices': list(range(len(feature_names))),\n",
    "    'names': feature_names\n",
    "}\n",
    "\n",
    "# Estrategia 3: SelectKBest con f_classif\n",
    "for k in [5, 8, 10]:\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_selected = selector.fit_transform(X_train, y_train)\n",
    "    selected_features_mask = selector.get_support()\n",
    "    kbest_indices = [i for i, selected in enumerate(selected_features_mask) if selected]\n",
    "    kbest_names = [feature_names[i] for i in kbest_indices]\n",
    "    \n",
    "    feature_selection_strategies[f'KBest_f_classif_{k}'] = {\n",
    "        'indices': kbest_indices,\n",
    "        'names': kbest_names\n",
    "    }\n",
    "\n",
    "# Estrategia 4: SelectKBest con mutual_info\n",
    "for k in [5, 8, 10]:\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    X_selected = selector.fit_transform(X_train, y_train)\n",
    "    selected_features_mask = selector.get_support()\n",
    "    mi_indices = [i for i, selected in enumerate(selected_features_mask) if selected]\n",
    "    mi_names = [feature_names[i] for i in mi_indices]\n",
    "    \n",
    "    feature_selection_strategies[f'KBest_mutual_info_{k}'] = {\n",
    "        'indices': mi_indices,\n",
    "        'names': mi_names\n",
    "    }\n",
    "\n",
    "print(\"Estrategias de seleccion creadas:\")\n",
    "for strategy_name, strategy_info in feature_selection_strategies.items():\n",
    "    print(f\"  {strategy_name}: {len(strategy_info['indices'])} variables\")\n",
    "\n",
    "print(f\"\\nUsando estrategia 'Backward' para LazyPredict inicial...\")\n",
    "final_selected_indices = feature_selection_strategies['Backward']['indices']\n",
    "final_selected_vars = feature_selection_strategies['Backward']['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECNICAS DE RESAMPLING\n",
      "==================================================\n",
      "SMOTE: [6253, 6253]\n",
      "ADASYN: [6258, 6253]\n",
      "UnderSample: [146, 146]\n",
      "\n",
      "Distribucion original: [6253, 146]\n",
      "Estrategias de resampling disponibles: ['Original', 'SMOTE', 'ADASYN', 'UnderSample']\n"
     ]
    }
   ],
   "source": [
    "print(\"TECNICAS DE RESAMPLING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_train_selected = X_train[:, final_selected_indices]\n",
    "X_test_selected = X_test[:, final_selected_indices]\n",
    "\n",
    "resampling_strategies = {}\n",
    "\n",
    "resampling_strategies['Original'] = {\n",
    "    'X_train': X_train_selected,\n",
    "    'y_train': y_train\n",
    "}\n",
    "\n",
    "if IMBLEARN_AVAILABLE:\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train_selected, y_train)\n",
    "        resampling_strategies['SMOTE'] = {\n",
    "            'X_train': X_train_smote,\n",
    "            'y_train': y_train_smote\n",
    "        }\n",
    "        print(f\"SMOTE: {y_train_smote.value_counts().tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"SMOTE error: {e}\")\n",
    "\n",
    "    try:\n",
    "        adasyn = ADASYN(random_state=42)\n",
    "        X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_selected, y_train)\n",
    "        resampling_strategies['ADASYN'] = {\n",
    "            'X_train': X_train_adasyn,\n",
    "            'y_train': y_train_adasyn\n",
    "        }\n",
    "        print(f\"ADASYN: {y_train_adasyn.value_counts().tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ADASYN error: {e}\")\n",
    "\n",
    "    try:\n",
    "        undersampler = RandomUnderSampler(random_state=42)\n",
    "        X_train_under, y_train_under = undersampler.fit_resample(X_train_selected, y_train)\n",
    "        resampling_strategies['UnderSample'] = {\n",
    "            'X_train': X_train_under,\n",
    "            'y_train': y_train_under\n",
    "        }\n",
    "        print(f\"UnderSample: {y_train_under.value_counts().tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"UnderSample error: {e}\")\n",
    "else:\n",
    "    print(\"imblearn no disponible - usando solo datos originales\")\n",
    "\n",
    "print(f\"\\nDistribucion original: {y_train.value_counts().tolist()}\")\n",
    "print(f\"Estrategias de resampling disponibles: {list(resampling_strategies.keys())}\")\n",
    "\n",
    "current_X_train = X_train_selected\n",
    "current_y_train = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAZYPREDICT - EXPLORACION DE MODELOS\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa517ef71e024e0dacfd12ba4da4504c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 146, number of negative: 6253\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 258\n",
      "[LightGBM] [Info] Number of data points in the train set: 6399, number of used features: 2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.022816 -> initscore=-3.757210\n",
      "[LightGBM] [Info] Start training from score -3.757210\n",
      "Resultados LazyPredict:\n",
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "NearestCentroid                    0.69               0.55     0.55      0.80   \n",
      "BaggingClassifier                  0.97               0.52     0.52      0.96   \n",
      "ExtraTreesClassifier               0.97               0.52     0.52      0.96   \n",
      "RandomForestClassifier             0.97               0.52     0.52      0.96   \n",
      "DecisionTreeClassifier             0.97               0.52     0.52      0.96   \n",
      "ExtraTreeClassifier                0.96               0.50     0.50      0.96   \n",
      "GaussianNB                         0.96               0.50     0.50      0.96   \n",
      "QuadraticDiscriminantAnalysis      0.96               0.50     0.50      0.96   \n",
      "DummyClassifier                    0.98               0.50     0.50      0.97   \n",
      "BernoulliNB                        0.98               0.50     0.50      0.97   \n",
      "AdaBoostClassifier                 0.98               0.50     0.50      0.97   \n",
      "Perceptron                         0.98               0.50     0.50      0.97   \n",
      "CalibratedClassifierCV             0.98               0.50     0.50      0.97   \n",
      "CategoricalNB                      0.98               0.50     0.50      0.97   \n",
      "LabelPropagation                   0.98               0.50     0.50      0.97   \n",
      "LinearSVC                          0.98               0.50     0.50      0.97   \n",
      "LogisticRegression                 0.98               0.50     0.50      0.97   \n",
      "LinearDiscriminantAnalysis         0.98               0.50     0.50      0.97   \n",
      "LabelSpreading                     0.98               0.50     0.50      0.97   \n",
      "SGDClassifier                      0.98               0.50     0.50      0.97   \n",
      "PassiveAggressiveClassifier        0.98               0.50     0.50      0.97   \n",
      "RidgeClassifier                    0.98               0.50     0.50      0.97   \n",
      "RidgeClassifierCV                  0.98               0.50     0.50      0.97   \n",
      "XGBClassifier                      0.98               0.50     0.50      0.97   \n",
      "SVC                                0.98               0.50     0.50      0.97   \n",
      "KNeighborsClassifier               0.98               0.50     0.50      0.97   \n",
      "LGBMClassifier                     0.98               0.50     0.50      0.97   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "NearestCentroid                      0.02  \n",
      "BaggingClassifier                    0.07  \n",
      "ExtraTreesClassifier                 0.26  \n",
      "RandomForestClassifier               0.50  \n",
      "DecisionTreeClassifier               0.02  \n",
      "ExtraTreeClassifier                  0.01  \n",
      "GaussianNB                           0.02  \n",
      "QuadraticDiscriminantAnalysis        0.03  \n",
      "DummyClassifier                      0.01  \n",
      "BernoulliNB                          0.02  \n",
      "AdaBoostClassifier                   0.18  \n",
      "Perceptron                           0.03  \n",
      "CalibratedClassifierCV               0.06  \n",
      "CategoricalNB                        0.02  \n",
      "LabelPropagation                     1.52  \n",
      "LinearSVC                            0.02  \n",
      "LogisticRegression                   0.02  \n",
      "LinearDiscriminantAnalysis           0.03  \n",
      "LabelSpreading                       2.19  \n",
      "SGDClassifier                        0.03  \n",
      "PassiveAggressiveClassifier          0.04  \n",
      "RidgeClassifier                      0.02  \n",
      "RidgeClassifierCV                    0.02  \n",
      "XGBClassifier                        0.12  \n",
      "SVC                                  0.21  \n",
      "KNeighborsClassifier                 0.11  \n",
      "LGBMClassifier                       0.18  \n",
      "\n",
      "TOP 10 MODELOS POR F1-SCORE:\n",
      " 1. DummyClassifier          : F1=0.9654, Acc=0.9769\n",
      " 2. BernoulliNB              : F1=0.9654, Acc=0.9769\n",
      " 3. AdaBoostClassifier       : F1=0.9654, Acc=0.9769\n",
      " 4. Perceptron               : F1=0.9654, Acc=0.9769\n",
      " 5. CalibratedClassifierCV   : F1=0.9654, Acc=0.9769\n",
      " 6. CategoricalNB            : F1=0.9654, Acc=0.9769\n",
      " 7. LabelPropagation         : F1=0.9654, Acc=0.9769\n",
      " 8. LinearSVC                : F1=0.9654, Acc=0.9769\n",
      " 9. LogisticRegression       : F1=0.9654, Acc=0.9769\n",
      "10. LinearDiscriminantAnalysis: F1=0.9654, Acc=0.9769\n",
      "\n",
      "Comparacion con baseline:\n",
      "Mejor DummyClassifier F1: 0.0383\n",
      "Mejor LazyPredict F1:    0.9654\n",
      "✓ LazyPredict supera baseline DummyClassifier\n",
      "\n",
      "Seleccionando top 5 para optimizacion: ['DummyClassifier', 'BernoulliNB', 'AdaBoostClassifier', 'Perceptron', 'CalibratedClassifierCV']\n"
     ]
    }
   ],
   "source": [
    "print(\"LAZYPREDICT - EXPLORACION DE MODELOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(current_X_train, X_test_selected, current_y_train, y_test)\n",
    "\n",
    "print(\"Resultados LazyPredict:\")\n",
    "print(models.round(4))\n",
    "\n",
    "top_10_models = models.nlargest(10, 'F1 Score')\n",
    "print(f\"\\nTOP 10 MODELOS POR F1-SCORE:\")\n",
    "for i, (model_name, row) in enumerate(top_10_models.iterrows(), 1):\n",
    "    print(f\"{i:2d}. {model_name:25s}: F1={row['F1 Score']:.4f}, Acc={row['Accuracy']:.4f}\")\n",
    "\n",
    "# Comparar con baseline\n",
    "best_f1_lazy = top_10_models.iloc[0]['F1 Score']\n",
    "best_dummy_f1 = max([res['f1_score'] for res in dummy_results.values()])\n",
    "\n",
    "print(f\"\\nComparacion con baseline:\")\n",
    "print(f\"Mejor DummyClassifier F1: {best_dummy_f1:.4f}\")\n",
    "print(f\"Mejor LazyPredict F1:    {best_f1_lazy:.4f}\")\n",
    "if best_f1_lazy > best_dummy_f1:\n",
    "    print(\"✓ LazyPredict supera baseline DummyClassifier\")\n",
    "else:\n",
    "    print(\"✗ LazyPredict NO supera baseline DummyClassifier\")\n",
    "\n",
    "top_5_names = top_10_models.head(5).index.tolist()\n",
    "print(f\"\\nSeleccionando top 5 para optimizacion: {top_5_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPEO Y CONFIGURACION DE MODELOS EXPANDIDA\n",
      "==================================================\n",
      "✓ imblearn modelos balanceados disponibles\n",
      "✓ LightGBM disponible\n",
      "✗ CatBoost no disponible\n",
      "\n",
      "+ Agregando RandomForestClassifier adicional para testing\n",
      "○ DummyClassifier usando LogisticRegression como backup\n",
      "○ BernoulliNB usando LogisticRegression como backup\n",
      "✓ AdaBoostClassifier mapeado correctamente\n",
      "○ Perceptron usando LogisticRegression como backup\n",
      "○ CalibratedClassifierCV usando LogisticRegression como backup\n",
      "\n",
      "Modelos a optimizar: 6\n",
      "\n",
      "Grids definidos para 12 tipos de modelos\n",
      "RandomForestClassifier_Test tiene grid expandido con:\n",
      "  n_estimators: 4 valores\n",
      "  max_depth: 6 valores\n",
      "  min_samples_split: 4 valores\n",
      "  min_samples_leaf: 4 valores\n",
      "  max_features: 4 valores\n",
      "  class_weight: 5 valores\n",
      "  bootstrap: 2 valores\n",
      "Total combinaciones RF: 15,360\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPEO Y CONFIGURACION DE MODELOS EXPANDIDA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model_mapping = {\n",
    "    'XGBClassifier': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVC': SVC(random_state=42, probability=True),\n",
    "    'MLPClassifier': MLPClassifier(random_state=42, max_iter=500),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(random_state=42),\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "if IMBLEARN_AVAILABLE:\n",
    "    model_mapping['BalancedRandomForestClassifier'] = BalancedRandomForestClassifier(random_state=42)\n",
    "    model_mapping['BalancedBaggingClassifier'] = BalancedBaggingClassifier(random_state=42)\n",
    "    model_mapping['EasyEnsembleClassifier'] = EasyEnsembleClassifier(random_state=42)\n",
    "    print(\"✓ imblearn modelos balanceados disponibles\")\n",
    "else:\n",
    "    print(\"✗ imblearn no disponible - sin modelos balanceados\")\n",
    "\n",
    "if LGBM_AVAILABLE:\n",
    "    model_mapping['LGBMClassifier'] = LGBMClassifier(random_state=42, verbose=-1)\n",
    "    print(\"✓ LightGBM disponible\")\n",
    "else:\n",
    "    print(\"✗ LightGBM no disponible\")\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    model_mapping['CatBoostClassifier'] = CatBoostClassifier(random_state=42, verbose=False)\n",
    "    print(\"✓ CatBoost disponible\")\n",
    "else:\n",
    "    print(\"✗ CatBoost no disponible\")\n",
    "\n",
    "# Agregar RandomForest adicional como modelo independiente para testing\n",
    "print(\"\\n+ Agregando RandomForestClassifier adicional para testing\")\n",
    "models_to_optimize = [('RandomForestClassifier_Test', RandomForestClassifier(random_state=42))]\n",
    "\n",
    "# Agregar los top 5 de LazyPredict\n",
    "for name in top_5_names:\n",
    "    if name in model_mapping:\n",
    "        models_to_optimize.append((name, model_mapping[name]))\n",
    "        print(f\"✓ {name} mapeado correctamente\")\n",
    "    else:\n",
    "        models_to_optimize.append((name, LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')))\n",
    "        print(f\"○ {name} usando LogisticRegression como backup\")\n",
    "\n",
    "print(f\"\\nModelos a optimizar: {len(models_to_optimize)}\")\n",
    "\n",
    "# Grillas expandidas especialmente para RandomForest\n",
    "param_grids = {\n",
    "    'RandomForestClassifier_Test': {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [3, 5, 8, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'min_samples_leaf': [1, 2, 4, 8],\n",
    "        'max_features': ['sqrt', 'log2', 0.3, 0.5],\n",
    "        'class_weight': ['balanced', 'balanced_subsample', {0: 1, 1: 10}, {0: 1, 1: 15}, {0: 1, 1: 20}],\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'scale_pos_weight': [1, 5, 10, 15, 20]\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'class_weight': ['balanced', 'balanced_subsample', {0: 1, 1: 10}, {0: 1, 1: 20}]\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5]\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': ['balanced', {0: 1, 1: 10}, {0: 1, 1: 20}]\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'class_weight': ['balanced', {0: 1, 1: 10}, {0: 1, 1: 20}]\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1]\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.5, 1.0, 1.5]\n",
    "    },\n",
    "    'ExtraTreesClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'class_weight': ['balanced', 'balanced_subsample']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }\n",
    "}\n",
    "\n",
    "if IMBLEARN_AVAILABLE:\n",
    "    param_grids['BalancedRandomForestClassifier'] = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'sampling_strategy': ['auto', 'majority', 'not majority']\n",
    "    }\n",
    "\n",
    "if LGBM_AVAILABLE:\n",
    "    param_grids['LGBMClassifier'] = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'is_unbalance': [True]\n",
    "    }\n",
    "\n",
    "print(f\"\\nGrids definidos para {len(param_grids)} tipos de modelos\")\n",
    "print(\"RandomForestClassifier_Test tiene grid expandido con:\")\n",
    "rf_grid = param_grids['RandomForestClassifier_Test']\n",
    "total_combinations = 1\n",
    "for param, values in rf_grid.items():\n",
    "    total_combinations *= len(values)\n",
    "    print(f\"  {param}: {len(values)} valores\")\n",
    "print(f\"Total combinaciones RF: {total_combinations:,}\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZACION DE HIPERPARAMETROS (ACELERADA)\n",
      "==================================================\n",
      "RandomForest combinaciones reducidas: 96 (vs 76,800 original)\n",
      "Tiempo estimado: 480 entrenamientos vs 384000 original\n",
      "\n",
      "Optimizing RandomForestClassifier_Test...\n",
      "  Grid reducido: 7 parámetros\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "  ✓ Best F1-CV: 0.0628\n",
      "  ✓ Best params: {'bootstrap': True, 'class_weight': 'balanced', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "\n",
      "Optimizing DummyClassifier...\n",
      "  Sin grid específico\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "  ✓ Best F1-CV: 0.0702\n",
      "  ✓ Best params: {}\n",
      "\n",
      "Optimizing BernoulliNB...\n",
      "  Sin grid específico\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "  ✓ Best F1-CV: 0.0702\n",
      "  ✓ Best params: {}\n",
      "\n",
      "Optimizing AdaBoostClassifier...\n",
      "  Sin grid específico\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "  ✓ Best F1-CV: 0.0000\n",
      "  ✓ Best params: {}\n",
      "\n",
      "Optimizing Perceptron...\n",
      "  Sin grid específico\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "  ✓ Best F1-CV: 0.0702\n",
      "  ✓ Best params: {}\n",
      "\n",
      "Optimizing CalibratedClassifierCV...\n",
      "  Sin grid específico\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "  ✓ Best F1-CV: 0.0702\n",
      "  ✓ Best params: {}\n",
      "\n",
      "==================================================\n",
      "OPTIMIZATION COMPLETED\n",
      "==================================================\n",
      "Resultados de optimización:\n",
      "RandomForestClassifier_Test: F1-CV = 0.0628\n",
      "DummyClassifier: F1-CV = 0.0702\n",
      "BernoulliNB: F1-CV = 0.0702\n",
      "AdaBoostClassifier: F1-CV = 0.0000\n",
      "Perceptron: F1-CV = 0.0702\n",
      "CalibratedClassifierCV: F1-CV = 0.0702\n"
     ]
    }
   ],
   "source": [
    "print(\"OPTIMIZACION DE HIPERPARAMETROS (ACELERADA)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Grillas reducidas para acelerar\n",
    "param_grids_fast = {\n",
    "    'RandomForestClassifier_Test': {\n",
    "        'n_estimators': [100, 200],  # Reducido de 4 a 2 valores\n",
    "        'max_depth': [5, 10, None],  # Reducido de 6 a 3 valores  \n",
    "        'min_samples_split': [2, 10],  # Reducido de 4 a 2 valores\n",
    "        'min_samples_leaf': [1, 4],  # Reducido de 4 a 2 valores\n",
    "        'max_features': ['sqrt', 0.5],  # Reducido de 4 a 2 valores\n",
    "        'class_weight': ['balanced', {0: 1, 1: 15}],  # Reducido de 5 a 2 valores\n",
    "        'bootstrap': [True]  # Solo True (más estable)\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.1, 0.2],\n",
    "        'scale_pos_weight': [10, 20]\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15],\n",
    "        'class_weight': ['balanced', {0: 1, 1: 15}]\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'class_weight': ['balanced', {0: 1, 1: 15}]\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [1, 10],\n",
    "        'kernel': ['rbf'],\n",
    "        'class_weight': ['balanced']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calcular combinaciones reducidas\n",
    "rf_fast = param_grids_fast['RandomForestClassifier_Test']\n",
    "rf_combinations = 1\n",
    "for param, values in rf_fast.items():\n",
    "    rf_combinations *= len(values)\n",
    "print(f\"RandomForest combinaciones reducidas: {rf_combinations} (vs 76,800 original)\")\n",
    "print(f\"Tiempo estimado: {rf_combinations * 5} entrenamientos vs {76800 * 5} original\")\n",
    "\n",
    "optimization_results = {}\n",
    "\n",
    "for model_name, model in models_to_optimize:\n",
    "    print(f\"\\nOptimizing {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Usar grids acelerados\n",
    "        if model_name in param_grids_fast:\n",
    "            param_grid = param_grids_fast[model_name]\n",
    "            print(f\"  Grid reducido: {len(param_grid)} parámetros\")\n",
    "        else:\n",
    "            param_grid = {}\n",
    "            print(f\"  Sin grid específico\")\n",
    "        \n",
    "        # Reducir CV a 3-fold para acelerar\n",
    "        skf_fast = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            scoring='f1',\n",
    "            cv=skf_fast,  # 3-fold en lugar de 5-fold\n",
    "            n_jobs=-1,\n",
    "            verbose=1  # Mostrar progreso\n",
    "        )\n",
    "        \n",
    "        print(f\"  Iniciando búsqueda...\")\n",
    "        grid_search.fit(current_X_train, current_y_train)\n",
    "        \n",
    "        optimization_results[model_name] = {\n",
    "            'best_estimator': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_cv_score': grid_search.best_score_\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ Best F1-CV: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"  ✓ Best params: {grid_search.best_params_}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {str(e)}\")\n",
    "        optimization_results[model_name] = {\n",
    "            'best_estimator': model,\n",
    "            'best_params': {},\n",
    "            'best_cv_score': 0\n",
    "        }\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"OPTIMIZATION COMPLETED\")\n",
    "print(f\"{'='*50}\")\n",
    "print(\"Resultados de optimización:\")\n",
    "for name, results in optimization_results.items():\n",
    "    print(f\"{name}: F1-CV = {results['best_cv_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZACION DE THRESHOLDS\n",
      "==================================================\n",
      "RandomForestClassifier_Test: threshold=0.650, F1=0.0588\n",
      "DummyClassifier: threshold=0.500, F1=0.0578\n",
      "BernoulliNB: threshold=0.500, F1=0.0578\n",
      "AdaBoostClassifier: threshold=0.250, F1=0.0539\n",
      "Perceptron: threshold=0.500, F1=0.0578\n",
      "CalibratedClassifierCV: threshold=0.500, F1=0.0578\n",
      "Threshold optimization completed\n"
     ]
    }
   ],
   "source": [
    "def optimize_threshold(model, X_val, y_val):\n",
    "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_f1\n",
    "\n",
    "print(\"OPTIMIZACION DE THRESHOLDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "threshold_results = {}\n",
    "\n",
    "for model_name, model_info in optimization_results.items():\n",
    "    model = model_info['best_estimator']\n",
    "    best_threshold, best_f1 = optimize_threshold(model, X_test_selected, y_test)\n",
    "    \n",
    "    threshold_results[model_name] = {\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_f1': best_f1,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name}: threshold={best_threshold:.3f}, F1={best_f1:.4f}\")\n",
    "\n",
    "print(\"Threshold optimization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier_Test: threshold=0.650, F1=0.0588\n",
      "DummyClassifier: threshold=0.500, F1=0.0578\n",
      "BernoulliNB: threshold=0.500, F1=0.0578\n",
      "AdaBoostClassifier: threshold=0.250, F1=0.0539\n",
      "Perceptron: threshold=0.500, F1=0.0578\n",
      "CalibratedClassifierCV: threshold=0.500, F1=0.0578\n",
      "Threshold optimization completed\n"
     ]
    }
   ],
   "source": [
    "threshold_results = {}\n",
    "\n",
    "for model_name, model_info in optimization_results.items():\n",
    "    model = model_info['best_estimator']\n",
    "    best_threshold, best_f1 = optimize_threshold(model, X_test_selected, y_test)\n",
    "    \n",
    "    threshold_results[model_name] = {\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_f1': best_f1,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name}: threshold={best_threshold:.3f}, F1={best_f1:.4f}\")\n",
    "\n",
    "print(\"Threshold optimization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUACION FINAL CON METRICAS MEJORADAS\n",
      "============================================================\n",
      "RandomForestClassifier_Test:\n",
      "  Threshold: 0.650\n",
      "  Accuracy:  0.9600\n",
      "  F1-Score:  0.0588\n",
      "  Precision: 0.0645\n",
      "  Recall:    0.0541\n",
      "  GINI:      0.1500\n",
      "  PR-AUC:    0.0289\n",
      "  MCC:       0.0387\n",
      "  Bal-Acc:   0.5177\n",
      "\n",
      "DummyClassifier:\n",
      "  Threshold: 0.500\n",
      "  Accuracy:  0.6944\n",
      "  F1-Score:  0.0578\n",
      "  Precision: 0.0311\n",
      "  Recall:    0.4054\n",
      "  GINI:      0.1831\n",
      "  PR-AUC:    0.0302\n",
      "  MCC:       0.0349\n",
      "  Bal-Acc:   0.5533\n",
      "\n",
      "BernoulliNB:\n",
      "  Threshold: 0.500\n",
      "  Accuracy:  0.6944\n",
      "  F1-Score:  0.0578\n",
      "  Precision: 0.0311\n",
      "  Recall:    0.4054\n",
      "  GINI:      0.1831\n",
      "  PR-AUC:    0.0302\n",
      "  MCC:       0.0349\n",
      "  Bal-Acc:   0.5533\n",
      "\n",
      "AdaBoostClassifier:\n",
      "  Threshold: 0.250\n",
      "  Accuracy:  0.7588\n",
      "  F1-Score:  0.0539\n",
      "  Precision: 0.0296\n",
      "  Recall:    0.2973\n",
      "  GINI:      0.1779\n",
      "  PR-AUC:    0.0314\n",
      "  MCC:       0.0239\n",
      "  Bal-Acc:   0.5335\n",
      "\n",
      "Perceptron:\n",
      "  Threshold: 0.500\n",
      "  Accuracy:  0.6944\n",
      "  F1-Score:  0.0578\n",
      "  Precision: 0.0311\n",
      "  Recall:    0.4054\n",
      "  GINI:      0.1831\n",
      "  PR-AUC:    0.0302\n",
      "  MCC:       0.0349\n",
      "  Bal-Acc:   0.5533\n",
      "\n",
      "CalibratedClassifierCV:\n",
      "  Threshold: 0.500\n",
      "  Accuracy:  0.6944\n",
      "  F1-Score:  0.0578\n",
      "  Precision: 0.0311\n",
      "  Recall:    0.4054\n",
      "  GINI:      0.1831\n",
      "  PR-AUC:    0.0302\n",
      "  MCC:       0.0349\n",
      "  Bal-Acc:   0.5533\n",
      "\n",
      "Comparacion con DummyClassifier:\n",
      "----------------------------------------\n",
      "Most_Frequent: F1=0.0000, Acc=0.9769\n",
      "Stratified: F1=0.0282, Acc=0.9569\n",
      "Uniform: F1=0.0383, Acc=0.4981\n",
      "\n",
      "Mejor modelo real:\n",
      "RandomForestClassifier_Test: F1=0.0588\n",
      "✓ El mejor modelo SUPERA el baseline DummyClassifier\n"
     ]
    }
   ],
   "source": [
    "def calculate_enhanced_metrics(y_true, y_proba, y_pred):\n",
    "    \"\"\"Calcula metricas mejoradas para datos desbalanceados\"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, matthews_corrcoef, balanced_accuracy_score, average_precision_score\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_proba)\n",
    "        gini = 2 * roc_auc - 1\n",
    "    except:\n",
    "        roc_auc = 0\n",
    "        gini = 0\n",
    "    \n",
    "    try:\n",
    "        pr_auc = average_precision_score(y_true, y_proba)\n",
    "    except:\n",
    "        pr_auc = 0\n",
    "    \n",
    "    try:\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    except:\n",
    "        mcc = 0\n",
    "    \n",
    "    try:\n",
    "        balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    except:\n",
    "        balanced_acc = 0\n",
    "    \n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    r2 = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'mae': mae,\n",
    "        'gini': gini,\n",
    "        'r2': r2,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'mcc': mcc,\n",
    "        'balanced_accuracy': balanced_acc\n",
    "    }\n",
    "\n",
    "final_results = {}\n",
    "\n",
    "print(\"EVALUACION FINAL CON METRICAS MEJORADAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, threshold_info in threshold_results.items():\n",
    "    model = threshold_info['model']\n",
    "    threshold = threshold_info['best_threshold']\n",
    "    \n",
    "    y_proba = model.predict_proba(X_test_selected)[:, 1]\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = calculate_enhanced_metrics(y_test, y_proba, y_pred)\n",
    "    metrics['threshold'] = threshold\n",
    "    metrics['model'] = model\n",
    "    \n",
    "    final_results[model_name] = metrics\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Threshold: {threshold:.3f}\")\n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  GINI:      {metrics['gini']:.4f}\")\n",
    "    print(f\"  PR-AUC:    {metrics['pr_auc']:.4f}\")\n",
    "    print(f\"  MCC:       {metrics['mcc']:.4f}\")\n",
    "    print(f\"  Bal-Acc:   {metrics['balanced_accuracy']:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"Comparacion con DummyClassifier:\")\n",
    "print(\"-\" * 40)\n",
    "for dummy_name, dummy_metrics in dummy_results.items():\n",
    "    print(f\"{dummy_name}: F1={dummy_metrics['f1_score']:.4f}, Acc={dummy_metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nMejor modelo real:\")\n",
    "best_model_name = max(final_results.keys(), key=lambda k: final_results[k]['f1_score'])\n",
    "best_f1 = final_results[best_model_name]['f1_score']\n",
    "print(f\"{best_model_name}: F1={best_f1:.4f}\")\n",
    "\n",
    "if best_f1 > best_dummy_f1:\n",
    "    print(\"✓ El mejor modelo SUPERA el baseline DummyClassifier\")\n",
    "else:\n",
    "    print(\"✗ El mejor modelo NO supera el baseline DummyClassifier\")\n",
    "    print(\"  Necesitas más técnicas de manejo de desbalance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REENTRENAMIENTO CON TODOS LOS DATOS\n",
      "================================================================================\n",
      "Mejor modelo seleccionado: RandomForestClassifier_Test\n",
      "Hiperparámetros óptimos: {'bootstrap': True, 'class_weight': 'balanced', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Threshold óptimo: 0.650\n",
      "\n",
      "Preparando todos los datos para reentrenamiento...\n",
      "Datos completos: (7999, 2)\n",
      "Distribución target completa: [7816, 183]\n",
      "\n",
      "Creando modelo final con hiperparámetros optimizados...\n",
      "Modelo configurado: RandomForestClassifier(class_weight='balanced', max_depth=5, min_samples_leaf=4,\n",
      "                       min_samples_split=10, n_estimators=200, random_state=42)\n",
      "\n",
      "Entrenando modelo final con 7999 registros...\n",
      "✓ Entrenamiento completado\n",
      "\n",
      "Validación del modelo final:\n",
      "Métricas en datos completos:\n",
      "  Accuracy:  0.9645\n",
      "  F1-Score:  0.1341\n",
      "  Precision: 0.1517\n",
      "  Recall:    0.1202\n",
      "================================================================================\n",
      "RESUMEN FINAL - CLASIFICACION GASTOS MEDICOS RC\n",
      "================================================================================\n",
      "Dataset original: 7,999 registros\n",
      "Distribución: 183/7999 positivos (2.3%)\n",
      "\n",
      "MODELO FINAL SELECCIONADO:\n",
      "  Algoritmo: RandomForestClassifier_Test\n",
      "  Entrenado con: 7,999 registros completos\n",
      "  Variables: 2 seleccionadas\n",
      "  Threshold: 0.650\n",
      "\n",
      "HIPERPARÁMETROS OPTIMIZADOS:\n",
      "  bootstrap: True\n",
      "  class_weight: balanced\n",
      "  max_depth: 5\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 10\n",
      "  n_estimators: 200\n",
      "\n",
      "RENDIMIENTO EN TEST SET ORIGINAL:\n",
      "  F1-Score:         0.0588\n",
      "  Precision:        0.0645\n",
      "  Recall:           0.0541\n",
      "  GINI:             0.1500\n",
      "  PR-AUC:           0.0289\n",
      "\n",
      "VARIABLES SELECCIONADAS (2):\n",
      "   1. distancia_al_campus\n",
      "   2. 2_o_mas_inquilinos_Si\n",
      "\n",
      "MODELO GUARDADO EN: ../models/clasificacion_medicos_best_model.pkl\n",
      "MEJORA SOBRE BASELINE: 1.5x mejor F1-score que DummyClassifier\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "best_model_name = max(final_results.keys(), key=lambda k: final_results[k]['f1_score'])\n",
    "best_model = final_results[best_model_name]['model']\n",
    "best_threshold = final_results[best_model_name]['threshold']\n",
    "best_params = optimization_results[best_model_name]['best_params']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REENTRENAMIENTO CON TODOS LOS DATOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mejor modelo seleccionado: {best_model_name}\")\n",
    "print(f\"Hiperparámetros óptimos: {best_params}\")\n",
    "print(f\"Threshold óptimo: {best_threshold:.3f}\")\n",
    "\n",
    "# Preparar TODOS los datos (8000 registros) con las variables seleccionadas\n",
    "print(f\"\\nPreparando todos los datos para reentrenamiento...\")\n",
    "X_all_selected = X_transformed[:, final_selected_indices]\n",
    "y_all = y.copy()\n",
    "\n",
    "print(f\"Datos completos: {X_all_selected.shape}\")\n",
    "print(f\"Distribución target completa: {y_all.value_counts().tolist()}\")\n",
    "\n",
    "# Crear nuevo modelo con los mismos hiperparámetros\n",
    "print(f\"\\nCreando modelo final con hiperparámetros optimizados...\")\n",
    "\n",
    "# Mapear el modelo base según el tipo\n",
    "model_mapping_final = {\n",
    "    'RandomForestClassifier_Test': RandomForestClassifier(random_state=42),\n",
    "    'XGBClassifier': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVC': SVC(random_state=42, probability=True),\n",
    "    'MLPClassifier': MLPClassifier(random_state=42, max_iter=500),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(random_state=42),\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "if best_model_name in model_mapping_final:\n",
    "    final_model = model_mapping_final[best_model_name]\n",
    "else:\n",
    "    final_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# Aplicar hiperparámetros optimizados\n",
    "final_model.set_params(**best_params)\n",
    "print(f\"Modelo configurado: {final_model}\")\n",
    "\n",
    "# Entrenar con TODOS los datos\n",
    "print(f\"\\nEntrenando modelo final con {X_all_selected.shape[0]} registros...\")\n",
    "final_model.fit(X_all_selected, y_all)\n",
    "print(\"✓ Entrenamiento completado\")\n",
    "\n",
    "# Validar que el modelo funciona\n",
    "print(f\"\\nValidación del modelo final:\")\n",
    "y_proba_all = final_model.predict_proba(X_all_selected)[:, 1]\n",
    "y_pred_all = (y_proba_all >= best_threshold).astype(int)\n",
    "\n",
    "final_accuracy = accuracy_score(y_all, y_pred_all)\n",
    "final_f1 = f1_score(y_all, y_pred_all)\n",
    "final_precision = precision_score(y_all, y_pred_all, zero_division=0)\n",
    "final_recall = recall_score(y_all, y_pred_all, zero_division=0)\n",
    "\n",
    "print(f\"Métricas en datos completos:\")\n",
    "print(f\"  Accuracy:  {final_accuracy:.4f}\")\n",
    "print(f\"  F1-Score:  {final_f1:.4f}\")\n",
    "print(f\"  Precision: {final_precision:.4f}\")\n",
    "print(f\"  Recall:    {final_recall:.4f}\")\n",
    "\n",
    "# Preparar información completa para guardar\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "model_info_final = {\n",
    "    'model': final_model,  # Modelo entrenado con todos los datos\n",
    "    'threshold': best_threshold,\n",
    "    'preprocessor': preprocessor,\n",
    "    'selected_indices': final_selected_indices,\n",
    "    'feature_names': final_selected_vars,\n",
    "    'best_params': best_params,\n",
    "    'model_name': best_model_name,\n",
    "    'training_metrics_full_data': {\n",
    "        'accuracy': final_accuracy,\n",
    "        'f1_score': final_f1,\n",
    "        'precision': final_precision,\n",
    "        'recall': final_recall\n",
    "    },\n",
    "    'test_metrics_original': final_results[best_model_name],  # Métricas en test set original\n",
    "    'feature_selection_strategy': 'Backward',\n",
    "    'resampling_strategy': 'Original',\n",
    "    'total_training_samples': X_all_selected.shape[0],\n",
    "    'all_strategies_tested': {\n",
    "        'feature_selection': list(feature_selection_strategies.keys()),\n",
    "        'resampling': list(resampling_strategies.keys())\n",
    "    },\n",
    "    'dummy_baseline': dummy_results,\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "joblib.dump(model_info_final, '../models/clasificacion_medicos_best_model.pkl')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESUMEN FINAL - CLASIFICACION GASTOS MEDICOS RC\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset original: {df.shape[0]:,} registros\")\n",
    "print(f\"Distribución: {sum(df['target'])}/{len(df['target'])} positivos ({100*sum(df['target'])/len(df['target']):.1f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"MODELO FINAL SELECCIONADO:\")\n",
    "print(f\"  Algoritmo: {best_model_name}\")\n",
    "print(f\"  Entrenado con: {X_all_selected.shape[0]:,} registros completos\")\n",
    "print(f\"  Variables: {len(final_selected_vars)} seleccionadas\")\n",
    "print(f\"  Threshold: {best_threshold:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"HIPERPARÁMETROS OPTIMIZADOS:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print()\n",
    "\n",
    "print(\"RENDIMIENTO EN TEST SET ORIGINAL:\")\n",
    "test_metrics = final_results[best_model_name]\n",
    "print(f\"  F1-Score:         {test_metrics['f1_score']:.4f}\")\n",
    "print(f\"  Precision:        {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:           {test_metrics['recall']:.4f}\")\n",
    "print(f\"  GINI:             {test_metrics['gini']:.4f}\")\n",
    "print(f\"  PR-AUC:           {test_metrics['pr_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nVARIABLES SELECCIONADAS ({len(final_selected_vars)}):\")\n",
    "for i, var in enumerate(final_selected_vars, 1):\n",
    "    print(f\"  {i:2d}. {var}\")\n",
    "\n",
    "print(f\"\\nMODELO GUARDADO EN: ../models/clasificacion_medicos_best_model.pkl\")\n",
    "\n",
    "best_dummy_f1 = max([res['f1_score'] for res in dummy_results.values()])\n",
    "improvement = test_metrics['f1_score'] / best_dummy_f1 if best_dummy_f1 > 0 else float('inf')\n",
    "print(f\"MEJORA SOBRE BASELINE: {improvement:.1f}x mejor F1-score que DummyClassifier\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
