{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score, average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.ensemble import BalancedRandomForestClassifier, BalancedBaggingClassifier, EasyEnsembleClassifier\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LGBM_AVAILABLE = True\n",
    "except:\n",
    "    LGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7999, 9)\n",
      "Resp_Civil_siniestros_num\n",
      "0.00    7932\n",
      "1.00      67\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/rc_full.csv')\n",
    "print(df.shape)\n",
    "print(df['Resp_Civil_siniestros_num'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    7932\n",
      "1      67\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['target'] = (df['Resp_Civil_siniestros_num'] > 0).astype(int)\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles en el dataset:\n",
      "  año_cursado\n",
      "  estudios_area\n",
      "  calif_promedio\n",
      "  2_o_mas_inquilinos\n",
      "  distancia_al_campus\n",
      "  genero\n",
      "  extintor_incendios\n",
      "  Resp_Civil_siniestros_num\n",
      "  Resp_Civil_siniestros_monto\n",
      "  target\n",
      "\n",
      "Variable 'tiene_monto' creada: correlación con target = 1.0000\n",
      "✗ Variable 'tiene_monto' muy correlacionada con target - no agregada\n",
      "\n",
      "Variables finales seleccionadas (7):\n",
      "   1. año_cursado\n",
      "   2. estudios_area\n",
      "   3. calif_promedio\n",
      "   4. 2_o_mas_inquilinos\n",
      "   5. distancia_al_campus\n",
      "   6. genero\n",
      "   7. extintor_incendios\n",
      "\n",
      "Categóricas: ['año_cursado', 'estudios_area', 'genero', '2_o_mas_inquilinos', 'extintor_incendios']\n",
      "Numéricas: ['calif_promedio', 'distancia_al_campus']\n"
     ]
    }
   ],
   "source": [
    "feature_vars = ['año_cursado', 'estudios_area', 'calif_promedio', '2_o_mas_inquilinos', \n",
    "                'distancia_al_campus', 'genero', 'extintor_incendios']\n",
    "\n",
    "print(\"Columnas disponibles en el dataset:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "if 'Resp_Civil_siniestros_monto' in df.columns:\n",
    "    df['tiene_monto'] = (df['Resp_Civil_siniestros_monto'] > 0).astype(int)\n",
    "    print(f\"\\nVariable 'tiene_monto' creada: correlación con target = {df['tiene_monto'].corr(df['target']):.4f}\")\n",
    "    \n",
    "    if df['tiene_monto'].corr(df['target']) < 0.99:\n",
    "        feature_vars.append('tiene_monto')\n",
    "        print(\"✓ Variable 'tiene_monto' agregada como predictora\")\n",
    "    else:\n",
    "        print(\"✗ Variable 'tiene_monto' muy correlacionada con target - no agregada\")\n",
    "\n",
    "X = df[feature_vars].copy()\n",
    "y = df['target'].copy()\n",
    "\n",
    "categorical_features = ['año_cursado', 'estudios_area', 'genero', '2_o_mas_inquilinos', 'extintor_incendios']\n",
    "numerical_features = ['calif_promedio', 'distancia_al_campus']\n",
    "\n",
    "if 'tiene_monto' in feature_vars:\n",
    "    categorical_features.append('tiene_monto')\n",
    "\n",
    "print(f\"\\nVariables finales seleccionadas ({len(feature_vars)}):\")\n",
    "for i, var in enumerate(feature_vars, 1):\n",
    "    print(f\"  {i:2d}. {var}\")\n",
    "    \n",
    "print(f\"\\nCategóricas: {categorical_features}\")\n",
    "print(f\"Numéricas: {numerical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed shape: (7999, 14)\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "feature_names = (numerical_features + \n",
    "                 list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "\n",
    "print(f\"Transformed shape: {X_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (6399, 14), Test: (1600, 14)\n",
      "Train target dist: [6345, 54]\n",
      "Test target dist: [1587, 13]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_transformed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Train target dist: {y_train.value_counts().tolist()}\")\n",
    "print(f\"Test target dist: {y_test.value_counts().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selection(X, y, feature_names, significance_level=0.05):\n",
    "    X_with_const = sm.add_constant(X)\n",
    "    current_features = feature_names.copy()\n",
    "    \n",
    "    while True:\n",
    "        feature_indices = [feature_names.index(f) for f in current_features]\n",
    "        current_X = X_with_const[:, [0] + [i+1 for i in feature_indices]]\n",
    "        \n",
    "        model = Logit(y, current_X).fit(disp=0)\n",
    "        p_values = model.pvalues[1:]\n",
    "        max_p_value = p_values.max()\n",
    "        \n",
    "        if max_p_value > significance_level:\n",
    "            worst_idx_in_pvalues = p_values.idxmax()\n",
    "            worst_idx_in_current = list(p_values.index).index(worst_idx_in_pvalues)\n",
    "            feature_to_remove = current_features[worst_idx_in_current]\n",
    "            \n",
    "            current_features.remove(feature_to_remove)\n",
    "            print(f\"Removed {feature_to_remove}, p-value: {max_p_value:.4f}\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    feature_indices = [feature_names.index(f) for f in current_features]\n",
    "    final_X = X_with_const[:, [0] + [i+1 for i in feature_indices]]\n",
    "    final_model = Logit(y, final_X).fit(disp=0)\n",
    "    \n",
    "    selected_indices = [feature_names.index(f) for f in current_features]\n",
    "    \n",
    "    return current_features, selected_indices, final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUACION BASELINE - DUMMYCLASSIFIER\n",
      "==================================================\n",
      "Most_Frequent: Acc=0.9919, F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "Stratified: Acc=0.9862, F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "Uniform: Acc=0.4956, F1=0.0049, Prec=0.0025, Rec=0.1538\n",
      "\n",
      "SELECCION BACKWARD DE VARIABLES\n",
      "==================================================\n",
      "Removed genero_Otro, p-value: 0.9556\n",
      "Removed extintor_incendios_Si, p-value: 0.7371\n",
      "Removed genero_Masculino, p-value: 0.7185\n",
      "Removed estudios_area_Ciencias, p-value: 0.6837\n",
      "Removed estudios_area_Otro, p-value: 0.7494\n",
      "Removed estudios_area_Humanidades, p-value: 0.6021\n",
      "Removed año_cursado_posgrado, p-value: 0.5407\n",
      "Removed año_cursado_4to año, p-value: 0.5401\n",
      "Removed año_cursado_3er año, p-value: 0.4401\n",
      "Removed calif_promedio, p-value: 0.3869\n",
      "Removed genero_No respuesta, p-value: 0.2722\n",
      "Removed año_cursado_2do año, p-value: 0.1045\n",
      "Variables seleccionadas: 2\n",
      "['distancia_al_campus', '2_o_mas_inquilinos_Si']\n"
     ]
    }
   ],
   "source": [
    "print(\"EVALUACION BASELINE - DUMMYCLASSIFIER\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "dummy_models = {\n",
    "    'Most_Frequent': DummyClassifier(strategy=\"most_frequent\", random_state=42),\n",
    "    'Stratified': DummyClassifier(strategy=\"stratified\", random_state=42),\n",
    "    'Uniform': DummyClassifier(strategy=\"uniform\", random_state=42)\n",
    "}\n",
    "\n",
    "dummy_results = {}\n",
    "for name, dummy in dummy_models.items():\n",
    "    dummy.fit(X_train, y_train)\n",
    "    y_pred_dummy = dummy.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_dummy)\n",
    "    f1 = f1_score(y_test, y_pred_dummy, zero_division=0)\n",
    "    precision = precision_score(y_test, y_pred_dummy, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_dummy, zero_division=0)\n",
    "    \n",
    "    dummy_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}: Acc={accuracy:.4f}, F1={f1:.4f}, Prec={precision:.4f}, Rec={recall:.4f}\")\n",
    "\n",
    "print(\"\\nSELECCION BACKWARD DE VARIABLES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "selected_vars, selected_indices, glm_model = backward_selection(X_train, y_train, feature_names)\n",
    "print(f\"Variables seleccionadas: {len(selected_vars)}\")\n",
    "print(selected_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTRATEGIAS DE SELECCION DE VARIABLES\n",
      "==================================================\n",
      "Estrategias de seleccion creadas:\n",
      "  Backward: 2 variables\n",
      "  All_Features: 14 variables\n",
      "  KBest_f_classif_5: 5 variables\n",
      "  KBest_f_classif_8: 8 variables\n",
      "  KBest_f_classif_10: 10 variables\n",
      "  KBest_mutual_info_5: 5 variables\n",
      "  KBest_mutual_info_8: 8 variables\n",
      "  KBest_mutual_info_10: 10 variables\n",
      "\n",
      "Usando estrategia 'Backward' para LazyPredict inicial...\n"
     ]
    }
   ],
   "source": [
    "print(\"ESTRATEGIAS DE SELECCION DE VARIABLES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "feature_selection_strategies = {}\n",
    "\n",
    "categorical_base_names = ['año_cursado', 'estudios_area', 'genero', '2_o_mas_inquilinos', 'extintor_incendios']\n",
    "backward_selected_indices = []\n",
    "backward_selected_vars = []\n",
    "\n",
    "for var in selected_vars:\n",
    "    if var in numerical_features:\n",
    "        backward_selected_indices.append(feature_names.index(var))\n",
    "        backward_selected_vars.append(var)\n",
    "\n",
    "for base_name in categorical_base_names:\n",
    "    categories_in_selected = [var for var in selected_vars if var.startswith(base_name)]\n",
    "    if categories_in_selected:\n",
    "        all_categories = [var for var in feature_names if var.startswith(base_name)]\n",
    "        for cat_var in all_categories:\n",
    "            if cat_var not in backward_selected_vars:\n",
    "                backward_selected_indices.append(feature_names.index(cat_var))\n",
    "                backward_selected_vars.append(cat_var)\n",
    "\n",
    "feature_selection_strategies['Backward'] = {\n",
    "    'indices': backward_selected_indices,\n",
    "    'names': backward_selected_vars\n",
    "}\n",
    "\n",
    "feature_selection_strategies['All_Features'] = {\n",
    "    'indices': list(range(len(feature_names))),\n",
    "    'names': feature_names\n",
    "}\n",
    "\n",
    "for k in [5, 8, 10]:\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_selected = selector.fit_transform(X_train, y_train)\n",
    "    selected_features_mask = selector.get_support()\n",
    "    kbest_indices = [i for i, selected in enumerate(selected_features_mask) if selected]\n",
    "    kbest_names = [feature_names[i] for i in kbest_indices]\n",
    "    \n",
    "    feature_selection_strategies[f'KBest_f_classif_{k}'] = {\n",
    "        'indices': kbest_indices,\n",
    "        'names': kbest_names\n",
    "    }\n",
    "\n",
    "for k in [5, 8, 10]:\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    X_selected = selector.fit_transform(X_train, y_train)\n",
    "    selected_features_mask = selector.get_support()\n",
    "    mi_indices = [i for i, selected in enumerate(selected_features_mask) if selected]\n",
    "    mi_names = [feature_names[i] for i in mi_indices]\n",
    "    \n",
    "    feature_selection_strategies[f'KBest_mutual_info_{k}'] = {\n",
    "        'indices': mi_indices,\n",
    "        'names': mi_names\n",
    "    }\n",
    "\n",
    "print(\"Estrategias de seleccion creadas:\")\n",
    "for strategy_name, strategy_info in feature_selection_strategies.items():\n",
    "    print(f\"  {strategy_name}: {len(strategy_info['indices'])} variables\")\n",
    "\n",
    "print(f\"\\nUsando estrategia 'Backward' para LazyPredict inicial...\")\n",
    "final_selected_indices = feature_selection_strategies['Backward']['indices']\n",
    "final_selected_vars = feature_selection_strategies['Backward']['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECNICAS DE RESAMPLING\n",
      "==================================================\n",
      "SMOTE: [6345, 6345]\n",
      "ADASYN: [6345, 6318]\n",
      "UnderSample: [54, 54]\n",
      "\n",
      "Distribucion original: [6345, 54]\n",
      "Estrategias de resampling disponibles: ['Original', 'SMOTE', 'ADASYN', 'UnderSample']\n"
     ]
    }
   ],
   "source": [
    "print(\"TECNICAS DE RESAMPLING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_train_selected = X_train[:, final_selected_indices]\n",
    "X_test_selected = X_test[:, final_selected_indices]\n",
    "\n",
    "resampling_strategies = {}\n",
    "\n",
    "resampling_strategies['Original'] = {\n",
    "    'X_train': X_train_selected,\n",
    "    'y_train': y_train\n",
    "}\n",
    "\n",
    "if IMBLEARN_AVAILABLE:\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train_selected, y_train)\n",
    "        resampling_strategies['SMOTE'] = {\n",
    "            'X_train': X_train_smote,\n",
    "            'y_train': y_train_smote\n",
    "        }\n",
    "        print(f\"SMOTE: {y_train_smote.value_counts().tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"SMOTE error: {e}\")\n",
    "\n",
    "    try:\n",
    "        adasyn = ADASYN(random_state=42)\n",
    "        X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_selected, y_train)\n",
    "        resampling_strategies['ADASYN'] = {\n",
    "            'X_train': X_train_adasyn,\n",
    "            'y_train': y_train_adasyn\n",
    "        }\n",
    "        print(f\"ADASYN: {y_train_adasyn.value_counts().tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ADASYN error: {e}\")\n",
    "\n",
    "    try:\n",
    "        undersampler = RandomUnderSampler(random_state=42)\n",
    "        X_train_under, y_train_under = undersampler.fit_resample(X_train_selected, y_train)\n",
    "        resampling_strategies['UnderSample'] = {\n",
    "            'X_train': X_train_under,\n",
    "            'y_train': y_train_under\n",
    "        }\n",
    "        print(f\"UnderSample: {y_train_under.value_counts().tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"UnderSample error: {e}\")\n",
    "else:\n",
    "    print(\"imblearn no disponible - usando solo datos originales\")\n",
    "\n",
    "print(f\"\\nDistribucion original: {y_train.value_counts().tolist()}\")\n",
    "print(f\"Estrategias de resampling disponibles: {list(resampling_strategies.keys())}\")\n",
    "\n",
    "current_X_train = X_train_selected\n",
    "current_y_train = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAZYPREDICT - EXPLORACION DE MODELOS\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f94aad57b648508b285c486be8a066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 54, number of negative: 6345\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 258\n",
      "[LightGBM] [Info] Number of data points in the train set: 6399, number of used features: 2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008439 -> initscore=-4.766438\n",
      "[LightGBM] [Info] Start training from score -4.766438\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Resultados LazyPredict:\n",
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "NearestCentroid                    0.72               0.55     0.55      0.83   \n",
      "BernoulliNB                        0.99               0.50     0.50      0.99   \n",
      "AdaBoostClassifier                 0.99               0.50     0.50      0.99   \n",
      "DummyClassifier                    0.99               0.50     0.50      0.99   \n",
      "CategoricalNB                      0.99               0.50     0.50      0.99   \n",
      "KNeighborsClassifier               0.99               0.50     0.50      0.99   \n",
      "LinearSVC                          0.99               0.50     0.50      0.99   \n",
      "LabelSpreading                     0.99               0.50     0.50      0.99   \n",
      "LinearDiscriminantAnalysis         0.99               0.50     0.50      0.99   \n",
      "LabelPropagation                   0.99               0.50     0.50      0.99   \n",
      "CalibratedClassifierCV             0.99               0.50     0.50      0.99   \n",
      "RidgeClassifierCV                  0.99               0.50     0.50      0.99   \n",
      "RidgeClassifier                    0.99               0.50     0.50      0.99   \n",
      "SVC                                0.99               0.50     0.50      0.99   \n",
      "SGDClassifier                      0.99               0.50     0.50      0.99   \n",
      "XGBClassifier                      0.99               0.50     0.50      0.99   \n",
      "LogisticRegression                 0.99               0.50     0.50      0.99   \n",
      "PassiveAggressiveClassifier        0.99               0.50     0.50      0.99   \n",
      "Perceptron                         0.99               0.50     0.50      0.99   \n",
      "LGBMClassifier                     0.99               0.50     0.50      0.99   \n",
      "BaggingClassifier                  0.99               0.50     0.50      0.99   \n",
      "DecisionTreeClassifier             0.99               0.50     0.50      0.99   \n",
      "ExtraTreeClassifier                0.99               0.50     0.50      0.98   \n",
      "RandomForestClassifier             0.99               0.50     0.50      0.98   \n",
      "ExtraTreesClassifier               0.98               0.50     0.50      0.98   \n",
      "GaussianNB                         0.98               0.50     0.50      0.98   \n",
      "QuadraticDiscriminantAnalysis      0.98               0.50     0.50      0.98   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "NearestCentroid                      0.03  \n",
      "BernoulliNB                          0.03  \n",
      "AdaBoostClassifier                   0.27  \n",
      "DummyClassifier                      0.03  \n",
      "CategoricalNB                        0.03  \n",
      "KNeighborsClassifier                 0.15  \n",
      "LinearSVC                            0.04  \n",
      "LabelSpreading                       2.26  \n",
      "LinearDiscriminantAnalysis           0.04  \n",
      "LabelPropagation                     1.56  \n",
      "CalibratedClassifierCV               0.09  \n",
      "RidgeClassifierCV                    0.03  \n",
      "RidgeClassifier                      0.02  \n",
      "SVC                                  0.17  \n",
      "SGDClassifier                        0.03  \n",
      "XGBClassifier                        4.44  \n",
      "LogisticRegression                   0.03  \n",
      "PassiveAggressiveClassifier          0.03  \n",
      "Perceptron                           0.03  \n",
      "LGBMClassifier                       0.38  \n",
      "BaggingClassifier                    0.09  \n",
      "DecisionTreeClassifier               0.03  \n",
      "ExtraTreeClassifier                  0.02  \n",
      "RandomForestClassifier               0.44  \n",
      "ExtraTreesClassifier                 0.32  \n",
      "GaussianNB                           0.02  \n",
      "QuadraticDiscriminantAnalysis        0.05  \n",
      "\n",
      "TOP 10 MODELOS POR F1-SCORE:\n",
      " 1. BernoulliNB              : F1=0.9878, Acc=0.9919\n",
      " 2. AdaBoostClassifier       : F1=0.9878, Acc=0.9919\n",
      " 3. DummyClassifier          : F1=0.9878, Acc=0.9919\n",
      " 4. CategoricalNB            : F1=0.9878, Acc=0.9919\n",
      " 5. KNeighborsClassifier     : F1=0.9878, Acc=0.9919\n",
      " 6. LinearSVC                : F1=0.9878, Acc=0.9919\n",
      " 7. LabelSpreading           : F1=0.9878, Acc=0.9919\n",
      " 8. LinearDiscriminantAnalysis: F1=0.9878, Acc=0.9919\n",
      " 9. LabelPropagation         : F1=0.9878, Acc=0.9919\n",
      "10. CalibratedClassifierCV   : F1=0.9878, Acc=0.9919\n",
      "\n",
      "Comparacion con baseline:\n",
      "Mejor DummyClassifier F1: 0.0049\n",
      "Mejor LazyPredict F1:    0.9878\n",
      "✓ LazyPredict supera baseline DummyClassifier\n",
      "\n",
      "Seleccionando top 5 para optimizacion: ['BernoulliNB', 'AdaBoostClassifier', 'DummyClassifier', 'CategoricalNB', 'KNeighborsClassifier']\n"
     ]
    }
   ],
   "source": [
    "print(\"LAZYPREDICT - EXPLORACION DE MODELOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(current_X_train, X_test_selected, current_y_train, y_test)\n",
    "\n",
    "print(\"Resultados LazyPredict:\")\n",
    "print(models.round(4))\n",
    "\n",
    "top_10_models = models.nlargest(10, 'F1 Score')\n",
    "print(f\"\\nTOP 10 MODELOS POR F1-SCORE:\")\n",
    "for i, (model_name, row) in enumerate(top_10_models.iterrows(), 1):\n",
    "    print(f\"{i:2d}. {model_name:25s}: F1={row['F1 Score']:.4f}, Acc={row['Accuracy']:.4f}\")\n",
    "\n",
    "best_f1_lazy = top_10_models.iloc[0]['F1 Score']\n",
    "best_dummy_f1 = max([res['f1_score'] for res in dummy_results.values()])\n",
    "\n",
    "print(f\"\\nComparacion con baseline:\")\n",
    "print(f\"Mejor DummyClassifier F1: {best_dummy_f1:.4f}\")\n",
    "print(f\"Mejor LazyPredict F1:    {best_f1_lazy:.4f}\")\n",
    "if best_f1_lazy > best_dummy_f1:\n",
    "    print(\"✓ LazyPredict supera baseline DummyClassifier\")\n",
    "else:\n",
    "    print(\"✗ LazyPredict NO supera baseline DummyClassifier\")\n",
    "\n",
    "top_5_names = top_10_models.head(5).index.tolist()\n",
    "print(f\"\\nSeleccionando top 5 para optimizacion: {top_5_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPEO Y CONFIGURACION DE MODELOS EXPANDIDA\n",
      "==================================================\n",
      "✓ imblearn modelos balanceados disponibles\n",
      "✓ LightGBM disponible\n",
      "✗ CatBoost no disponible\n",
      "\n",
      "+ Agregando RandomForestClassifier adicional para testing\n",
      "○ BernoulliNB usando LogisticRegression como backup\n",
      "✓ AdaBoostClassifier mapeado correctamente\n",
      "○ DummyClassifier usando LogisticRegression como backup\n",
      "○ CategoricalNB usando LogisticRegression como backup\n",
      "✓ KNeighborsClassifier mapeado correctamente\n",
      "\n",
      "Modelos a optimizar: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPEO Y CONFIGURACION DE MODELOS EXPANDIDA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model_mapping = {\n",
    "    'XGBClassifier': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVC': SVC(random_state=42, probability=True),\n",
    "    'MLPClassifier': MLPClassifier(random_state=42, max_iter=500),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(random_state=42),\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "if IMBLEARN_AVAILABLE:\n",
    "    model_mapping['BalancedRandomForestClassifier'] = BalancedRandomForestClassifier(random_state=42)\n",
    "    model_mapping['BalancedBaggingClassifier'] = BalancedBaggingClassifier(random_state=42)\n",
    "    model_mapping['EasyEnsembleClassifier'] = EasyEnsembleClassifier(random_state=42)\n",
    "    print(\"✓ imblearn modelos balanceados disponibles\")\n",
    "else:\n",
    "    print(\"✗ imblearn no disponible - sin modelos balanceados\")\n",
    "\n",
    "if LGBM_AVAILABLE:\n",
    "    model_mapping['LGBMClassifier'] = LGBMClassifier(random_state=42, verbose=-1)\n",
    "    print(\"✓ LightGBM disponible\")\n",
    "else:\n",
    "    print(\"✗ LightGBM no disponible\")\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    model_mapping['CatBoostClassifier'] = CatBoostClassifier(random_state=42, verbose=False)\n",
    "    print(\"✓ CatBoost disponible\")\n",
    "else:\n",
    "    print(\"✗ CatBoost no disponible\")\n",
    "\n",
    "print(\"\\n+ Agregando RandomForestClassifier adicional para testing\")\n",
    "models_to_optimize = [('RandomForestClassifier_Test', RandomForestClassifier(random_state=42))]\n",
    "\n",
    "for name in top_5_names:\n",
    "    if name in model_mapping:\n",
    "        models_to_optimize.append((name, model_mapping[name]))\n",
    "        print(f\"✓ {name} mapeado correctamente\")\n",
    "    else:\n",
    "        models_to_optimize.append((name, LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')))\n",
    "        print(f\"○ {name} usando LogisticRegression como backup\")\n",
    "\n",
    "print(f\"\\nModelos a optimizar: {len(models_to_optimize)}\")\n",
    "\n",
    "param_grids_fast = {\n",
    "    'RandomForestClassifier_Test': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [5, 10, None],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'min_samples_leaf': [1, 4],\n",
    "        'max_features': ['sqrt', 0.5],\n",
    "        'class_weight': ['balanced', {0: 1, 1: 15}],\n",
    "        'bootstrap': [True]\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.1, 0.2],\n",
    "        'scale_pos_weight': [10, 20]\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15],\n",
    "        'class_weight': ['balanced', {0: 1, 1: 15}]\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.1, 0.2],\n",
    "        'max_depth': [3, 5]\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'class_weight': ['balanced', {0: 1, 1: 15}]\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [1, 10],\n",
    "        'kernel': ['rbf'],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'hidden_layer_sizes': [(50,), (100,)],\n",
    "        'alpha': [0.001, 0.01]\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [1.0, 1.5]\n",
    "    },\n",
    "    'ExtraTreesClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors': [5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }\n",
    "}\n",
    "\n",
    "if IMBLEARN_AVAILABLE:\n",
    "    param_grids_fast['BalancedRandomForestClassifier'] = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15],\n",
    "        'sampling_strategy': ['auto']\n",
    "    }\n",
    "\n",
    "if LGBM_AVAILABLE:\n",
    "    param_grids_fast['LGBMClassifier'] = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.1, 0.2],\n",
    "        'is_unbalance': [True]\n",
    "    }\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZACION DE HIPERPARAMETROS (ACELERADA)\n",
      "==================================================\n",
      "\n",
      "Optimizing RandomForestClassifier_Test...\n",
      "  Grid reducido: 7 parámetros\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "  ✓ Best F1-CV: 0.0358\n",
      "  ✓ Best params: {'bootstrap': True, 'class_weight': 'balanced', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "Optimizing BernoulliNB...\n",
      "  Sin grid específico\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "  ✓ Best F1-CV: 0.0348\n",
      "  ✓ Best params: {}\n",
      "\n",
      "Optimizing AdaBoostClassifier...\n",
      "  Grid reducido: 2 parámetros\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "  ✓ Best F1-CV: 0.0000\n",
      "  ✓ Best params: {'learning_rate': 1.0, 'n_estimators': 100}\n",
      "\n",
      "Optimizing DummyClassifier...\n",
      "  Sin grid específico\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "  ✓ Best F1-CV: 0.0348\n",
      "  ✓ Best params: {}\n",
      "\n",
      "Optimizing CategoricalNB...\n",
      "  Sin grid específico\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "  ✓ Best F1-CV: 0.0348\n",
      "  ✓ Best params: {}\n",
      "\n",
      "Optimizing KNeighborsClassifier...\n",
      "  Grid reducido: 2 parámetros\n",
      "  Iniciando búsqueda...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "  ✓ Best F1-CV: 0.0000\n",
      "  ✓ Best params: {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "\n",
      "==================================================\n",
      "OPTIMIZATION COMPLETED\n",
      "==================================================\n",
      "Resultados de optimización:\n",
      "RandomForestClassifier_Test: F1-CV = 0.0358\n",
      "BernoulliNB: F1-CV = 0.0348\n",
      "AdaBoostClassifier: F1-CV = 0.0000\n",
      "DummyClassifier: F1-CV = 0.0348\n",
      "CategoricalNB: F1-CV = 0.0348\n",
      "KNeighborsClassifier: F1-CV = 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"OPTIMIZACION DE HIPERPARAMETROS (ACELERADA)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "optimization_results = {}\n",
    "\n",
    "for model_name, model in models_to_optimize:\n",
    "    print(f\"\\nOptimizing {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        if model_name in param_grids_fast:\n",
    "            param_grid = param_grids_fast[model_name]\n",
    "            print(f\"  Grid reducido: {len(param_grid)} parámetros\")\n",
    "        else:\n",
    "            param_grid = {}\n",
    "            print(f\"  Sin grid específico\")\n",
    "        \n",
    "        skf_fast = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            scoring='f1',\n",
    "            cv=skf_fast,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"  Iniciando búsqueda...\")\n",
    "        grid_search.fit(current_X_train, current_y_train)\n",
    "        \n",
    "        optimization_results[model_name] = {\n",
    "            'best_estimator': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_cv_score': grid_search.best_score_\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ Best F1-CV: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"  ✓ Best params: {grid_search.best_params_}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {str(e)}\")\n",
    "        optimization_results[model_name] = {\n",
    "            'best_estimator': model,\n",
    "            'best_params': {},\n",
    "            'best_cv_score': 0\n",
    "        }\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"OPTIMIZATION COMPLETED\")\n",
    "print(f\"{'='*50}\")\n",
    "print(\"Resultados de optimización:\")\n",
    "for name, results in optimization_results.items():\n",
    "    print(f\"{name}: F1-CV = {results['best_cv_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZACION DE THRESHOLDS\n",
      "==================================================\n",
      "RandomForestClassifier_Test: threshold=0.650, F1=0.0169\n",
      "BernoulliNB: threshold=0.650, F1=0.0408\n",
      "AdaBoostClassifier: threshold=0.250, F1=0.0219\n",
      "DummyClassifier: threshold=0.650, F1=0.0408\n",
      "CategoricalNB: threshold=0.650, F1=0.0408\n",
      "KNeighborsClassifier: threshold=0.500, F1=0.0000\n",
      "Threshold optimization completed\n"
     ]
    }
   ],
   "source": [
    "def optimize_threshold(model, X_val, y_val):\n",
    "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_f1\n",
    "\n",
    "print(\"OPTIMIZACION DE THRESHOLDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "threshold_results = {}\n",
    "\n",
    "for model_name, model_info in optimization_results.items():\n",
    "    model = model_info['best_estimator']\n",
    "    best_threshold, best_f1 = optimize_threshold(model, X_test_selected, y_test)\n",
    "    \n",
    "    threshold_results[model_name] = {\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_f1': best_f1,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name}: threshold={best_threshold:.3f}, F1={best_f1:.4f}\")\n",
    "\n",
    "print(\"Threshold optimization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUACION FINAL CON METRICAS MEJORADAS\n",
      "============================================================\n",
      "RandomForestClassifier_Test:\n",
      "  Threshold: 0.650\n",
      "  Accuracy:  0.8550\n",
      "  F1-Score:  0.0169\n",
      "  Precision: 0.0090\n",
      "  Recall:    0.1538\n",
      "  GINI:      -0.2520\n",
      "  PR-AUC:    0.0069\n",
      "  MCC:       0.0038\n",
      "  Bal-Acc:   0.5073\n",
      "\n",
      "BernoulliNB:\n",
      "  Threshold: 0.650\n",
      "  Accuracy:  0.9119\n",
      "  F1-Score:  0.0408\n",
      "  Precision: 0.0224\n",
      "  Recall:    0.2308\n",
      "  GINI:      0.2888\n",
      "  PR-AUC:    0.0131\n",
      "  MCC:       0.0480\n",
      "  Bal-Acc:   0.5741\n",
      "\n",
      "AdaBoostClassifier:\n",
      "  Threshold: 0.250\n",
      "  Accuracy:  0.6644\n",
      "  F1-Score:  0.0219\n",
      "  Precision: 0.0112\n",
      "  Recall:    0.4615\n",
      "  GINI:      0.1426\n",
      "  PR-AUC:    0.0097\n",
      "  MCC:       0.0243\n",
      "  Bal-Acc:   0.5638\n",
      "\n",
      "DummyClassifier:\n",
      "  Threshold: 0.650\n",
      "  Accuracy:  0.9119\n",
      "  F1-Score:  0.0408\n",
      "  Precision: 0.0224\n",
      "  Recall:    0.2308\n",
      "  GINI:      0.2888\n",
      "  PR-AUC:    0.0131\n",
      "  MCC:       0.0480\n",
      "  Bal-Acc:   0.5741\n",
      "\n",
      "CategoricalNB:\n",
      "  Threshold: 0.650\n",
      "  Accuracy:  0.9119\n",
      "  F1-Score:  0.0408\n",
      "  Precision: 0.0224\n",
      "  Recall:    0.2308\n",
      "  GINI:      0.2888\n",
      "  PR-AUC:    0.0131\n",
      "  MCC:       0.0480\n",
      "  Bal-Acc:   0.5741\n",
      "\n",
      "KNeighborsClassifier:\n",
      "  Threshold: 0.500\n",
      "  Accuracy:  0.9919\n",
      "  F1-Score:  0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "  GINI:      -0.0164\n",
      "  PR-AUC:    0.0081\n",
      "  MCC:       0.0000\n",
      "  Bal-Acc:   0.5000\n",
      "\n",
      "Comparacion con DummyClassifier:\n",
      "----------------------------------------\n",
      "Most_Frequent: F1=0.0000, Acc=0.9919\n",
      "Stratified: F1=0.0000, Acc=0.9862\n",
      "Uniform: F1=0.0049, Acc=0.4956\n",
      "\n",
      "Mejor modelo real:\n",
      "BernoulliNB: F1=0.0408\n",
      "✓ El mejor modelo SUPERA el baseline DummyClassifier\n"
     ]
    }
   ],
   "source": [
    "def calculate_enhanced_metrics(y_true, y_proba, y_pred):\n",
    "    from sklearn.metrics import roc_auc_score, matthews_corrcoef, balanced_accuracy_score, average_precision_score\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_proba)\n",
    "        gini = 2 * roc_auc - 1\n",
    "    except:\n",
    "        roc_auc = 0\n",
    "        gini = 0\n",
    "    \n",
    "    try:\n",
    "        pr_auc = average_precision_score(y_true, y_proba)\n",
    "    except:\n",
    "        pr_auc = 0\n",
    "    \n",
    "    try:\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    except:\n",
    "        mcc = 0\n",
    "    \n",
    "    try:\n",
    "        balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    except:\n",
    "        balanced_acc = 0\n",
    "    \n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    r2 = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'mae': mae,\n",
    "        'gini': gini,\n",
    "        'r2': r2,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'mcc': mcc,\n",
    "        'balanced_accuracy': balanced_acc\n",
    "    }\n",
    "\n",
    "final_results = {}\n",
    "\n",
    "print(\"EVALUACION FINAL CON METRICAS MEJORADAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, threshold_info in threshold_results.items():\n",
    "    model = threshold_info['model']\n",
    "    threshold = threshold_info['best_threshold']\n",
    "    \n",
    "    y_proba = model.predict_proba(X_test_selected)[:, 1]\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = calculate_enhanced_metrics(y_test, y_proba, y_pred)\n",
    "    metrics['threshold'] = threshold\n",
    "    metrics['model'] = model\n",
    "    \n",
    "    final_results[model_name] = metrics\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Threshold: {threshold:.3f}\")\n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  GINI:      {metrics['gini']:.4f}\")\n",
    "    print(f\"  PR-AUC:    {metrics['pr_auc']:.4f}\")\n",
    "    print(f\"  MCC:       {metrics['mcc']:.4f}\")\n",
    "    print(f\"  Bal-Acc:   {metrics['balanced_accuracy']:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"Comparacion con DummyClassifier:\")\n",
    "print(\"-\" * 40)\n",
    "for dummy_name, dummy_metrics in dummy_results.items():\n",
    "    print(f\"{dummy_name}: F1={dummy_metrics['f1_score']:.4f}, Acc={dummy_metrics['accuracy']:.4f}\")\n",
    "\n",
    "best_model_name = max(final_results.keys(), key=lambda k: final_results[k]['f1_score'])\n",
    "best_f1 = final_results[best_model_name]['f1_score']\n",
    "print(f\"\\nMejor modelo real:\")\n",
    "print(f\"{best_model_name}: F1={best_f1:.4f}\")\n",
    "\n",
    "if best_f1 > best_dummy_f1:\n",
    "    print(\"✓ El mejor modelo SUPERA el baseline DummyClassifier\")\n",
    "else:\n",
    "    print(\"✗ El mejor modelo NO supera el baseline DummyClassifier\")\n",
    "    print(\"  Necesitas más técnicas de manejo de desbalance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REENTRENAMIENTO CON TODOS LOS DATOS\n",
      "================================================================================\n",
      "Mejor modelo seleccionado: BernoulliNB\n",
      "Hiperparámetros óptimos: {}\n",
      "Threshold óptimo: 0.650\n",
      "\n",
      "Preparando todos los datos para reentrenamiento...\n",
      "Datos completos: (7999, 2)\n",
      "Distribución target completa: [7932, 67]\n",
      "\n",
      "Creando modelo final con hiperparámetros optimizados...\n",
      "Modelo configurado: LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
      "\n",
      "Entrenando modelo final con 7999 registros...\n",
      "✓ Entrenamiento completado\n",
      "\n",
      "Validación del modelo final:\n",
      "Métricas en datos completos:\n",
      "  Accuracy:  0.9104\n",
      "  F1-Score:  0.0376\n",
      "  Precision: 0.0206\n",
      "  Recall:    0.2090\n",
      "================================================================================\n",
      "RESUMEN FINAL - CLASIFICACION RESPONSABILIDAD CIVIL\n",
      "================================================================================\n",
      "Dataset original: 7,999 registros\n",
      "Distribución: 67/7999 positivos (0.8%)\n",
      "\n",
      "BASELINE DUMMYCLASSIFIER:\n",
      "  Most_Frequent  : F1=0.0000, Acc=0.9919\n",
      "  Stratified     : F1=0.0000, Acc=0.9862\n",
      "  Uniform        : F1=0.0049, Acc=0.4956\n",
      "\n",
      "MODELO FINAL SELECCIONADO:\n",
      "  Algoritmo: BernoulliNB\n",
      "  Entrenado con: 7,999 registros completos\n",
      "  Variables: 2 seleccionadas\n",
      "  Threshold: 0.650\n",
      "\n",
      "HIPERPARÁMETROS OPTIMIZADOS:\n",
      "\n",
      "RENDIMIENTO EN TEST SET ORIGINAL:\n",
      "  F1-Score:         0.0408\n",
      "  Precision:        0.0224\n",
      "  Recall:           0.2308\n",
      "  GINI:             0.2888\n",
      "  PR-AUC:           0.0131\n",
      "\n",
      "VARIABLES SELECCIONADAS (2):\n",
      "   1. distancia_al_campus\n",
      "   2. 2_o_mas_inquilinos_Si\n",
      "\n",
      "COMPARACION TODOS LOS MODELOS (por F1-Score):\n",
      "--------------------------------------------------\n",
      " 1. ✓ BernoulliNB              : F1=0.0408, PR-AUC=0.0131\n",
      " 2. ✓ DummyClassifier          : F1=0.0408, PR-AUC=0.0131\n",
      " 3. ✓ CategoricalNB            : F1=0.0408, PR-AUC=0.0131\n",
      " 4. ✓ AdaBoostClassifier       : F1=0.0219, PR-AUC=0.0097\n",
      " 5. ✓ RandomForestClassifier_Test: F1=0.0169, PR-AUC=0.0069\n",
      " 6. ✗ KNeighborsClassifier     : F1=0.0000, PR-AUC=0.0081\n",
      "\n",
      "MODELO GUARDADO EN: ../models/clasificacion_rc_best_model.pkl\n",
      "\n",
      "MEJORA SOBRE BASELINE: 8.3x mejor F1-score que DummyClassifier\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "best_model_name = max(final_results.keys(), key=lambda k: final_results[k]['f1_score'])\n",
    "best_model = final_results[best_model_name]['model']\n",
    "best_threshold = final_results[best_model_name]['threshold']\n",
    "best_params = optimization_results[best_model_name]['best_params']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REENTRENAMIENTO CON TODOS LOS DATOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mejor modelo seleccionado: {best_model_name}\")\n",
    "print(f\"Hiperparámetros óptimos: {best_params}\")\n",
    "print(f\"Threshold óptimo: {best_threshold:.3f}\")\n",
    "\n",
    "print(f\"\\nPreparando todos los datos para reentrenamiento...\")\n",
    "X_all_selected = X_transformed[:, final_selected_indices]\n",
    "y_all = y.copy()\n",
    "\n",
    "print(f\"Datos completos: {X_all_selected.shape}\")\n",
    "print(f\"Distribución target completa: {y_all.value_counts().tolist()}\")\n",
    "\n",
    "print(f\"\\nCreando modelo final con hiperparámetros optimizados...\")\n",
    "\n",
    "model_mapping_final = {\n",
    "    'RandomForestClassifier_Test': RandomForestClassifier(random_state=42),\n",
    "    'XGBClassifier': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVC': SVC(random_state=42, probability=True),\n",
    "    'MLPClassifier': MLPClassifier(random_state=42, max_iter=500),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(random_state=42),\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "if best_model_name in model_mapping_final:\n",
    "    final_model = model_mapping_final[best_model_name]\n",
    "else:\n",
    "    final_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "\n",
    "final_model.set_params(**best_params)\n",
    "print(f\"Modelo configurado: {final_model}\")\n",
    "\n",
    "print(f\"\\nEntrenando modelo final con {X_all_selected.shape[0]} registros...\")\n",
    "final_model.fit(X_all_selected, y_all)\n",
    "print(\"✓ Entrenamiento completado\")\n",
    "\n",
    "print(f\"\\nValidación del modelo final:\")\n",
    "y_proba_all = final_model.predict_proba(X_all_selected)[:, 1]\n",
    "y_pred_all = (y_proba_all >= best_threshold).astype(int)\n",
    "\n",
    "final_accuracy = accuracy_score(y_all, y_pred_all)\n",
    "final_f1 = f1_score(y_all, y_pred_all)\n",
    "final_precision = precision_score(y_all, y_pred_all, zero_division=0)\n",
    "final_recall = recall_score(y_all, y_pred_all, zero_division=0)\n",
    "\n",
    "print(f\"Métricas en datos completos:\")\n",
    "print(f\"  Accuracy:  {final_accuracy:.4f}\")\n",
    "print(f\"  F1-Score:  {final_f1:.4f}\")\n",
    "print(f\"  Precision: {final_precision:.4f}\")\n",
    "print(f\"  Recall:    {final_recall:.4f}\")\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "model_info_final = {\n",
    "    'model': final_model,\n",
    "    'threshold': best_threshold,\n",
    "    'preprocessor': preprocessor,\n",
    "    'selected_indices': final_selected_indices,\n",
    "    'feature_names': final_selected_vars,\n",
    "    'best_params': best_params,\n",
    "    'model_name': best_model_name,\n",
    "    'training_metrics_full_data': {\n",
    "        'accuracy': final_accuracy,\n",
    "        'f1_score': final_f1,\n",
    "        'precision': final_precision,\n",
    "        'recall': final_recall\n",
    "    },\n",
    "    'test_metrics_original': final_results[best_model_name],\n",
    "    'feature_selection_strategy': 'Backward',\n",
    "    'resampling_strategy': 'Original',\n",
    "    'total_training_samples': X_all_selected.shape[0],\n",
    "    'all_strategies_tested': {\n",
    "        'feature_selection': list(feature_selection_strategies.keys()),\n",
    "        'resampling': list(resampling_strategies.keys())\n",
    "    },\n",
    "    'dummy_baseline': dummy_results,\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "joblib.dump(model_info_final, '../models/clasificacion_rc_best_model.pkl')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESUMEN FINAL - CLASIFICACION RESPONSABILIDAD CIVIL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset original: {df.shape[0]:,} registros\")\n",
    "print(f\"Distribución: {sum(df['target'])}/{len(df['target'])} positivos ({100*sum(df['target'])/len(df['target']):.1f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"BASELINE DUMMYCLASSIFIER:\")\n",
    "for dummy_name, dummy_metrics in dummy_results.items():\n",
    "    print(f\"  {dummy_name:15s}: F1={dummy_metrics['f1_score']:.4f}, Acc={dummy_metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nMODELO FINAL SELECCIONADO:\")\n",
    "print(f\"  Algoritmo: {best_model_name}\")\n",
    "print(f\"  Entrenado con: {X_all_selected.shape[0]:,} registros completos\")\n",
    "print(f\"  Variables: {len(final_selected_vars)} seleccionadas\")\n",
    "print(f\"  Threshold: {best_threshold:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"HIPERPARÁMETROS OPTIMIZADOS:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print()\n",
    "\n",
    "print(\"RENDIMIENTO EN TEST SET ORIGINAL:\")\n",
    "test_metrics = final_results[best_model_name]\n",
    "print(f\"  F1-Score:         {test_metrics['f1_score']:.4f}\")\n",
    "print(f\"  Precision:        {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:           {test_metrics['recall']:.4f}\")\n",
    "print(f\"  GINI:             {test_metrics['gini']:.4f}\")\n",
    "print(f\"  PR-AUC:           {test_metrics['pr_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nVARIABLES SELECCIONADAS ({len(final_selected_vars)}):\")\n",
    "for i, var in enumerate(final_selected_vars, 1):\n",
    "    print(f\"  {i:2d}. {var}\")\n",
    "\n",
    "print(f\"\\nCOMPARACION TODOS LOS MODELOS (por F1-Score):\")\n",
    "print(\"-\" * 50)\n",
    "sorted_models = sorted(final_results.items(), key=lambda x: x[1]['f1_score'], reverse=True)\n",
    "for i, (model_name, metrics) in enumerate(sorted_models, 1):\n",
    "    superiority = \"✓\" if metrics['f1_score'] > best_dummy_f1 else \"✗\"\n",
    "    print(f\"{i:2d}. {superiority} {model_name:25s}: F1={metrics['f1_score']:.4f}, PR-AUC={metrics['pr_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nMODELO GUARDADO EN: ../models/clasificacion_rc_best_model.pkl\")\n",
    "\n",
    "improvement = test_metrics['f1_score'] / best_dummy_f1 if best_dummy_f1 > 0 else float('inf')\n",
    "print(f\"\\nMEJORA SOBRE BASELINE: {improvement:.1f}x mejor F1-score que DummyClassifier\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
