{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score, average_precision_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Modelos específicos para multinomial\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegressionMultinomial\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.ensemble import BalancedRandomForestClassifier, BalancedBaggingClassifier, EasyEnsembleClassifier\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LGBM_AVAILABLE = True\n",
    "except:\n",
    "    LGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "import joblib\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (183, 15)\n",
      "\n",
      "Distribución de Gastos_Medicos_RC_siniestros_num:\n",
      "Gastos_Medicos_RC_siniestros_num\n",
      "1.00    179\n",
      "2.00      4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Porcentajes:\n",
      "  1.0: 179 registros (97.8%)\n",
      "  2.0: 4 registros (2.2%)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/medicos_siniestrados.csv')\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nDistribución de Gastos_Medicos_RC_siniestros_num:\")\n",
    "freq_counts = df['Gastos_Medicos_RC_siniestros_num'].value_counts().sort_index()\n",
    "print(freq_counts)\n",
    "print(f\"\\nPorcentajes:\")\n",
    "for val, count in freq_counts.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  {val}: {count} registros ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución target:\n",
      "target\n",
      "1    179\n",
      "2      4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tipo de problema: Clasificación multinomial (frecuencia 1 vs 2 siniestros)\n",
      "Clases: [np.int64(1), np.int64(2)]\n"
     ]
    }
   ],
   "source": [
    "# La variable objetivo es la frecuencia (sin transformar)\n",
    "# Valores: 1 = 1 siniestro, 2 = 2 siniestros\n",
    "df['target'] = df['Gastos_Medicos_RC_siniestros_num'].astype(int)\n",
    "print(f\"Distribución target:\")\n",
    "print(df['target'].value_counts().sort_index())\n",
    "print(f\"\\nTipo de problema: Clasificación multinomial (frecuencia 1 vs 2 siniestros)\")\n",
    "print(f\"Clases: {sorted(df['target'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles en el dataset:\n",
      "  año_cursado\n",
      "  estudios_area\n",
      "  calif_promedio\n",
      "  2_o_mas_inquilinos\n",
      "  distancia_al_campus\n",
      "  genero\n",
      "  extintor_incendios\n",
      "  Gastos_Adicionales_siniestros_num\n",
      "  Gastos_Adicionales_siniestros_monto\n",
      "  Gastos_Medicos_RC_siniestros_num\n",
      "  Gastos_Medicos_RC_siniestros_monto\n",
      "  Resp_Civil_siniestros_num\n",
      "  Resp_Civil_siniestros_monto\n",
      "  Contenidos_siniestros_num\n",
      "  Contenidos_siniestros_monto\n",
      "  target\n",
      "\n",
      "Variable 'monto_promedio' creada (monto/frecuencia)\n",
      "Correlación monto_promedio vs target: 0.0264\n",
      "\n",
      "Variables finales seleccionadas (8):\n",
      "   1. año_cursado\n",
      "   2. estudios_area\n",
      "   3. calif_promedio\n",
      "   4. 2_o_mas_inquilinos\n",
      "   5. distancia_al_campus\n",
      "   6. genero\n",
      "   7. extintor_incendios\n",
      "   8. monto_promedio\n",
      "\n",
      "Categóricas: ['año_cursado', 'estudios_area', 'genero', '2_o_mas_inquilinos', 'extintor_incendios']\n",
      "Numéricas: ['calif_promedio', 'distancia_al_campus', 'monto_promedio']\n"
     ]
    }
   ],
   "source": [
    "feature_vars = ['año_cursado', 'estudios_area', 'calif_promedio', '2_o_mas_inquilinos', \n",
    "                'distancia_al_campus', 'genero', 'extintor_incendios']\n",
    "\n",
    "print(\"Columnas disponibles en el dataset:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "# Agregar monto como variable explicativa\n",
    "if 'Gastos_Medicos_RC_siniestros_monto' in df.columns:\n",
    "    df['monto_promedio'] = df['Gastos_Medicos_RC_siniestros_monto'] / df['Gastos_Medicos_RC_siniestros_num']\n",
    "    feature_vars.append('monto_promedio')\n",
    "    print(f\"\\nVariable 'monto_promedio' creada (monto/frecuencia)\")\n",
    "    print(f\"Correlación monto_promedio vs target: {df['monto_promedio'].corr(df['target']):.4f}\")\n",
    "\n",
    "X = df[feature_vars].copy()\n",
    "y = df['target'].copy()\n",
    "\n",
    "categorical_features = ['año_cursado', 'estudios_area', 'genero', '2_o_mas_inquilinos', 'extintor_incendios']\n",
    "numerical_features = ['calif_promedio', 'distancia_al_campus']\n",
    "\n",
    "if 'monto_promedio' in feature_vars:\n",
    "    numerical_features.append('monto_promedio')\n",
    "\n",
    "print(f\"\\nVariables finales seleccionadas ({len(feature_vars)}):\")\n",
    "for i, var in enumerate(feature_vars, 1):\n",
    "    print(f\"  {i:2d}. {var}\")\n",
    "    \n",
    "print(f\"\\nCategóricas: {categorical_features}\")\n",
    "print(f\"Numéricas: {numerical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed shape: (183, 15)\n",
      "Feature names: 15 variables\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "feature_names = (numerical_features + \n",
    "                 list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "\n",
    "print(f\"Transformed shape: {X_transformed.shape}\")\n",
    "print(f\"Feature names: {len(feature_names)} variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (146, 15), Test: (37, 15)\n",
      "Train target dist: Counter({1: 143, 2: 3})\n",
      "Test target dist: Counter({1: 36, 2: 1})\n",
      "\n",
      "Porcentajes en train:\n",
      "  Clase 1: 143 (97.9%)\n",
      "  Clase 2: 3 (2.1%)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_transformed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Train target dist: {Counter(y_train)}\")\n",
    "print(f\"Test target dist: {Counter(y_test)}\")\n",
    "print(f\"\\nPorcentajes en train:\")\n",
    "for cls, count in Counter(y_train).items():\n",
    "    pct = (count / len(y_train)) * 100\n",
    "    print(f\"  Clase {cls}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selection_multinomial(X, y, feature_names, significance_level=0.05):\n",
    "    \"\"\"Selección backward para regresión logística multinomial\"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.feature_selection import f_classif\n",
    "    from scipy.stats import chi2\n",
    "    \n",
    "    current_features = feature_names.copy()\n",
    "    removed_features = []\n",
    "    \n",
    "    while True:\n",
    "        # Ajustar modelo con características actuales\n",
    "        feature_indices = [feature_names.index(f) for f in current_features]\n",
    "        X_current = X[:, feature_indices]\n",
    "        \n",
    "        # Usar F-test para variables multinomiales\n",
    "        f_scores, p_values = f_classif(X_current, y)\n",
    "        \n",
    "        max_p_value = p_values.max()\n",
    "        max_p_idx = p_values.argmax()\n",
    "        \n",
    "        if max_p_value > significance_level:\n",
    "            feature_to_remove = current_features[max_p_idx]\n",
    "            current_features.remove(feature_to_remove)\n",
    "            removed_features.append(feature_to_remove)\n",
    "            print(f\"Removed {feature_to_remove}, p-value: {max_p_value:.4f}\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    selected_indices = [feature_names.index(f) for f in current_features]\n",
    "    \n",
    "    return current_features, selected_indices, removed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUACION BASELINE - DUMMYCLASSIFIER\n",
      "==================================================\n",
      "Most_Frequent: Acc=0.9730, F1_macro=0.4932, F1_weighted=0.9596, MAE=0.0270\n",
      "Stratified: Acc=0.9730, F1_macro=0.4932, F1_weighted=0.9596, MAE=0.0270\n",
      "Uniform: Acc=0.4865, F1_macro=0.3684, F1_weighted=0.6267, MAE=0.5135\n",
      "\n",
      "SELECCION BACKWARD DE VARIABLES (MULTINOMIAL)\n",
      "==================================================\n",
      "Removed 2_o_mas_inquilinos_Si, p-value: 0.9248\n",
      "Removed distancia_al_campus, p-value: 0.9225\n",
      "Removed genero_No respuesta, p-value: 0.7194\n",
      "Removed monto_promedio, p-value: 0.7053\n",
      "Removed año_cursado_posgrado, p-value: 0.5718\n",
      "Removed año_cursado_4to año, p-value: 0.5069\n",
      "Removed estudios_area_Ciencias, p-value: 0.3477\n",
      "Removed año_cursado_3er año, p-value: 0.3112\n",
      "Removed estudios_area_Otro, p-value: 0.3025\n",
      "Removed extintor_incendios_Si, p-value: 0.2093\n",
      "Removed genero_Masculino, p-value: 0.1697\n",
      "Removed estudios_area_Humanidades, p-value: 0.0977\n",
      "\n",
      "Variables seleccionadas: 3\n",
      "Variables: ['calif_promedio', 'año_cursado_2do año', 'genero_Otro']\n",
      "\n",
      "Variables removidas: ['2_o_mas_inquilinos_Si', 'distancia_al_campus', 'genero_No respuesta', 'monto_promedio', 'año_cursado_posgrado', 'año_cursado_4to año', 'estudios_area_Ciencias', 'año_cursado_3er año', 'estudios_area_Otro', 'extintor_incendios_Si', 'genero_Masculino', 'estudios_area_Humanidades']\n"
     ]
    }
   ],
   "source": [
    "print(\"EVALUACION BASELINE - DUMMYCLASSIFIER\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "dummy_models = {\n",
    "    'Most_Frequent': DummyClassifier(strategy=\"most_frequent\", random_state=42),\n",
    "    'Stratified': DummyClassifier(strategy=\"stratified\", random_state=42),\n",
    "    'Uniform': DummyClassifier(strategy=\"uniform\", random_state=42)\n",
    "}\n",
    "\n",
    "dummy_results = {}\n",
    "for name, dummy in dummy_models.items():\n",
    "    dummy.fit(X_train, y_train)\n",
    "    y_pred_dummy = dummy.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_dummy)\n",
    "    f1_macro = f1_score(y_test, y_pred_dummy, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(y_test, y_pred_dummy, average='weighted', zero_division=0)\n",
    "    mae = mean_absolute_error(y_test, y_pred_dummy)\n",
    "    \n",
    "    dummy_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'mae': mae\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}: Acc={accuracy:.4f}, F1_macro={f1_macro:.4f}, F1_weighted={f1_weighted:.4f}, MAE={mae:.4f}\")\n",
    "\n",
    "print(\"\\nSELECCION BACKWARD DE VARIABLES (MULTINOMIAL)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "selected_vars, selected_indices, removed_vars = backward_selection_multinomial(X_train, y_train, feature_names)\n",
    "print(f\"\\nVariables seleccionadas: {len(selected_vars)}\")\n",
    "print(f\"Variables: {selected_vars}\")\n",
    "print(f\"\\nVariables removidas: {removed_vars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTRATEGIAS DE SELECCION DE VARIABLES\n",
      "==================================================\n",
      "Estrategias de seleccion creadas:\n",
      "  Backward: 8 variables\n",
      "  All_Features: 15 variables\n",
      "  KBest_f_classif_5: 5 variables\n",
      "  KBest_f_classif_8: 8 variables\n",
      "  KBest_f_classif_10: 10 variables\n",
      "  KBest_mutual_info_5: 5 variables\n",
      "  KBest_mutual_info_8: 8 variables\n",
      "  KBest_mutual_info_10: 10 variables\n",
      "\n",
      "Usando estrategia 'All_Features' para LazyPredict inicial...\n"
     ]
    }
   ],
   "source": [
    "print(\"ESTRATEGIAS DE SELECCION DE VARIABLES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "feature_selection_strategies = {}\n",
    "\n",
    "# Estrategia 1: Variables seleccionadas por backward\n",
    "categorical_base_names = ['año_cursado', 'estudios_area', 'genero', '2_o_mas_inquilinos', 'extintor_incendios']\n",
    "backward_selected_indices = []\n",
    "backward_selected_vars = []\n",
    "\n",
    "for var in selected_vars:\n",
    "    if var in numerical_features:\n",
    "        backward_selected_indices.append(feature_names.index(var))\n",
    "        backward_selected_vars.append(var)\n",
    "\n",
    "for base_name in categorical_base_names:\n",
    "    categories_in_selected = [var for var in selected_vars if var.startswith(base_name)]\n",
    "    if categories_in_selected:\n",
    "        all_categories = [var for var in feature_names if var.startswith(base_name)]\n",
    "        for cat_var in all_categories:\n",
    "            if cat_var not in backward_selected_vars:\n",
    "                backward_selected_indices.append(feature_names.index(cat_var))\n",
    "                backward_selected_vars.append(cat_var)\n",
    "\n",
    "feature_selection_strategies['Backward'] = {\n",
    "    'indices': backward_selected_indices,\n",
    "    'names': backward_selected_vars\n",
    "}\n",
    "\n",
    "# Estrategia 2: Todas las variables\n",
    "feature_selection_strategies['All_Features'] = {\n",
    "    'indices': list(range(len(feature_names))),\n",
    "    'names': feature_names\n",
    "}\n",
    "\n",
    "# Estrategia 3: SelectKBest con f_classif\n",
    "for k in [5, 8, 10]:\n",
    "    if k <= len(feature_names):\n",
    "        selector = SelectKBest(score_func=f_classif, k=k)\n",
    "        X_selected = selector.fit_transform(X_train, y_train)\n",
    "        selected_features_mask = selector.get_support()\n",
    "        kbest_indices = [i for i, selected in enumerate(selected_features_mask) if selected]\n",
    "        kbest_names = [feature_names[i] for i in kbest_indices]\n",
    "        \n",
    "        feature_selection_strategies[f'KBest_f_classif_{k}'] = {\n",
    "            'indices': kbest_indices,\n",
    "            'names': kbest_names\n",
    "        }\n",
    "\n",
    "# Estrategia 4: SelectKBest con mutual_info\n",
    "for k in [5, 8, 10]:\n",
    "    if k <= len(feature_names):\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "        X_selected = selector.fit_transform(X_train, y_train)\n",
    "        selected_features_mask = selector.get_support()\n",
    "        mi_indices = [i for i, selected in enumerate(selected_features_mask) if selected]\n",
    "        mi_names = [feature_names[i] for i in mi_indices]\n",
    "        \n",
    "        feature_selection_strategies[f'KBest_mutual_info_{k}'] = {\n",
    "            'indices': mi_indices,\n",
    "            'names': mi_names\n",
    "        }\n",
    "\n",
    "print(\"Estrategias de seleccion creadas:\")\n",
    "for strategy_name, strategy_info in feature_selection_strategies.items():\n",
    "    print(f\"  {strategy_name}: {len(strategy_info['indices'])} variables\")\n",
    "\n",
    "print(f\"\\nUsando estrategia 'All_Features' para LazyPredict inicial...\")\n",
    "final_selected_indices = feature_selection_strategies['All_Features']['indices']\n",
    "final_selected_vars = feature_selection_strategies['All_Features']['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECNICAS DE RESAMPLING PARA MULTINOMIAL\n",
      "==================================================\n",
      "SMOTE: Counter({1: 143, 2: 143})\n",
      "UnderSample: Counter({1: 3, 2: 3})\n",
      "\n",
      "Distribucion original: Counter({1: 143, 2: 3})\n",
      "Estrategias de resampling disponibles: ['Original', 'SMOTE', 'UnderSample']\n"
     ]
    }
   ],
   "source": [
    "print(\"TECNICAS DE RESAMPLING PARA MULTINOMIAL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_train_selected = X_train[:, final_selected_indices]\n",
    "X_test_selected = X_test[:, final_selected_indices]\n",
    "\n",
    "resampling_strategies = {}\n",
    "\n",
    "# Datos originales\n",
    "resampling_strategies['Original'] = {\n",
    "    'X_train': X_train_selected,\n",
    "    'y_train': y_train\n",
    "}\n",
    "\n",
    "if IMBLEARN_AVAILABLE:\n",
    "    try:\n",
    "        # SMOTE para multinomial\n",
    "        smote = SMOTE(random_state=42, k_neighbors=1)  # k_neighbors=1 para dataset pequeño\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train_selected, y_train)\n",
    "        resampling_strategies['SMOTE'] = {\n",
    "            'X_train': X_train_smote,\n",
    "            'y_train': y_train_smote\n",
    "        }\n",
    "        print(f\"SMOTE: {Counter(y_train_smote)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"SMOTE error: {e}\")\n",
    "\n",
    "    try:\n",
    "        # RandomUnderSampler\n",
    "        undersampler = RandomUnderSampler(random_state=42, sampling_strategy='auto')\n",
    "        X_train_under, y_train_under = undersampler.fit_resample(X_train_selected, y_train)\n",
    "        resampling_strategies['UnderSample'] = {\n",
    "            'X_train': X_train_under,\n",
    "            'y_train': y_train_under\n",
    "        }\n",
    "        print(f\"UnderSample: {Counter(y_train_under)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"UnderSample error: {e}\")\n",
    "else:\n",
    "    print(\"imblearn no disponible - usando solo datos originales\")\n",
    "\n",
    "print(f\"\\nDistribucion original: {Counter(y_train)}\")\n",
    "print(f\"Estrategias de resampling disponibles: {list(resampling_strategies.keys())}\")\n",
    "\n",
    "# Usar datos originales por defecto\n",
    "current_X_train = X_train_selected\n",
    "current_y_train = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAZYPREDICT - EXPLORACION DE MODELOS MULTINOMIALES\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7d6805f4e040a8b80e6b780b1eed50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3, number of negative: 143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 146, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020548 -> initscore=-3.864232\n",
      "[LightGBM] [Info] Start training from score -3.864232\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Resultados LazyPredict:\n",
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "BernoulliNB                        0.97               0.50     0.50      0.96   \n",
      "ExtraTreesClassifier               0.97               0.50     0.50      0.96   \n",
      "CalibratedClassifierCV             0.97               0.50     0.50      0.96   \n",
      "ExtraTreeClassifier                0.97               0.50     0.50      0.96   \n",
      "DummyClassifier                    0.97               0.50     0.50      0.96   \n",
      "SVC                                0.97               0.50     0.50      0.96   \n",
      "RandomForestClassifier             0.97               0.50     0.50      0.96   \n",
      "LogisticRegression                 0.97               0.50     0.50      0.96   \n",
      "KNeighborsClassifier               0.97               0.50     0.50      0.96   \n",
      "RidgeClassifierCV                  0.97               0.50     0.50      0.96   \n",
      "RidgeClassifier                    0.97               0.50     0.50      0.96   \n",
      "QuadraticDiscriminantAnalysis      0.97               0.50     0.50      0.96   \n",
      "AdaBoostClassifier                 0.95               0.49     0.49      0.95   \n",
      "LabelPropagation                   0.95               0.49     0.49      0.95   \n",
      "DecisionTreeClassifier             0.95               0.49     0.49      0.95   \n",
      "BaggingClassifier                  0.95               0.49     0.49      0.95   \n",
      "LabelSpreading                     0.95               0.49     0.49      0.95   \n",
      "LGBMClassifier                     0.95               0.49     0.49      0.95   \n",
      "PassiveAggressiveClassifier        0.95               0.49     0.49      0.95   \n",
      "LinearSVC                          0.95               0.49     0.49      0.95   \n",
      "LinearDiscriminantAnalysis         0.95               0.49     0.49      0.95   \n",
      "SGDClassifier                      0.95               0.49     0.49      0.95   \n",
      "NearestCentroid                    0.92               0.47     0.47      0.93   \n",
      "GaussianNB                         0.89               0.46     0.46      0.92   \n",
      "Perceptron                         0.89               0.46     0.46      0.92   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "BernoulliNB                          0.02  \n",
      "ExtraTreesClassifier                 0.12  \n",
      "CalibratedClassifierCV               0.04  \n",
      "ExtraTreeClassifier                  0.01  \n",
      "DummyClassifier                      0.01  \n",
      "SVC                                  0.03  \n",
      "RandomForestClassifier               0.21  \n",
      "LogisticRegression                   0.03  \n",
      "KNeighborsClassifier                 0.02  \n",
      "RidgeClassifierCV                    0.02  \n",
      "RidgeClassifier                      0.02  \n",
      "QuadraticDiscriminantAnalysis        0.02  \n",
      "AdaBoostClassifier                   0.27  \n",
      "LabelPropagation                     0.02  \n",
      "DecisionTreeClassifier               0.02  \n",
      "BaggingClassifier                    0.05  \n",
      "LabelSpreading                       0.02  \n",
      "LGBMClassifier                       0.05  \n",
      "PassiveAggressiveClassifier          0.02  \n",
      "LinearSVC                            0.03  \n",
      "LinearDiscriminantAnalysis           0.02  \n",
      "SGDClassifier                        0.02  \n",
      "NearestCentroid                      0.02  \n",
      "GaussianNB                           0.02  \n",
      "Perceptron                           0.03  \n",
      "\n",
      "TOP 10 MODELOS POR F1-SCORE:\n",
      " 1. BernoulliNB              : F1=0.9596, Acc=0.9730\n",
      " 2. ExtraTreesClassifier     : F1=0.9596, Acc=0.9730\n",
      " 3. CalibratedClassifierCV   : F1=0.9596, Acc=0.9730\n",
      " 4. ExtraTreeClassifier      : F1=0.9596, Acc=0.9730\n",
      " 5. DummyClassifier          : F1=0.9596, Acc=0.9730\n",
      " 6. SVC                      : F1=0.9596, Acc=0.9730\n",
      " 7. RandomForestClassifier   : F1=0.9596, Acc=0.9730\n",
      " 8. LogisticRegression       : F1=0.9596, Acc=0.9730\n",
      " 9. KNeighborsClassifier     : F1=0.9596, Acc=0.9730\n",
      "10. RidgeClassifierCV        : F1=0.9596, Acc=0.9730\n",
      "\n",
      "Comparacion con baseline:\n",
      "Mejor DummyClassifier F1: 0.9596\n",
      "Mejor LazyPredict F1:    0.9596\n",
      "✗ LazyPredict NO supera baseline DummyClassifier\n",
      "\n",
      "Seleccionando top 5 para optimizacion: ['BernoulliNB', 'ExtraTreesClassifier', 'CalibratedClassifierCV', 'ExtraTreeClassifier', 'DummyClassifier']\n"
     ]
    }
   ],
   "source": [
    "print(\"LAZYPREDICT - EXPLORACION DE MODELOS MULTINOMIALES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "    models, predictions = clf.fit(current_X_train, X_test_selected, current_y_train, y_test)\n",
    "\n",
    "    print(\"Resultados LazyPredict:\")\n",
    "    print(models.round(4))\n",
    "\n",
    "    # Ordenar por F1 Score\n",
    "    top_10_models = models.nlargest(10, 'F1 Score')\n",
    "    print(f\"\\nTOP 10 MODELOS POR F1-SCORE:\")\n",
    "    for i, (model_name, row) in enumerate(top_10_models.iterrows(), 1):\n",
    "        print(f\"{i:2d}. {model_name:25s}: F1={row['F1 Score']:.4f}, Acc={row['Accuracy']:.4f}\")\n",
    "\n",
    "    best_f1_lazy = top_10_models.iloc[0]['F1 Score']\n",
    "    best_dummy_f1 = max([res['f1_weighted'] for res in dummy_results.values()])\n",
    "\n",
    "    print(f\"\\nComparacion con baseline:\")\n",
    "    print(f\"Mejor DummyClassifier F1: {best_dummy_f1:.4f}\")\n",
    "    print(f\"Mejor LazyPredict F1:    {best_f1_lazy:.4f}\")\n",
    "    if best_f1_lazy > best_dummy_f1:\n",
    "        print(\"✓ LazyPredict supera baseline DummyClassifier\")\n",
    "    else:\n",
    "        print(\"✗ LazyPredict NO supera baseline DummyClassifier\")\n",
    "\n",
    "    top_5_names = top_10_models.head(5).index.tolist()\n",
    "    print(f\"\\nSeleccionando top 5 para optimizacion: {top_5_names}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error en LazyPredict: {e}\")\n",
    "    print(\"Continuando con modelos predefinidos...\")\n",
    "    top_5_names = ['LogisticRegression', 'RandomForestClassifier', 'XGBClassifier', 'GradientBoostingClassifier', 'SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPEO Y CONFIGURACION DE MODELOS MULTINOMIALES\n",
      "==================================================\n",
      "Agregando modelos multinomiales específicos:\n",
      "✓ Modelos logísticos multinomial y OvR agregados\n",
      "✓ imblearn modelos balanceados disponibles\n",
      "✓ LightGBM disponible\n",
      "✗ CatBoost no disponible\n",
      "○ BernoulliNB usando LogisticRegression multinomial como backup\n",
      "✓ ExtraTreesClassifier mapeado correctamente\n",
      "○ CalibratedClassifierCV usando LogisticRegression multinomial como backup\n",
      "\n",
      "Modelos a optimizar: 5\n",
      "  - LogisticRegression_Multinomial\n",
      "  - LogisticRegression_OvR\n",
      "  - BernoulliNB\n",
      "  - ExtraTreesClassifier\n",
      "  - CalibratedClassifierCV\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPEO Y CONFIGURACION DE MODELOS MULTINOMIALES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Modelos base con soporte multinomial\n",
    "model_mapping = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial', solver='lbfgs'),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBClassifier': XGBClassifier(random_state=42, eval_metric='mlogloss', objective='multi:softprob'),\n",
    "    'SVC': SVC(random_state=42, probability=True),\n",
    "    'MLPClassifier': MLPClassifier(random_state=42, max_iter=500),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(random_state=42, algorithm='SAMME'),\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "}\n",
    "\n",
    "# Modelos específicos multinomiales\n",
    "print(\"Agregando modelos multinomiales específicos:\")\n",
    "model_mapping['LogisticRegression_Multinomial'] = LogisticRegression(\n",
    "    random_state=42, max_iter=1000, multi_class='multinomial', solver='newton-cg'\n",
    ")\n",
    "model_mapping['LogisticRegression_OvR'] = LogisticRegression(\n",
    "    random_state=42, max_iter=1000, multi_class='ovr', solver='liblinear'\n",
    ")\n",
    "print(\"✓ Modelos logísticos multinomial y OvR agregados\")\n",
    "\n",
    "if IMBLEARN_AVAILABLE:\n",
    "    model_mapping['BalancedRandomForestClassifier'] = BalancedRandomForestClassifier(random_state=42)\n",
    "    model_mapping['BalancedBaggingClassifier'] = BalancedBaggingClassifier(random_state=42)\n",
    "    print(\"✓ imblearn modelos balanceados disponibles\")\n",
    "else:\n",
    "    print(\"✗ imblearn no disponible - sin modelos balanceados\")\n",
    "\n",
    "if LGBM_AVAILABLE:\n",
    "    model_mapping['LGBMClassifier'] = LGBMClassifier(random_state=42, verbose=-1, objective='multiclass')\n",
    "    print(\"✓ LightGBM disponible\")\n",
    "else:\n",
    "    print(\"✗ LightGBM no disponible\")\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    model_mapping['CatBoostClassifier'] = CatBoostClassifier(random_state=42, verbose=False, loss_function='MultiClass')\n",
    "    print(\"✓ CatBoost disponible\")\n",
    "else:\n",
    "    print(\"✗ CatBoost no disponible\")\n",
    "\n",
    "# Preparar modelos para optimización\n",
    "models_to_optimize = []\n",
    "\n",
    "# Agregar modelos multinomiales específicos\n",
    "models_to_optimize.append(('LogisticRegression_Multinomial', model_mapping['LogisticRegression_Multinomial']))\n",
    "models_to_optimize.append(('LogisticRegression_OvR', model_mapping['LogisticRegression_OvR']))\n",
    "\n",
    "# Agregar top 5 de LazyPredict\n",
    "for name in top_5_names[:3]:  # Solo top 3 para acelerar\n",
    "    if name in model_mapping:\n",
    "        models_to_optimize.append((name, model_mapping[name]))\n",
    "        print(f\"✓ {name} mapeado correctamente\")\n",
    "    else:\n",
    "        models_to_optimize.append((name, LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial')))\n",
    "        print(f\"○ {name} usando LogisticRegression multinomial como backup\")\n",
    "\n",
    "print(f\"\\nModelos a optimizar: {len(models_to_optimize)}\")\n",
    "for name, _ in models_to_optimize:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZACION DE HIPERPARAMETROS MULTINOMIALES\n",
      "==================================================\n",
      "\n",
      "Optimizing LogisticRegression_Multinomial...\n",
      "  Grid específico: 2 parámetros\n",
      "  Iniciando búsqueda...\n",
      "  ✓ Best F1-macro CV: 0.4948\n",
      "  ✓ Best params: {'C': 0.1, 'class_weight': None}\n",
      "\n",
      "Optimizing LogisticRegression_OvR...\n",
      "  Grid específico: 2 parámetros\n",
      "  Iniciando búsqueda...\n",
      "  ✓ Best F1-macro CV: 0.5462\n",
      "  ✓ Best params: {'C': 0.1, 'class_weight': 'balanced'}\n",
      "\n",
      "Optimizing BernoulliNB...\n",
      "  Sin grid específico\n",
      "  Iniciando búsqueda...\n",
      "  ✓ Best F1-macro CV: 0.4948\n",
      "  ✓ Best params: {}\n",
      "\n",
      "Optimizing ExtraTreesClassifier...\n",
      "  Sin grid específico\n",
      "  Iniciando búsqueda...\n",
      "  ✓ Best F1-macro CV: 0.4948\n",
      "  ✓ Best params: {}\n",
      "\n",
      "Optimizing CalibratedClassifierCV...\n",
      "  Sin grid específico\n",
      "  Iniciando búsqueda...\n",
      "  ✓ Best F1-macro CV: 0.4948\n",
      "  ✓ Best params: {}\n",
      "\n",
      "==================================================\n",
      "OPTIMIZATION COMPLETED\n",
      "==================================================\n",
      "Resultados de optimización:\n",
      "LogisticRegression_Multinomial: F1-macro CV = 0.4948\n",
      "LogisticRegression_OvR: F1-macro CV = 0.5462\n",
      "BernoulliNB: F1-macro CV = 0.4948\n",
      "ExtraTreesClassifier: F1-macro CV = 0.4948\n",
      "CalibratedClassifierCV: F1-macro CV = 0.4948\n"
     ]
    }
   ],
   "source": [
    "print(\"OPTIMIZACION DE HIPERPARAMETROS MULTINOMIALES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Grillas específicas para modelos multinomiales\n",
    "param_grids_multinomial = {\n",
    "    'LogisticRegression_Multinomial': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    },\n",
    "    'LogisticRegression_OvR': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [5, 10, None],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.1, 0.2]\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.1, 0.2],\n",
    "        'max_depth': [3, 5]\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    }\n",
    "}\n",
    "\n",
    "optimization_results = {}\n",
    "\n",
    "for model_name, model in models_to_optimize:\n",
    "    print(f\"\\nOptimizing {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        if model_name in param_grids_multinomial:\n",
    "            param_grid = param_grids_multinomial[model_name]\n",
    "            print(f\"  Grid específico: {len(param_grid)} parámetros\")\n",
    "        else:\n",
    "            param_grid = {}\n",
    "            print(f\"  Sin grid específico\")\n",
    "        \n",
    "        # Usar StratifiedKFold para multinomial\n",
    "        skf_multi = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            scoring='f1_macro',  # F1 macro para multinomial\n",
    "            cv=skf_multi,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        print(f\"  Iniciando búsqueda...\")\n",
    "        grid_search.fit(current_X_train, current_y_train)\n",
    "        \n",
    "        optimization_results[model_name] = {\n",
    "            'best_estimator': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_cv_score': grid_search.best_score_\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ Best F1-macro CV: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"  ✓ Best params: {grid_search.best_params_}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {str(e)}\")\n",
    "        optimization_results[model_name] = {\n",
    "            'best_estimator': model,\n",
    "            'best_params': {},\n",
    "            'best_cv_score': 0\n",
    "        }\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"OPTIMIZATION COMPLETED\")\n",
    "print(f\"{'='*50}\")\n",
    "print(\"Resultados de optimización:\")\n",
    "for name, results in optimization_results.items():\n",
    "    print(f\"{name}: F1-macro CV = {results['best_cv_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUACION FINAL CON METRICAS MULTINOMIALES\n",
      "============================================================\n",
      "LogisticRegression_Multinomial:\n",
      "  Accuracy:     0.9730\n",
      "  F1-macro:     0.4932\n",
      "  F1-weighted:  0.9596\n",
      "  MAE:          0.0270\n",
      "  MSE:          0.0270\n",
      "  MCC:          0.0000\n",
      "  Bal-Acc:      0.5000\n",
      "\n",
      "Reporte detallado para LogisticRegression_Multinomial:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      1.00      0.99        36\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.97        37\n",
      "   macro avg       0.49      0.50      0.49        37\n",
      "weighted avg       0.95      0.97      0.96        37\n",
      "\n",
      "Matriz de confusión:\n",
      "Error evaluando LogisticRegression_Multinomial: name 'confusion_matrix' is not defined\n",
      "LogisticRegression_OvR:\n",
      "  Accuracy:     0.7838\n",
      "  F1-macro:     0.4394\n",
      "  F1-weighted:  0.8550\n",
      "  MAE:          0.2162\n",
      "  MSE:          0.2162\n",
      "  MCC:          -0.0805\n",
      "  Bal-Acc:      0.4028\n",
      "\n",
      "BernoulliNB:\n",
      "  Accuracy:     0.9730\n",
      "  F1-macro:     0.4932\n",
      "  F1-weighted:  0.9596\n",
      "  MAE:          0.0270\n",
      "  MSE:          0.0270\n",
      "  MCC:          0.0000\n",
      "  Bal-Acc:      0.5000\n",
      "\n",
      "Reporte detallado para BernoulliNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      1.00      0.99        36\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.97        37\n",
      "   macro avg       0.49      0.50      0.49        37\n",
      "weighted avg       0.95      0.97      0.96        37\n",
      "\n",
      "Matriz de confusión:\n",
      "Error evaluando BernoulliNB: name 'confusion_matrix' is not defined\n",
      "ExtraTreesClassifier:\n",
      "  Accuracy:     0.9730\n",
      "  F1-macro:     0.4932\n",
      "  F1-weighted:  0.9596\n",
      "  MAE:          0.0270\n",
      "  MSE:          0.0270\n",
      "  MCC:          0.0000\n",
      "  Bal-Acc:      0.5000\n",
      "\n",
      "Reporte detallado para ExtraTreesClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      1.00      0.99        36\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.97        37\n",
      "   macro avg       0.49      0.50      0.49        37\n",
      "weighted avg       0.95      0.97      0.96        37\n",
      "\n",
      "Matriz de confusión:\n",
      "Error evaluando ExtraTreesClassifier: name 'confusion_matrix' is not defined\n",
      "CalibratedClassifierCV:\n",
      "  Accuracy:     0.9730\n",
      "  F1-macro:     0.4932\n",
      "  F1-weighted:  0.9596\n",
      "  MAE:          0.0270\n",
      "  MSE:          0.0270\n",
      "  MCC:          0.0000\n",
      "  Bal-Acc:      0.5000\n",
      "\n",
      "Reporte detallado para CalibratedClassifierCV:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      1.00      0.99        36\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.97        37\n",
      "   macro avg       0.49      0.50      0.49        37\n",
      "weighted avg       0.95      0.97      0.96        37\n",
      "\n",
      "Matriz de confusión:\n",
      "Error evaluando CalibratedClassifierCV: name 'confusion_matrix' is not defined\n",
      "Comparacion con DummyClassifier:\n",
      "----------------------------------------\n",
      "Most_Frequent: F1_macro=0.4932, Acc=0.9730, MAE=0.0270\n",
      "Stratified: F1_macro=0.4932, Acc=0.9730, MAE=0.0270\n",
      "Uniform: F1_macro=0.3684, Acc=0.4865, MAE=0.5135\n",
      "\n",
      "Mejor modelo real:\n",
      "LogisticRegression_Multinomial: F1_macro=0.4932\n",
      "✗ El mejor modelo NO supera el baseline DummyClassifier\n",
      "  Dataset muy pequeño, considerar más datos o regularización\n"
     ]
    }
   ],
   "source": [
    "def calculate_enhanced_metrics_multinomial(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"Calcula métricas mejoradas para clasificación multinomial\"\"\"\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    try:\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    except:\n",
    "        mcc = 0\n",
    "    \n",
    "    try:\n",
    "        balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    except:\n",
    "        balanced_acc = 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'mcc': mcc,\n",
    "        'balanced_accuracy': balanced_acc\n",
    "    }\n",
    "\n",
    "final_results = {}\n",
    "\n",
    "print(\"EVALUACION FINAL CON METRICAS MULTINOMIALES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, model_info in optimization_results.items():\n",
    "    model = model_info['best_estimator']\n",
    "    \n",
    "    try:\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        y_proba = None\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_proba = model.predict_proba(X_test_selected)\n",
    "        \n",
    "        metrics = calculate_enhanced_metrics_multinomial(y_test, y_pred, y_proba)\n",
    "        metrics['model'] = model\n",
    "        \n",
    "        final_results[model_name] = metrics\n",
    "        \n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  Accuracy:     {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  F1-macro:     {metrics['f1_macro']:.4f}\")\n",
    "        print(f\"  F1-weighted:  {metrics['f1_weighted']:.4f}\")\n",
    "        print(f\"  MAE:          {metrics['mae']:.4f}\")\n",
    "        print(f\"  MSE:          {metrics['mse']:.4f}\")\n",
    "        print(f\"  MCC:          {metrics['mcc']:.4f}\")\n",
    "        print(f\"  Bal-Acc:      {metrics['balanced_accuracy']:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        # Reporte detallado para el mejor modelo\n",
    "        if len(final_results) == 1 or metrics['f1_macro'] == max([r['f1_macro'] for r in final_results.values()]):\n",
    "            print(f\"Reporte detallado para {model_name}:\")\n",
    "            print(classification_report(y_test, y_pred, zero_division=0))\n",
    "            print(f\"Matriz de confusión:\")\n",
    "            print(confusion_matrix(y_test, y_pred))\n",
    "            print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluando {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"Comparacion con DummyClassifier:\")\n",
    "print(\"-\" * 40)\n",
    "for dummy_name, dummy_metrics in dummy_results.items():\n",
    "    print(f\"{dummy_name}: F1_macro={dummy_metrics['f1_macro']:.4f}, Acc={dummy_metrics['accuracy']:.4f}, MAE={dummy_metrics['mae']:.4f}\")\n",
    "\n",
    "if final_results:\n",
    "    best_model_name = max(final_results.keys(), key=lambda k: final_results[k]['f1_macro'])\n",
    "    best_f1 = final_results[best_model_name]['f1_macro']\n",
    "    best_dummy_f1 = max([res['f1_macro'] for res in dummy_results.values()])\n",
    "    \n",
    "    print(f\"\\nMejor modelo real:\")\n",
    "    print(f\"{best_model_name}: F1_macro={best_f1:.4f}\")\n",
    "\n",
    "    if best_f1 > best_dummy_f1:\n",
    "        print(\"✓ El mejor modelo SUPERA el baseline DummyClassifier\")\n",
    "    else:\n",
    "        print(\"✗ El mejor modelo NO supera el baseline DummyClassifier\")\n",
    "        print(\"  Dataset muy pequeño, considerar más datos o regularización\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REENTRENAMIENTO CON TODOS LOS DATOS (FRECUENCIA)\n",
      "================================================================================\n",
      "Mejor modelo seleccionado: LogisticRegression_Multinomial\n",
      "Hiperparámetros óptimos: {'C': 0.1, 'class_weight': None}\n",
      "\n",
      "Preparando todos los datos para reentrenamiento...\n",
      "Datos completos: (183, 15)\n",
      "Distribución target completa: Counter({1: 179, 2: 4})\n",
      "\n",
      "Creando modelo final con hiperparámetros optimizados...\n",
      "Modelo configurado: LogisticRegression(C=0.1, max_iter=1000, multi_class='multinomial',\n",
      "                   random_state=42)\n",
      "\n",
      "Entrenando modelo final con 183 registros...\n",
      "✓ Entrenamiento completado\n",
      "\n",
      "Validación del modelo final:\n",
      "Métricas en datos completos:\n",
      "  Accuracy:     0.9781\n",
      "  F1-macro:     0.4945\n",
      "  F1-weighted:  0.9673\n",
      "  MAE:          0.0219\n",
      "================================================================================\n",
      "RESUMEN FINAL - FRECUENCIA GASTOS MEDICOS RC\n",
      "================================================================================\n",
      "Dataset original: 183 registros (solo siniestrados)\n",
      "Distribución frecuencias: {1: 179, 2: 4}\n",
      "\n",
      "BASELINE DUMMYCLASSIFIER:\n",
      "  Most_Frequent  : F1_macro=0.4932, MAE=0.0270\n",
      "  Stratified     : F1_macro=0.4932, MAE=0.0270\n",
      "  Uniform        : F1_macro=0.3684, MAE=0.5135\n",
      "\n",
      "MODELO FINAL SELECCIONADO:\n",
      "  Algoritmo: LogisticRegression_Multinomial\n",
      "  Tipo: Clasificación Multinomial\n",
      "  Clases: [np.int64(1), np.int64(2)] (frecuencias de siniestros)\n",
      "  Entrenado con: 183 registros completos\n",
      "  Variables: 15 seleccionadas\n",
      "\n",
      "HIPERPARÁMETROS OPTIMIZADOS:\n",
      "  C: 0.1\n",
      "  class_weight: None\n",
      "\n",
      "RENDIMIENTO EN TEST SET ORIGINAL:\n",
      "  F1-macro:     0.4932\n",
      "  F1-weighted:  0.9596\n",
      "  Accuracy:     0.9730\n",
      "  MAE:          0.0270\n",
      "  MCC:          0.0000\n",
      "\n",
      "VARIABLES SELECCIONADAS (15):\n",
      "   1. calif_promedio\n",
      "   2. distancia_al_campus\n",
      "   3. monto_promedio\n",
      "   4. año_cursado_2do año\n",
      "   5. año_cursado_3er año\n",
      "   6. año_cursado_4to año\n",
      "   7. año_cursado_posgrado\n",
      "   8. estudios_area_Ciencias\n",
      "   9. estudios_area_Humanidades\n",
      "  10. estudios_area_Otro\n",
      "  11. genero_Masculino\n",
      "  12. genero_No respuesta\n",
      "  13. genero_Otro\n",
      "  14. 2_o_mas_inquilinos_Si\n",
      "  15. extintor_incendios_Si\n",
      "\n",
      "COMPARACION TODOS LOS MODELOS (por F1-macro):\n",
      "--------------------------------------------------\n",
      " 1. ✗ LogisticRegression_Multinomial: F1_macro=0.4932, MAE=0.0270\n",
      " 2. ✗ BernoulliNB              : F1_macro=0.4932, MAE=0.0270\n",
      " 3. ✗ ExtraTreesClassifier     : F1_macro=0.4932, MAE=0.0270\n",
      " 4. ✗ CalibratedClassifierCV   : F1_macro=0.4932, MAE=0.0270\n",
      " 5. ✗ LogisticRegression_OvR   : F1_macro=0.4394, MAE=0.2162\n",
      "\n",
      "MODELO GUARDADO EN: ../models/frecuencia_medicos_best_model.pkl\n",
      "\n",
      "MEJORA SOBRE BASELINE: 1.0x mejor F1-macro que DummyClassifier\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if final_results:\n",
    "    best_model_name = max(final_results.keys(), key=lambda k: final_results[k]['f1_macro'])\n",
    "    best_model = final_results[best_model_name]['model']\n",
    "    best_params = optimization_results[best_model_name]['best_params']\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"REENTRENAMIENTO CON TODOS LOS DATOS (FRECUENCIA)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Mejor modelo seleccionado: {best_model_name}\")\n",
    "    print(f\"Hiperparámetros óptimos: {best_params}\")\n",
    "\n",
    "    print(f\"\\nPreparando todos los datos para reentrenamiento...\")\n",
    "    X_all_selected = X_transformed[:, final_selected_indices]\n",
    "    y_all = y.copy()\n",
    "\n",
    "    print(f\"Datos completos: {X_all_selected.shape}\")\n",
    "    print(f\"Distribución target completa: {Counter(y_all)}\")\n",
    "\n",
    "    print(f\"\\nCreando modelo final con hiperparámetros optimizados...\")\n",
    "\n",
    "    # Mapeo para crear modelo final\n",
    "    model_mapping_final = {\n",
    "        'LogisticRegression_Multinomial': LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial', solver='lbfgs'),\n",
    "        'LogisticRegression_OvR': LogisticRegression(random_state=42, max_iter=1000, multi_class='ovr', solver='liblinear'),\n",
    "        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial'),\n",
    "        'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "        'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n",
    "        'XGBClassifier': XGBClassifier(random_state=42, eval_metric='mlogloss', objective='multi:softprob'),\n",
    "        'SVC': SVC(random_state=42, probability=True),\n",
    "    }\n",
    "\n",
    "    if best_model_name in model_mapping_final:\n",
    "        final_model = model_mapping_final[best_model_name]\n",
    "    else:\n",
    "        final_model = LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial')\n",
    "\n",
    "    final_model.set_params(**best_params)\n",
    "    print(f\"Modelo configurado: {final_model}\")\n",
    "\n",
    "    print(f\"\\nEntrenando modelo final con {X_all_selected.shape[0]} registros...\")\n",
    "    final_model.fit(X_all_selected, y_all)\n",
    "    print(\"✓ Entrenamiento completado\")\n",
    "\n",
    "    print(f\"\\nValidación del modelo final:\")\n",
    "    y_pred_all = final_model.predict(X_all_selected)\n",
    "\n",
    "    final_accuracy = accuracy_score(y_all, y_pred_all)\n",
    "    final_f1_macro = f1_score(y_all, y_pred_all, average='macro')\n",
    "    final_f1_weighted = f1_score(y_all, y_pred_all, average='weighted')\n",
    "    final_mae = mean_absolute_error(y_all, y_pred_all)\n",
    "\n",
    "    print(f\"Métricas en datos completos:\")\n",
    "    print(f\"  Accuracy:     {final_accuracy:.4f}\")\n",
    "    print(f\"  F1-macro:     {final_f1_macro:.4f}\")\n",
    "    print(f\"  F1-weighted:  {final_f1_weighted:.4f}\")\n",
    "    print(f\"  MAE:          {final_mae:.4f}\")\n",
    "\n",
    "    # Guardar modelo\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "    model_info_final = {\n",
    "        'model': final_model,\n",
    "        'preprocessor': preprocessor,\n",
    "        'selected_indices': final_selected_indices,\n",
    "        'feature_names': final_selected_vars,\n",
    "        'best_params': best_params,\n",
    "        'model_name': best_model_name,\n",
    "        'training_metrics_full_data': {\n",
    "            'accuracy': final_accuracy,\n",
    "            'f1_macro': final_f1_macro,\n",
    "            'f1_weighted': final_f1_weighted,\n",
    "            'mae': final_mae\n",
    "        },\n",
    "        'test_metrics_original': final_results[best_model_name],\n",
    "        'feature_selection_strategy': 'All_Features',\n",
    "        'resampling_strategy': 'Original',\n",
    "        'total_training_samples': X_all_selected.shape[0],\n",
    "        'all_strategies_tested': {\n",
    "            'feature_selection': list(feature_selection_strategies.keys()),\n",
    "            'resampling': list(resampling_strategies.keys())\n",
    "        },\n",
    "        'dummy_baseline': dummy_results,\n",
    "        'target_classes': sorted(y_all.unique()),\n",
    "        'class_distribution': dict(Counter(y_all)),\n",
    "        'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "    joblib.dump(model_info_final, '../models/frecuencia_medicos_best_model.pkl')\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"RESUMEN FINAL - FRECUENCIA GASTOS MEDICOS RC\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Dataset original: {df.shape[0]:,} registros (solo siniestrados)\")\n",
    "    print(f\"Distribución frecuencias: {dict(Counter(df['target']))}\")\n",
    "    print()\n",
    "\n",
    "    print(\"BASELINE DUMMYCLASSIFIER:\")\n",
    "    for dummy_name, dummy_metrics in dummy_results.items():\n",
    "        print(f\"  {dummy_name:15s}: F1_macro={dummy_metrics['f1_macro']:.4f}, MAE={dummy_metrics['mae']:.4f}\")\n",
    "\n",
    "    print(f\"\\nMODELO FINAL SELECCIONADO:\")\n",
    "    print(f\"  Algoritmo: {best_model_name}\")\n",
    "    print(f\"  Tipo: Clasificación Multinomial\")\n",
    "    print(f\"  Clases: {sorted(y_all.unique())} (frecuencias de siniestros)\")\n",
    "    print(f\"  Entrenado con: {X_all_selected.shape[0]:,} registros completos\")\n",
    "    print(f\"  Variables: {len(final_selected_vars)} seleccionadas\")\n",
    "    print()\n",
    "\n",
    "    print(\"HIPERPARÁMETROS OPTIMIZADOS:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print()\n",
    "\n",
    "    print(\"RENDIMIENTO EN TEST SET ORIGINAL:\")\n",
    "    test_metrics = final_results[best_model_name]\n",
    "    print(f\"  F1-macro:     {test_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  F1-weighted:  {test_metrics['f1_weighted']:.4f}\")\n",
    "    print(f\"  Accuracy:     {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  MAE:          {test_metrics['mae']:.4f}\")\n",
    "    print(f\"  MCC:          {test_metrics['mcc']:.4f}\")\n",
    "\n",
    "    print(f\"\\nVARIABLES SELECCIONADAS ({len(final_selected_vars)}):\")\n",
    "    for i, var in enumerate(final_selected_vars, 1):\n",
    "        print(f\"  {i:2d}. {var}\")\n",
    "\n",
    "    print(f\"\\nCOMPARACION TODOS LOS MODELOS (por F1-macro):\")\n",
    "    print(\"-\" * 50)\n",
    "    sorted_models = sorted(final_results.items(), key=lambda x: x[1]['f1_macro'], reverse=True)\n",
    "    best_dummy_f1 = max([res['f1_macro'] for res in dummy_results.values()])\n",
    "    for i, (model_name, metrics) in enumerate(sorted_models, 1):\n",
    "        superiority = \"✓\" if metrics['f1_macro'] > best_dummy_f1 else \"✗\"\n",
    "        print(f\"{i:2d}. {superiority} {model_name:25s}: F1_macro={metrics['f1_macro']:.4f}, MAE={metrics['mae']:.4f}\")\n",
    "\n",
    "    print(f\"\\nMODELO GUARDADO EN: ../models/frecuencia_medicos_best_model.pkl\")\n",
    "\n",
    "    improvement = test_metrics['f1_macro'] / best_dummy_f1 if best_dummy_f1 > 0 else float('inf')\n",
    "    print(f\"\\nMEJORA SOBRE BASELINE: {improvement:.1f}x mejor F1-macro que DummyClassifier\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No se pudieron evaluar modelos. Revisar datos o configuración.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
