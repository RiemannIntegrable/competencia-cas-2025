{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Modelo de Frecuencia - Gastos Adicionales de Vivienda\n",
    "\n",
    "## Objetivo\n",
    "Predecir la **frecuencia de siniestros** (número de siniestros) en la cobertura de Gastos Adicionales de Vivienda para asegurados que ya siniestrados (clasificación multinomial).\n",
    "\n",
    "## Pipeline\n",
    "1. **Fase 1**: Preprocesamiento + Multinomial Logit + Selección Backward + LazyPredict\n",
    "\n",
    "**Dataset**: Solo registros siniestrados de Gastos Adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Imports y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Librerías importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocesamiento\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Modelos de clasificación\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Intentar importar XGBoost y LightGBM (opcionales)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "    print(\"⚠ XGBoost no disponible\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    HAS_LGBM = True\n",
    "except ImportError:\n",
    "    HAS_LGBM = False\n",
    "    print(\"⚠ LightGBM no disponible\")\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "# LazyPredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# GLM Multinomial Logit\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Carga y Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original: (416, 15)\n",
      "\n",
      "Columnas: ['año_cursado', 'estudios_area', 'calif_promedio', '2_o_mas_inquilinos', 'distancia_al_campus', 'genero', 'extintor_incendios', 'Gastos_Adicionales_siniestros_num', 'Gastos_Adicionales_siniestros_monto', 'Gastos_Medicos_RC_siniestros_num', 'Gastos_Medicos_RC_siniestros_monto', 'Resp_Civil_siniestros_num', 'Resp_Civil_siniestros_monto', 'Contenidos_siniestros_num', 'Contenidos_siniestros_monto']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>año_cursado</th>\n",
       "      <th>estudios_area</th>\n",
       "      <th>calif_promedio</th>\n",
       "      <th>2_o_mas_inquilinos</th>\n",
       "      <th>distancia_al_campus</th>\n",
       "      <th>genero</th>\n",
       "      <th>extintor_incendios</th>\n",
       "      <th>Gastos_Adicionales_siniestros_num</th>\n",
       "      <th>Gastos_Adicionales_siniestros_monto</th>\n",
       "      <th>Gastos_Medicos_RC_siniestros_num</th>\n",
       "      <th>Gastos_Medicos_RC_siniestros_monto</th>\n",
       "      <th>Resp_Civil_siniestros_num</th>\n",
       "      <th>Resp_Civil_siniestros_monto</th>\n",
       "      <th>Contenidos_siniestros_num</th>\n",
       "      <th>Contenidos_siniestros_monto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3er año</td>\n",
       "      <td>Humanidades</td>\n",
       "      <td>4.78</td>\n",
       "      <td>Si</td>\n",
       "      <td>1.33</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Si</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8909.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3er año</td>\n",
       "      <td>Otro</td>\n",
       "      <td>7.42</td>\n",
       "      <td>Si</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>Si</td>\n",
       "      <td>1.00</td>\n",
       "      <td>366.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2do año</td>\n",
       "      <td>Humanidades</td>\n",
       "      <td>5.09</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>Si</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2936.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>posgrado</td>\n",
       "      <td>Ciencias</td>\n",
       "      <td>7.48</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>Si</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3989.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1er año</td>\n",
       "      <td>Ciencias</td>\n",
       "      <td>8.56</td>\n",
       "      <td>Si</td>\n",
       "      <td>1.45</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>No</td>\n",
       "      <td>1.00</td>\n",
       "      <td>14256.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  año_cursado estudios_area  calif_promedio 2_o_mas_inquilinos  \\\n",
       "0     3er año   Humanidades            4.78                 Si   \n",
       "1     3er año          Otro            7.42                 Si   \n",
       "2     2do año   Humanidades            5.09                 No   \n",
       "3    posgrado      Ciencias            7.48                 No   \n",
       "4     1er año      Ciencias            8.56                 Si   \n",
       "\n",
       "   distancia_al_campus     genero extintor_incendios  \\\n",
       "0                 1.33  Masculino                 Si   \n",
       "1                 0.00   Femenino                 Si   \n",
       "2                 0.00   Femenino                 Si   \n",
       "3                 0.00   Femenino                 Si   \n",
       "4                 1.45  Masculino                 No   \n",
       "\n",
       "   Gastos_Adicionales_siniestros_num  Gastos_Adicionales_siniestros_monto  \\\n",
       "0                               1.00                              8909.20   \n",
       "1                               1.00                               366.11   \n",
       "2                               1.00                              2936.17   \n",
       "3                               1.00                              3989.42   \n",
       "4                               1.00                             14256.36   \n",
       "\n",
       "   Gastos_Medicos_RC_siniestros_num  Gastos_Medicos_RC_siniestros_monto  \\\n",
       "0                              0.00                                0.00   \n",
       "1                              0.00                                0.00   \n",
       "2                              0.00                                0.00   \n",
       "3                              0.00                                0.00   \n",
       "4                              0.00                                0.00   \n",
       "\n",
       "   Resp_Civil_siniestros_num  Resp_Civil_siniestros_monto  \\\n",
       "0                       0.00                         0.00   \n",
       "1                       0.00                         0.00   \n",
       "2                       0.00                         0.00   \n",
       "3                       0.00                         0.00   \n",
       "4                       0.00                         0.00   \n",
       "\n",
       "   Contenidos_siniestros_num  Contenidos_siniestros_monto  \n",
       "0                       0.00                         0.00  \n",
       "1                       0.00                         0.00  \n",
       "2                       0.00                         0.00  \n",
       "3                       0.00                         0.00  \n",
       "4                       0.00                         0.00  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar datos de estudiantes siniestrados\n",
    "df = pd.read_csv(\"../data/processed/adicionales_siniestrados.csv\")\n",
    "\n",
    "print(f\"Dataset original: {df.shape}\")\n",
    "print(f\"\\nColumnas: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset preparado: (416, 8)\n",
      "\n",
      "Distribución variable objetivo (frecuencia):\n",
      "Gastos_Adicionales_siniestros_num\n",
      "1.00    405\n",
      "2.00     11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Estadísticas de frecuencia:\n",
      "count   416.00\n",
      "mean      1.03\n",
      "std       0.16\n",
      "min       1.00\n",
      "25%       1.00\n",
      "50%       1.00\n",
      "75%       1.00\n",
      "max       2.00\n",
      "Name: Gastos_Adicionales_siniestros_num, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Eliminar columna de monto (no se usa en modelo de frecuencia)\n",
    "df = df.drop('Gastos_Adicionales_siniestros_monto', axis=1)\n",
    "\n",
    "# Eliminar otras coberturas (no son features relevantes para este modelo)\n",
    "df = df.drop([\n",
    "    'Gastos_Medicos_RC_siniestros_num', 'Gastos_Medicos_RC_siniestros_monto',\n",
    "    'Resp_Civil_siniestros_num', 'Resp_Civil_siniestros_monto',\n",
    "    'Contenidos_siniestros_num', 'Contenidos_siniestros_monto'\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\nDataset preparado: {df.shape}\")\n",
    "print(f\"\\nDistribución variable objetivo (frecuencia):\")\n",
    "print(df['Gastos_Adicionales_siniestros_num'].value_counts().sort_index())\n",
    "print(f\"\\nEstadísticas de frecuencia:\")\n",
    "print(df['Gastos_Adicionales_siniestros_num'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Análisis Exploratorio Rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables numéricas: ['calif_promedio', 'distancia_al_campus']\n",
      "Variables categóricas: ['año_cursado', 'estudios_area', '2_o_mas_inquilinos', 'genero', 'extintor_incendios']\n",
      "\n",
      "Total features: 7\n",
      "\n",
      "Valores faltantes:\n",
      "año_cursado                          0\n",
      "estudios_area                        0\n",
      "calif_promedio                       0\n",
      "2_o_mas_inquilinos                   0\n",
      "distancia_al_campus                  0\n",
      "genero                               0\n",
      "extintor_incendios                   0\n",
      "Gastos_Adicionales_siniestros_num    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identificar tipos de variables\n",
    "numeric_features = ['calif_promedio', 'distancia_al_campus']\n",
    "categorical_features = ['año_cursado', 'estudios_area', '2_o_mas_inquilinos', 'genero', 'extintor_incendios']\n",
    "\n",
    "print(\"Variables numéricas:\", numeric_features)\n",
    "print(\"Variables categóricas:\", categorical_features)\n",
    "print(f\"\\nTotal features: {len(numeric_features) + len(categorical_features)}\")\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(f\"\\nValores faltantes:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ESTADÍSTICAS DESCRIPTIVAS - FRECUENCIA PROMEDIO POR CATEGORÍA\n",
      "================================================================================\n",
      "\n",
      "año_cursado:\n",
      "             mean  count    sum\n",
      "año_cursado                    \n",
      "1er año      1.03    104 107.00\n",
      "2do año      1.02     86  88.00\n",
      "3er año      1.01     92  93.00\n",
      "4to año      1.04     98 102.00\n",
      "posgrado     1.03     36  37.00\n",
      "\n",
      "estudios_area:\n",
      "                mean  count    sum\n",
      "estudios_area                     \n",
      "Administracion  1.01    111 112.00\n",
      "Ciencias        1.05     99 104.00\n",
      "Humanidades     1.04     91  95.00\n",
      "Otro            1.01    115 116.00\n",
      "\n",
      "2_o_mas_inquilinos:\n",
      "                    mean  count    sum\n",
      "2_o_mas_inquilinos                    \n",
      "No                  1.01    278 282.00\n",
      "Si                  1.05    138 145.00\n",
      "\n",
      "genero:\n",
      "              mean  count    sum\n",
      "genero                          \n",
      "Femenino      1.02    185 188.00\n",
      "Masculino     1.02    199 204.00\n",
      "No respuesta  1.10     10  11.00\n",
      "Otro          1.09     22  24.00\n",
      "\n",
      "extintor_incendios:\n",
      "                    mean  count    sum\n",
      "extintor_incendios                    \n",
      "No                  1.02    133 136.00\n",
      "Si                  1.03    283 291.00\n"
     ]
    }
   ],
   "source": [
    "# Estadísticas descriptivas - Frecuencia promedio por categoría\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS - FRECUENCIA PROMEDIO POR CATEGORÍA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for var in categorical_features:\n",
    "    print(f\"\\n{var}:\")\n",
    "    tabla = df.groupby(var)['Gastos_Adicionales_siniestros_num'].agg(['mean', 'count', 'sum'])\n",
    "    print(tabla.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento y Split de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (332, 7) | Frecuencia promedio: 1.0271\n",
      "Test set:  (84, 7) | Frecuencia promedio: 1.0238\n",
      "\n",
      "Distribución train:\n",
      "Gastos_Adicionales_siniestros_num\n",
      "1.00    323\n",
      "2.00      9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución test:\n",
      "Gastos_Adicionales_siniestros_num\n",
      "1.00    82\n",
      "2.00     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separar features y target\n",
    "X = df.drop('Gastos_Adicionales_siniestros_num', axis=1)\n",
    "y = df['Gastos_Adicionales_siniestros_num']\n",
    "\n",
    "# Split train/test estratificado 80/20 (estratificado por frecuencia)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape} | Frecuencia promedio: {y_train.mean():.4f}\")\n",
    "print(f\"Test set:  {X_test.shape} | Frecuencia promedio: {y_test.mean():.4f}\")\n",
    "print(f\"\\nDistribución train:\\n{y_train.value_counts().sort_index()}\")\n",
    "print(f\"\\nDistribución test:\\n{y_test.value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features después de preprocesamiento: 14\n",
      "Shape X_train: (332, 14)\n",
      "Shape X_test: (84, 14)\n"
     ]
    }
   ],
   "source": [
    "# Pipeline de preprocesamiento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Ajustar y transformar datos\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Obtener nombres de features después de one-hot encoding\n",
    "feature_names_num = numeric_features\n",
    "feature_names_cat = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "feature_names = list(feature_names_num) + list(feature_names_cat)\n",
    "\n",
    "# Convertir a DataFrame (mantener índices originales para alinear con y_train/y_test)\n",
    "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index)\n",
    "X_test_df = pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index)\n",
    "\n",
    "print(f\"\\nFeatures después de preprocesamiento: {len(feature_names)}\")\n",
    "print(f\"Shape X_train: {X_train_df.shape}\")\n",
    "print(f\"Shape X_test: {X_test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Fase 1 - Multinomial Logit con Selección Backward de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODELO MULTINOMIAL LOGIT COMPLETO (todas las variables)\n",
      "================================================================================\n",
      "                                  MNLogit Regression Results                                 \n",
      "=============================================================================================\n",
      "Dep. Variable:     Gastos_Adicionales_siniestros_num   No. Observations:                  332\n",
      "Model:                                       MNLogit   Df Residuals:                      317\n",
      "Method:                                          MLE   Df Model:                           14\n",
      "Date:                               Fri, 10 Oct 2025   Pseudo R-squ.:                  0.2599\n",
      "Time:                                       00:05:31   Log-Likelihood:                -30.601\n",
      "converged:                                     False   LL-Null:                       -41.348\n",
      "Covariance Type:                           nonrobust   LLR p-value:                   0.08963\n",
      "=======================================================================================================\n",
      "Gastos_Adicionales_siniestros_num=2       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "const                                  -5.6044      1.566     -3.578      0.000      -8.674      -2.534\n",
      "calif_promedio                         -0.5444      0.363     -1.498      0.134      -1.256       0.168\n",
      "distancia_al_campus                    -0.0180      0.403     -0.045      0.964      -0.808       0.772\n",
      "año_cursado_2do año                    -0.7173      1.321     -0.543      0.587      -3.306       1.871\n",
      "año_cursado_3er año                    -0.3814      1.275     -0.299      0.765      -2.881       2.118\n",
      "año_cursado_4to año                     0.7492      0.987      0.759      0.448      -1.186       2.685\n",
      "año_cursado_posgrado                   -0.1801      1.340     -0.134      0.893      -2.806       2.445\n",
      "estudios_area_Ciencias                  1.5597      1.219      1.280      0.201      -0.829       3.949\n",
      "estudios_area_Humanidades               1.9648      1.217      1.615      0.106      -0.420       4.349\n",
      "estudios_area_Otro                    -22.1339   6.42e+04     -0.000      1.000   -1.26e+05    1.26e+05\n",
      "2_o_mas_inquilinos_Si                   1.1032      0.786      1.404      0.160      -0.437       2.644\n",
      "genero_Masculino                        0.0054      0.858      0.006      0.995      -1.677       1.688\n",
      "genero_No respuesta                     2.2258      1.559      1.428      0.153      -0.829       5.281\n",
      "genero_Otro                             1.5953      1.115      1.431      0.152      -0.589       3.780\n",
      "extintor_incendios_Si                  -0.0742      0.832     -0.089      0.929      -1.704       1.556\n",
      "=======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Modelo Multinomial Logit completo para referencia\n",
    "X_train_const = sm.add_constant(X_train_df)\n",
    "\n",
    "# Usar MNLogit para clasificación multinomial de frecuencia\n",
    "from statsmodels.discrete.discrete_model import MNLogit\n",
    "\n",
    "mnlogit_full = MNLogit(y_train, X_train_const)\n",
    "result_full = mnlogit_full.fit(disp=0)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODELO MULTINOMIAL LOGIT COMPLETO (todas las variables)\")\n",
    "print(\"=\" * 80)\n",
    "print(result_full.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SELECCIÓN BACKWARD DE VARIABLES (α = 0.05)\n",
      "================================================================================\n",
      "\n",
      "⚠ No hay más features para evaluar\n",
      "\n",
      "================================================================================\n",
      "VARIABLES SELECCIONADAS: 14\n",
      "================================================================================\n",
      "  • calif_promedio\n",
      "  • distancia_al_campus\n",
      "  • año_cursado_2do año\n",
      "  • año_cursado_3er año\n",
      "  • año_cursado_4to año\n",
      "  • año_cursado_posgrado\n",
      "  • estudios_area_Ciencias\n",
      "  • estudios_area_Humanidades\n",
      "  • estudios_area_Otro\n",
      "  • 2_o_mas_inquilinos_Si\n",
      "  • genero_Masculino\n",
      "  • genero_No respuesta\n",
      "  • genero_Otro\n",
      "  • extintor_incendios_Si\n",
      "\n",
      "                                  MNLogit Regression Results                                 \n",
      "=============================================================================================\n",
      "Dep. Variable:     Gastos_Adicionales_siniestros_num   No. Observations:                  332\n",
      "Model:                                       MNLogit   Df Residuals:                      317\n",
      "Method:                                          MLE   Df Model:                           14\n",
      "Date:                               Fri, 10 Oct 2025   Pseudo R-squ.:                  0.2599\n",
      "Time:                                       00:05:32   Log-Likelihood:                -30.601\n",
      "converged:                                     False   LL-Null:                       -41.348\n",
      "Covariance Type:                           nonrobust   LLR p-value:                   0.08963\n",
      "=======================================================================================================\n",
      "Gastos_Adicionales_siniestros_num=2       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "const                                  -5.6044      1.566     -3.578      0.000      -8.674      -2.534\n",
      "calif_promedio                         -0.5444      0.363     -1.498      0.134      -1.256       0.168\n",
      "distancia_al_campus                    -0.0180      0.403     -0.045      0.964      -0.808       0.772\n",
      "año_cursado_2do año                    -0.7173      1.321     -0.543      0.587      -3.306       1.871\n",
      "año_cursado_3er año                    -0.3814      1.275     -0.299      0.765      -2.881       2.118\n",
      "año_cursado_4to año                     0.7492      0.987      0.759      0.448      -1.186       2.685\n",
      "año_cursado_posgrado                   -0.1801      1.340     -0.134      0.893      -2.806       2.445\n",
      "estudios_area_Ciencias                  1.5597      1.219      1.280      0.201      -0.829       3.949\n",
      "estudios_area_Humanidades               1.9648      1.217      1.615      0.106      -0.420       4.349\n",
      "estudios_area_Otro                    -22.1339   6.42e+04     -0.000      1.000   -1.26e+05    1.26e+05\n",
      "2_o_mas_inquilinos_Si                   1.1032      0.786      1.404      0.160      -0.437       2.644\n",
      "genero_Masculino                        0.0054      0.858      0.006      0.995      -1.677       1.688\n",
      "genero_No respuesta                     2.2258      1.559      1.428      0.153      -0.829       5.281\n",
      "genero_Otro                             1.5953      1.115      1.431      0.152      -0.589       3.780\n",
      "extintor_incendios_Si                  -0.0742      0.832     -0.089      0.929      -1.704       1.556\n",
      "=======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Selección Backward basada en p-valores con Multinomial Logit\n",
    "from statsmodels.discrete.discrete_model import MNLogit\n",
    "\n",
    "def backward_elimination(X, y, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Realiza selección backward de variables basada en p-valores del Multinomial Logit.\n",
    "    \"\"\"\n",
    "    features = list(X.columns)\n",
    "    \n",
    "    while len(features) > 0:\n",
    "        X_const = sm.add_constant(X[features])\n",
    "        model = MNLogit(y, X_const).fit(disp=0)\n",
    "        \n",
    "        # Obtener p-valores excluyendo la constante\n",
    "        # En MNLogit, los p-valores están organizados por clase\n",
    "        p_values = model.pvalues.iloc[:, 1:]  # Excluir constante\n",
    "        \n",
    "        # Obtener el máximo p-valor entre todas las clases\n",
    "        if p_values.empty or len(p_values.columns) == 0:\n",
    "            print(\"⚠ No hay más features para evaluar\")\n",
    "            break\n",
    "        \n",
    "        max_p_values = p_values.max(axis=0)  # Máximo p-valor por feature\n",
    "        max_p_value = max_p_values.max()\n",
    "        \n",
    "        # Si el p-valor máximo es menor o igual al nivel de significancia, todas son significativas\n",
    "        if pd.isna(max_p_value) or max_p_value <= significance_level:\n",
    "            break\n",
    "            \n",
    "        feature_to_remove = max_p_values.idxmax()\n",
    "        features.remove(feature_to_remove)\n",
    "        print(f\"Eliminando: {feature_to_remove} (p-valor: {max_p_value:.4f})\")\n",
    "    \n",
    "    return features, model\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SELECCIÓN BACKWARD DE VARIABLES (α = 0.05)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "selected_features, mnlogit_backward = backward_elimination(X_train_df, y_train, significance_level=0.05)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(f\"VARIABLES SELECCIONADAS: {len(selected_features)}\")\n",
    "print(\"=\" * 80)\n",
    "for feat in selected_features:\n",
    "    print(f\"  • {feat}\")\n",
    "print()\n",
    "print(mnlogit_backward.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURES FINALES (con todos los niveles de categóricas)\n",
      "================================================================================\n",
      "\n",
      "Total features: 14\n",
      "\n",
      "Features seleccionadas:\n",
      "  ✓ 2_o_mas_inquilinos_Si\n",
      "  ✓ año_cursado_2do año\n",
      "  ✓ año_cursado_3er año\n",
      "  ✓ año_cursado_4to año\n",
      "  ✓ año_cursado_posgrado\n",
      "  ✓ calif_promedio\n",
      "  ✓ distancia_al_campus\n",
      "  ✓ estudios_area_Ciencias\n",
      "  ✓ estudios_area_Humanidades\n",
      "  ✓ estudios_area_Otro\n",
      "  ✓ extintor_incendios_Si\n",
      "  ✓ genero_Masculino\n",
      "  ✓ genero_No respuesta\n",
      "  ✓ genero_Otro\n",
      "\n",
      "Shape final - Train: (332, 14) | Test: (84, 14)\n"
     ]
    }
   ],
   "source": [
    "# Expandir variables categóricas completas\n",
    "def expand_categorical_features(selected_features, all_feature_names, categorical_features):\n",
    "    selected_cat_vars = set()\n",
    "    for feat in selected_features:\n",
    "        for cat_var in categorical_features:\n",
    "            if feat.startswith(cat_var + '_'):\n",
    "                selected_cat_vars.add(cat_var)\n",
    "                break\n",
    "    \n",
    "    final_features = []\n",
    "    for feat in selected_features:\n",
    "        is_categorical = any(feat.startswith(cat_var + '_') for cat_var in categorical_features)\n",
    "        if not is_categorical:\n",
    "            final_features.append(feat)\n",
    "    \n",
    "    for feat in all_feature_names:\n",
    "        for cat_var in selected_cat_vars:\n",
    "            if feat.startswith(cat_var + '_') and feat not in final_features:\n",
    "                final_features.append(feat)\n",
    "                break\n",
    "    \n",
    "    return sorted(final_features)\n",
    "\n",
    "final_selected_features = expand_categorical_features(selected_features, feature_names, categorical_features)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURES FINALES (con todos los niveles de categóricas)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal features: {len(final_selected_features)}\")\n",
    "print(\"\\nFeatures seleccionadas:\")\n",
    "for feat in final_selected_features:\n",
    "    indicator = \"✓\" if feat in selected_features else \"+\"\n",
    "    print(f\"  {indicator} {feat}\")\n",
    "\n",
    "X_train_selected = X_train_df[final_selected_features]\n",
    "X_test_selected = X_test_df[final_selected_features]\n",
    "\n",
    "print(f\"\\nShape final - Train: {X_train_selected.shape} | Test: {X_test_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Fase 1 - LazyPredict para Identificar Mejores Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EJECUTANDO LAZYPREDICT (CLASIFICACIÓN MULTINOMIAL)\n",
      "================================================================================\n",
      "\n",
      "Dataset: 332 muestras, 14 features\n",
      "Esto puede tomar varios minutos...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ad38bb99b543bd82e09440b229fcd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9, number of negative: 323\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 186\n",
      "[LightGBM] [Info] Number of data points in the train set: 332, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.027108 -> initscore=-3.580428\n",
      "[LightGBM] [Info] Start training from score -3.580428\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "================================================================================\n",
      "RESULTADOS DE LAZYPREDICT\n",
      "================================================================================\n",
      "\n",
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "ExtraTreeClassifier                0.96               0.74     0.74      0.97   \n",
      "AdaBoostClassifier                 0.98               0.50     0.50      0.96   \n",
      "BaggingClassifier                  0.98               0.50     0.50      0.96   \n",
      "CalibratedClassifierCV             0.98               0.50     0.50      0.96   \n",
      "BernoulliNB                        0.98               0.50     0.50      0.96   \n",
      "DummyClassifier                    0.98               0.50     0.50      0.96   \n",
      "LogisticRegression                 0.98               0.50     0.50      0.96   \n",
      "LinearDiscriminantAnalysis         0.98               0.50     0.50      0.96   \n",
      "KNeighborsClassifier               0.98               0.50     0.50      0.96   \n",
      "SVC                                0.98               0.50     0.50      0.96   \n",
      "RidgeClassifierCV                  0.98               0.50     0.50      0.96   \n",
      "RandomForestClassifier             0.98               0.50     0.50      0.96   \n",
      "LinearSVC                          0.98               0.50     0.50      0.96   \n",
      "PassiveAggressiveClassifier        0.98               0.50     0.50      0.96   \n",
      "RidgeClassifier                    0.98               0.50     0.50      0.96   \n",
      "QuadraticDiscriminantAnalysis      0.98               0.50     0.50      0.96   \n",
      "ExtraTreesClassifier               0.96               0.49     0.49      0.96   \n",
      "LGBMClassifier                     0.96               0.49     0.49      0.96   \n",
      "SGDClassifier                      0.96               0.49     0.49      0.96   \n",
      "LabelPropagation                   0.95               0.49     0.49      0.95   \n",
      "LabelSpreading                     0.95               0.49     0.49      0.95   \n",
      "Perceptron                         0.94               0.48     0.48      0.95   \n",
      "DecisionTreeClassifier             0.93               0.48     0.48      0.94   \n",
      "GaussianNB                         0.32               0.41     0.41      0.47   \n",
      "NearestCentroid                    0.79               0.40     0.40      0.86   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "ExtraTreeClassifier                  0.01  \n",
      "AdaBoostClassifier                   0.08  \n",
      "BaggingClassifier                    0.02  \n",
      "CalibratedClassifierCV               0.03  \n",
      "BernoulliNB                          0.02  \n",
      "DummyClassifier                      0.01  \n",
      "LogisticRegression                   0.01  \n",
      "LinearDiscriminantAnalysis           0.01  \n",
      "KNeighborsClassifier                 0.01  \n",
      "SVC                                  0.01  \n",
      "RidgeClassifierCV                    0.02  \n",
      "RandomForestClassifier               0.11  \n",
      "LinearSVC                            0.01  \n",
      "PassiveAggressiveClassifier          0.01  \n",
      "RidgeClassifier                      0.01  \n",
      "QuadraticDiscriminantAnalysis        0.01  \n",
      "ExtraTreesClassifier                 0.08  \n",
      "LGBMClassifier                       0.03  \n",
      "SGDClassifier                        0.01  \n",
      "LabelPropagation                     0.01  \n",
      "LabelSpreading                       0.02  \n",
      "Perceptron                           0.01  \n",
      "DecisionTreeClassifier               0.01  \n",
      "GaussianNB                           0.01  \n",
      "NearestCentroid                      0.01  \n"
     ]
    }
   ],
   "source": [
    "# Ejecutar LazyPredict para clasificación multinomial\n",
    "print(\"=\" * 80)\n",
    "print(\"EJECUTANDO LAZYPREDICT (CLASIFICACIÓN MULTINOMIAL)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset: {X_train_selected.shape[0]} muestras, {X_train_selected.shape[1]} features\")\n",
    "print(\"Esto puede tomar varios minutos...\\n\")\n",
    "\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, predictions=True, random_state=42)\n",
    "models, predictions = clf.fit(X_train_selected, X_test_selected, y_train, y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTADOS DE LAZYPREDICT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOP 5 MODELOS POR F1 SCORE\n",
      "================================================================================\n",
      "\n",
      "                        Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                    \n",
      "ExtraTreeClassifier         0.96               0.74     0.74      0.97   \n",
      "AdaBoostClassifier          0.98               0.50     0.50      0.96   \n",
      "BaggingClassifier           0.98               0.50     0.50      0.96   \n",
      "CalibratedClassifierCV      0.98               0.50     0.50      0.96   \n",
      "BernoulliNB                 0.98               0.50     0.50      0.96   \n",
      "\n",
      "                        Time Taken  \n",
      "Model                               \n",
      "ExtraTreeClassifier           0.01  \n",
      "AdaBoostClassifier            0.08  \n",
      "BaggingClassifier             0.02  \n",
      "CalibratedClassifierCV        0.03  \n",
      "BernoulliNB                   0.02  \n",
      "\n",
      "================================================================================\n",
      "MODELOS SELECCIONADOS PARA FASE 2 (GridSearchCV)\n",
      "================================================================================\n",
      "\n",
      "Total modelos a optimizar: 9\n",
      "\n",
      "De LazyPredict (Top 5):\n",
      "1. ExtraTreeClassifier - F1: 0.9677 | Accuracy: 0.9643\n",
      "2. AdaBoostClassifier - F1: 0.9644 | Accuracy: 0.9762\n",
      "3. BaggingClassifier - F1: 0.9644 | Accuracy: 0.9762\n",
      "4. CalibratedClassifierCV - F1: 0.9644 | Accuracy: 0.9762\n",
      "5. BernoulliNB - F1: 0.9644 | Accuracy: 0.9762\n",
      "\n",
      "Modelos adicionales solicitados:\n",
      "  • LogisticRegression - F1: 0.9644 | Accuracy: 0.9762\n",
      "  • RandomForestClassifier - F1: 0.9644 | Accuracy: 0.9762\n",
      "  • XGBClassifier - (no evaluado en LazyPredict)\n",
      "  • GradientBoostingClassifier - (no evaluado en LazyPredict)\n"
     ]
    }
   ],
   "source": [
    "# Top 5 modelos por F1 Score (mayor es mejor para clasificación)\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 5 MODELOS POR F1 SCORE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "top_5_models = models.nlargest(5, 'F1 Score')\n",
    "print(top_5_models)\n",
    "\n",
    "# Agregar modelos adicionales solicitados explícitamente\n",
    "modelos_adicionales = ['LogisticRegression', 'RandomForestClassifier', 'XGBClassifier', 'GradientBoostingClassifier']\n",
    "modelos_para_optimizar = list(set(list(top_5_models.index) + modelos_adicionales))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODELOS SELECCIONADOS PARA FASE 2 (GridSearchCV)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal modelos a optimizar: {len(modelos_para_optimizar)}\")\n",
    "print(\"\\nDe LazyPredict (Top 5):\")\n",
    "for i, model_name in enumerate(top_5_models.index, 1):\n",
    "    f1 = top_5_models.loc[model_name, 'F1 Score']\n",
    "    acc = top_5_models.loc[model_name, 'Accuracy']\n",
    "    print(f\"{i}. {model_name} - F1: {f1:.4f} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nModelos adicionales solicitados:\")\n",
    "for model_name in modelos_adicionales:\n",
    "    if model_name in models.index:\n",
    "        f1 = models.loc[model_name, 'F1 Score']\n",
    "        acc = models.loc[model_name, 'Accuracy']\n",
    "        print(f\"  • {model_name} - F1: {f1:.4f} | Accuracy: {acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"  • {model_name} - (no evaluado en LazyPredict)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "## Fin de Fase 1\n",
    "\n",
    "**Fase 1 completada exitosamente:**\n",
    "1. ✅ Preprocesamiento con StandardScaler + OneHotEncoder\n",
    "2. ✅ Multinomial Logit con todas las variables\n",
    "3. ✅ Selección Backward de variables (α = 0.05)\n",
    "4. ✅ Expansión de variables categóricas completas\n",
    "5. ✅ LazyPredict para identificar mejores algoritmos\n",
    "\n",
    "**Próximos pasos (Fase 2):**\n",
    "- Optimización de hiperparámetros con GridSearchCV\n",
    "- Evaluación en conjunto de test\n",
    "- Selección del mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewgwsrryye",
   "metadata": {},
   "source": [
    "---\n",
    "## FASE 2: Optimización de Hiperparámetros con GridSearchCV\n",
    "\n",
    "**Objetivo:** Optimizar los mejores modelos identificados en Fase 1 usando validación cruzada estratificada\n",
    "**Métrica de optimización:** F1-Score (weighted) - mayor es mejor\n",
    "**Métricas de evaluación:** F1-Score, Accuracy, Precision, Recall, Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fs4bygelvqq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Funciones de evaluación definidas\n"
     ]
    }
   ],
   "source": [
    "# Función de evaluación Gini para clasificación multinomial\n",
    "def gini_coefficient_multiclass(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Calcula el coeficiente de Gini promedio para clasificación multinomial.\n",
    "    Usa One-vs-Rest para cada clase.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    \n",
    "    classes = np.unique(y_true)\n",
    "    y_true_bin = label_binarize(y_true, classes=classes)\n",
    "    \n",
    "    gini_scores = []\n",
    "    for i in range(len(classes)):\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "            gini = 2 * auc - 1\n",
    "            gini_scores.append(gini)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return np.mean(gini_scores) if gini_scores else np.nan\n",
    "\n",
    "print(\"✓ Funciones de evaluación definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d0ugd1r37mb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Grillas con parámetros de regularización/suavizado:\n",
      "  - LogisticRegression: C (regularización L1/L2) + class_weight\n",
      "  - DecisionTreeClassifier: poda (ccp_alpha) + class_weight\n",
      "  - RandomForestClassifier: poda + min_samples + class_weight\n",
      "  - GradientBoostingClassifier: learning_rate + subsample + poda\n",
      "  - AdaBoostClassifier: learning_rate + estimator\n",
      "  - XGBClassifier: reg_alpha/lambda + learning_rate\n",
      "  - KNeighborsClassifier: n_neighbors + weights\n",
      "  - SVC: C (regularización) + class_weight\n",
      "  - LinearDiscriminantAnalysis: shrinkage\n",
      "  - QuadraticDiscriminantAnalysis: reg_param\n"
     ]
    }
   ],
   "source": [
    "# Grillas de hiperparámetros con parámetros de suavizado/regularización\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],  # Inverso de regularización\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['saga'],  # Soporta l1 y l2 para multinomial\n",
    "        'multi_class': ['multinomial'],\n",
    "        'max_iter': [1000],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'DecisionTreeClassifier': {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [10, 20, 50],\n",
    "        'min_samples_leaf': [5, 10, 20],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'ccp_alpha': [0.0, 0.01, 0.05],  # Poda de complejidad\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7, None],\n",
    "        'min_samples_split': [10, 20, 50],\n",
    "        'min_samples_leaf': [5, 10, 20],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'class_weight': ['balanced', 'balanced_subsample', None],\n",
    "        'ccp_alpha': [0.0, 0.01],  # Poda\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'GradientBoostingClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [10, 20],\n",
    "        'min_samples_leaf': [5, 10],\n",
    "        'subsample': [0.7, 0.8, 1.0],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'ccp_alpha': [0.0, 0.01],  # Poda\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'AdaBoostClassifier': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "        'algorithm': ['SAMME'],\n",
    "        'estimator': [\n",
    "            DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "            DecisionTreeClassifier(max_depth=2, random_state=42),\n",
    "            DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "        ],\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'XGBClassifier': {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'subsample': [0.7, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 1.0],  # L1 regularization\n",
    "        'reg_lambda': [1, 2, 5],  # L2 regularization\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors': [3, 5, 7, 10],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    \n",
    "    'SVC': {\n",
    "        'C': [0.1, 1.0, 10.0],  # Regularización\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'probability': [True],  # Para predict_proba\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'shrinkage': [None, 0.1, 0.5, 0.9, 'auto']\n",
    "    },\n",
    "    \n",
    "    'QuadraticDiscriminantAnalysis': {\n",
    "        'reg_param': [0.0, 0.1, 0.3, 0.5, 0.7]  # Regularización\n",
    "    }\n",
    "}\n",
    "\n",
    "model_mapping = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(random_state=42),\n",
    "    'XGBClassifier': XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'SVC': SVC(random_state=42),\n",
    "    'LinearDiscriminantAnalysis': LinearDiscriminantAnalysis(),\n",
    "    'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "print(\"✓ Grillas con parámetros de regularización/suavizado:\")\n",
    "print(\"  - LogisticRegression: C (regularización L1/L2) + class_weight\")\n",
    "print(\"  - DecisionTreeClassifier: poda (ccp_alpha) + class_weight\")\n",
    "print(\"  - RandomForestClassifier: poda + min_samples + class_weight\")\n",
    "print(\"  - GradientBoostingClassifier: learning_rate + subsample + poda\")\n",
    "print(\"  - AdaBoostClassifier: learning_rate + estimator\")\n",
    "print(\"  - XGBClassifier: reg_alpha/lambda + learning_rate\")\n",
    "print(\"  - KNeighborsClassifier: n_neighbors + weights\")\n",
    "print(\"  - SVC: C (regularización) + class_weight\")\n",
    "print(\"  - LinearDiscriminantAnalysis: shrinkage\")\n",
    "print(\"  - QuadraticDiscriminantAnalysis: reg_param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "alw4kuyhf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GRID SEARCH CON VALIDACIÓN CRUZADA\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[1/9] Optimizando: BernoulliNB\n",
      "================================================================================\n",
      "⚠ No hay grilla para BernoulliNB, saltando...\n",
      "\n",
      "================================================================================\n",
      "[2/9] Optimizando: RandomForestClassifier\n",
      "================================================================================\n",
      "Combinaciones: 1296 | Total fits: 6480\n",
      "✓ F1-Score CV: 0.9596\n",
      "Mejores parámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 50, 'random_state': 42}\n",
      "\n",
      "================================================================================\n",
      "[3/9] Optimizando: CalibratedClassifierCV\n",
      "================================================================================\n",
      "⚠ No hay grilla para CalibratedClassifierCV, saltando...\n",
      "\n",
      "================================================================================\n",
      "[4/9] Optimizando: AdaBoostClassifier\n",
      "================================================================================\n",
      "Combinaciones: 36 | Total fits: 180\n",
      "✓ F1-Score CV: 0.9596\n",
      "Mejores parámetros: {'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=1, random_state=42), 'learning_rate': 0.01, 'n_estimators': 100, 'random_state': 42}\n",
      "\n",
      "================================================================================\n",
      "[5/9] Optimizando: XGBClassifier\n",
      "================================================================================\n",
      "Combinaciones: 2187 | Total fits: 10935\n",
      "✗ Error: \n",
      "All the 10935 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10935 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/riemannintegrable/anaconda3/envs/competencia_cas_python/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/riemannintegrable/anaconda3/envs/competencia_cas_python/lib/python3.13/site-packages/xgboost/core.py\", line 705, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/riemannintegrable/anaconda3/envs/competencia_cas_python/lib/python3.13/site-packages/xgboost/sklearn.py\", line 1641, in fit\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Invalid classes inferred from unique values of `y`.  Expected: [0 1], got [1. 2.]\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[6/9] Optimizando: ExtraTreeClassifier\n",
      "================================================================================\n",
      "⚠ No hay grilla para ExtraTreeClassifier, saltando...\n",
      "\n",
      "================================================================================\n",
      "[7/9] Optimizando: LogisticRegression\n",
      "================================================================================\n",
      "Combinaciones: 24 | Total fits: 120\n",
      "✓ F1-Score CV: 0.9596\n",
      "Mejores parámetros: {'C': 0.001, 'class_weight': None, 'max_iter': 1000, 'multi_class': 'multinomial', 'penalty': 'l1', 'random_state': 42, 'solver': 'saga'}\n",
      "\n",
      "================================================================================\n",
      "[8/9] Optimizando: GradientBoostingClassifier\n",
      "================================================================================\n",
      "Combinaciones: 1296 | Total fits: 6480\n",
      "✓ F1-Score CV: 0.9596\n",
      "Mejores parámetros: {'ccp_alpha': 0.0, 'learning_rate': 0.01, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 50, 'random_state': 42, 'subsample': 0.7}\n",
      "\n",
      "================================================================================\n",
      "[9/9] Optimizando: BaggingClassifier\n",
      "================================================================================\n",
      "⚠ No hay grilla para BaggingClassifier, saltando...\n",
      "\n",
      "================================================================================\n",
      "Completado: 4/9 modelos\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV con validación cruzada\n",
    "print(\"=\" * 80)\n",
    "print(\"GRID SEARCH CON VALIDACIÓN CRUZADA\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Scorer F1-weighted (apropiado para desbalanceo de clases)\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "optimized_models = {}\n",
    "grid_results = {}\n",
    "\n",
    "for i, model_name in enumerate(modelos_para_optimizar, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{i}/{len(modelos_para_optimizar)}] Optimizando: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if model_name not in param_grids or model_name not in model_mapping:\n",
    "        print(f\"⚠ No hay grilla para {model_name}, saltando...\")\n",
    "        continue\n",
    "    \n",
    "    base_model = model_mapping[model_name]\n",
    "    param_grid = param_grids[model_name]\n",
    "    \n",
    "    n_combinations = np.prod([len(v) if isinstance(v, list) else 1 for v in param_grid.values()])\n",
    "    print(f\"Combinaciones: {int(n_combinations)} | Total fits: {int(5 * n_combinations)}\")\n",
    "    \n",
    "    try:\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=f1_scorer,  # Optimiza F1-score weighted\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "        \n",
    "        optimized_models[model_name] = grid_search.best_estimator_\n",
    "        grid_results[model_name] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score_cv': grid_search.best_score_\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ F1-Score CV: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"Mejores parámetros: {grid_search.best_params_}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {str(e)}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Completado: {len(optimized_models)}/{len(modelos_para_optimizar)} modelos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "t1wlyv79ssg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUACIÓN EN TEST SET\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Evaluando: RandomForestClassifier\n",
      "================================================================================\n",
      "F1 (CV): 0.9596 | F1 (Test): 0.9644\n",
      "Accuracy: 0.9762 | Precision: 0.9529 | Recall: 0.9762\n",
      "Gini: 0.0122\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[82  0]\n",
      " [ 2  0]]\n",
      "\n",
      "Distribución de predicciones:\n",
      "  Predicho: {1.0: np.int64(84)}\n",
      "  Real:     {1.0: np.int64(82), 2.0: np.int64(2)}\n",
      "\n",
      "================================================================================\n",
      "Evaluando: AdaBoostClassifier\n",
      "================================================================================\n",
      "F1 (CV): 0.9596 | F1 (Test): 0.9644\n",
      "Accuracy: 0.9762 | Precision: 0.9529 | Recall: 0.9762\n",
      "Gini: 0.0244\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[82  0]\n",
      " [ 2  0]]\n",
      "\n",
      "Distribución de predicciones:\n",
      "  Predicho: {1.0: np.int64(84)}\n",
      "  Real:     {1.0: np.int64(82), 2.0: np.int64(2)}\n",
      "\n",
      "================================================================================\n",
      "Evaluando: LogisticRegression\n",
      "================================================================================\n",
      "F1 (CV): 0.9596 | F1 (Test): 0.9644\n",
      "Accuracy: 0.9762 | Precision: 0.9529 | Recall: 0.9762\n",
      "Gini: 0.0000\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[82  0]\n",
      " [ 2  0]]\n",
      "\n",
      "Distribución de predicciones:\n",
      "  Predicho: {1.0: np.int64(84)}\n",
      "  Real:     {1.0: np.int64(82), 2.0: np.int64(2)}\n",
      "\n",
      "================================================================================\n",
      "Evaluando: GradientBoostingClassifier\n",
      "================================================================================\n",
      "F1 (CV): 0.9596 | F1 (Test): 0.9644\n",
      "Accuracy: 0.9762 | Precision: 0.9529 | Recall: 0.9762\n",
      "Gini: 0.1220\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[82  0]\n",
      " [ 2  0]]\n",
      "\n",
      "Distribución de predicciones:\n",
      "  Predicho: {1.0: np.int64(84)}\n",
      "  Real:     {1.0: np.int64(82), 2.0: np.int64(2)}\n",
      "\n",
      "================================================================================\n",
      "RESUMEN DE RESULTADOS (ordenado por F1-Score Test)\n",
      "================================================================================\n",
      "\n",
      "                    Modelo  F1_CV  F1_Test  Accuracy  Precision  Recall  Gini\n",
      "    RandomForestClassifier   0.96     0.96      0.98       0.95    0.98  0.01\n",
      "        AdaBoostClassifier   0.96     0.96      0.98       0.95    0.98  0.02\n",
      "        LogisticRegression   0.96     0.96      0.98       0.95    0.98  0.00\n",
      "GradientBoostingClassifier   0.96     0.96      0.98       0.95    0.98  0.12\n"
     ]
    }
   ],
   "source": [
    "# Evaluación en test set\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUACIÓN EN TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for model_name, model in optimized_models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluando: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Predicciones en test\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    \n",
    "    # Predicciones de probabilidad (si el modelo lo soporta)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test_selected)\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        y_pred_proba = None  # No todas las decisiones son probabilidades\n",
    "    else:\n",
    "        y_pred_proba = None\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Calcular Gini si hay probabilidades\n",
    "    try:\n",
    "        if y_pred_proba is not None:\n",
    "            gini = gini_coefficient_multiclass(y_test, y_pred_proba)\n",
    "        else:\n",
    "            gini = np.nan\n",
    "    except:\n",
    "        gini = np.nan\n",
    "    \n",
    "    test_results.append({\n",
    "        'Modelo': model_name,\n",
    "        'F1_CV': grid_results[model_name]['best_score_cv'],\n",
    "        'F1_Test': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Gini': gini\n",
    "    })\n",
    "    \n",
    "    print(f\"F1 (CV): {grid_results[model_name]['best_score_cv']:.4f} | F1 (Test): {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "    print(f\"Gini: {gini:.4f}\")\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\nMatriz de Confusión:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Distribución de predicciones\n",
    "    print(f\"\\nDistribución de predicciones:\")\n",
    "    pred_dist = pd.Series(y_pred).value_counts().sort_index()\n",
    "    real_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "    print(f\"  Predicho: {dict(pred_dist)}\")\n",
    "    print(f\"  Real:     {dict(real_dist)}\")\n",
    "\n",
    "df_results = pd.DataFrame(test_results).sort_values('F1_Test', ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESUMEN DE RESULTADOS (ordenado por F1-Score Test)\")\n",
    "print(f\"{'='*80}\")\n",
    "print()\n",
    "print(df_results.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
